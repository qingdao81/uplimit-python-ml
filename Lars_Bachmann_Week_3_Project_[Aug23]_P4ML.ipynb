{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qingdao81/uplimit-python-ml/blob/main/Lars_Bachmann_Week_3_Project_%5BAug23%5D_P4ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2040e3d-bfb8-40b8-8861-4485db413cf7",
      "metadata": {
        "id": "a2040e3d-bfb8-40b8-8861-4485db413cf7"
      },
      "source": [
        "> 1. DUPLICATE THIS COLAB DOCUMENT TO START WORKING ON IT: On the top-left corner of this page, go to File > Save a copy to drive.\n",
        "> 2. SHARE SETTINGS: In the new notebook, set the sharing settings to \"Anyone with the link\" by clicking \"Share\" on the top-right corner.\n",
        "\n",
        "<center>\n",
        "  <img src=https://www.freevector.com/uploads/vector/preview/31087/07Januari2021-06_generated.jpg width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "\n",
        "# Week 3: (Un)Supervised Predictions for Airbnb listings!\n",
        "\n",
        "This is the third and last week's project 😢 of *Intermediate Python for Data Science*. Here we are going to put everything we've learned over the last three weeks together and create yet another exciting algorithm that we can then use for creating a Streamlit App!\n",
        "\n",
        "We'll first start using unsupervised learning, and with that information we are going to make a supervised model. All by using Pipelines, ColumnTransformers, Plotly and some other parts! Let's do it 💪💪!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Dataset\n",
        "\n",
        "You'll need to download some prerequisite Python packages in order to run all the code below. Let's install them!"
      ],
      "metadata": {
        "id": "rISRrL2X93Yx"
      },
      "id": "rISRrL2X93Yx"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install numpy streamlit gdown pandas==1.5.2 scikit-learn==1.2.0"
      ],
      "metadata": {
        "id": "JspZZKEj9ytH"
      },
      "id": "JspZZKEj9ytH",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c21b1ad-25c8-4a99-a4e0-764d568f711b",
      "metadata": {
        "id": "0c21b1ad-25c8-4a99-a4e0-764d568f711b"
      },
      "source": [
        "We will download the datasets from Google Drive just like we did the previous weeks using the [Pickle](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter11.03-Pickle-Files.html) format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# Download file from Google Drive\n",
        "# This file is based on data from: http://insideairbnb.com/get-the-data/\n",
        "file_id = \"1KTF77Sj0kWyft9gNT3_6k84gauPA95rG\"\n",
        "downloaded_file = \"listings.pkl\"\n",
        "# Download the files from Google Drive\n",
        "gdown.download(id=file_id, output=downloaded_file)\n",
        "\n",
        "# Show all columns (instead of cascading columns in the middle)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Don't show numbers in scientific notation\n",
        "pd.set_option(\"display.float_format\", \"{:.2f}\".format)"
      ],
      "metadata": {
        "id": "IC0UndnWK9M1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f145cc4-84d3-45fc-c598-b6e7c14ed7df"
      },
      "id": "IC0UndnWK9M1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KTF77Sj0kWyft9gNT3_6k84gauPA95rG\n",
            "To: /content/listings.pkl\n",
            "100%|██████████| 494k/494k [00:00<00:00, 67.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seed allows us to generate a random dataset split and\n",
        "# algorithms that are the same on every computer. Otherwise,\n",
        "# every time you run the split, you'd get a different dataset split.\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "ax73vOWcgkfM"
      },
      "id": "ax73vOWcgkfM",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Dataset\n",
        "Please load the downloaded file as a DataFrame (df). The method for loading these datasets is the same as what we did on the Uplimit platform."
      ],
      "metadata": {
        "id": "RNofYa-GU8D2"
      },
      "id": "RNofYa-GU8D2"
    },
    {
      "cell_type": "markdown",
      "id": "4511c023-4bc6-4399-b5bf-c4977fdf1ea0",
      "metadata": {
        "id": "4511c023-4bc6-4399-b5bf-c4977fdf1ea0"
      },
      "source": [
        "#### Task 1: Read Pickle\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/loading-inspect-dataset#corise_cladmotdl002n3b6prslqdc79)\n",
        "\n",
        "Read the Python Pickle file we've just downloaded as `df_list`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read a Python Pickle file\n",
        "df_list = pd.read_pickle(downloaded_file)"
      ],
      "metadata": {
        "id": "-nEH0subOtug"
      },
      "id": "-nEH0subOtug",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list = pd.read_pickle(\"listings.pkl\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "Hg4BjXRjOcII"
      },
      "id": "Hg4BjXRjOcII"
    },
    {
      "cell_type": "markdown",
      "id": "3f375704-9648-49cc-a6fc-a7f7208b476f",
      "metadata": {
        "tags": [],
        "id": "3f375704-9648-49cc-a6fc-a7f7208b476f"
      },
      "source": [
        "Now let's have a look at the **Listings DataFrame** and see what kinds of datapoints there are in the dataset. Show the first 2 rows."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first two rows of the dataset\n",
        "df_list.head(2)"
      ],
      "metadata": {
        "id": "JaHn5P0_O3qT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "9219505a-a0d0-496f-8ebe-93fc9f40f777"
      },
      "id": "JaHn5P0_O3qT",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  host_acceptance_rate            neighbourhood     room_type  \\\n",
              "0  23726716                  0.95  De Pijp - Rivierenbuurt  Private room   \n",
              "1  35815046                  1.00   De Baarsjes - Oud-West   Shared room   \n",
              "\n",
              "   price_in_dollar  amenities  accommodates  host_is_superhost  \\\n",
              "0           127.00         15             7              False   \n",
              "1            62.00         13             2              False   \n",
              "\n",
              "   has_availability  review_scores_rating  instant_bookable  \\\n",
              "0              True                  4.61             False   \n",
              "1              True                  4.38             False   \n",
              "\n",
              "   number_of_reviews_l30d  discount_per_5_days_booked  \\\n",
              "0                       3                        8.00   \n",
              "1                       6                        4.00   \n",
              "\n",
              "   discount_per_10_days_booked  discount_per_30_and_more_days_booked  \\\n",
              "0                        15.00                                 16.00   \n",
              "1                        10.00                                 16.00   \n",
              "\n",
              "   host_reported_average_tip service_cost  \n",
              "0                       1.03        $4.99  \n",
              "1                       1.26        $2.99  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec2d4a54-406e-437e-b9f1-ea053f6dc06c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>host_acceptance_rate</th>\n",
              "      <th>neighbourhood</th>\n",
              "      <th>room_type</th>\n",
              "      <th>price_in_dollar</th>\n",
              "      <th>amenities</th>\n",
              "      <th>accommodates</th>\n",
              "      <th>host_is_superhost</th>\n",
              "      <th>has_availability</th>\n",
              "      <th>review_scores_rating</th>\n",
              "      <th>instant_bookable</th>\n",
              "      <th>number_of_reviews_l30d</th>\n",
              "      <th>discount_per_5_days_booked</th>\n",
              "      <th>discount_per_10_days_booked</th>\n",
              "      <th>discount_per_30_and_more_days_booked</th>\n",
              "      <th>host_reported_average_tip</th>\n",
              "      <th>service_cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23726716</td>\n",
              "      <td>0.95</td>\n",
              "      <td>De Pijp - Rivierenbuurt</td>\n",
              "      <td>Private room</td>\n",
              "      <td>127.00</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>4.61</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>8.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.03</td>\n",
              "      <td>$4.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35815046</td>\n",
              "      <td>1.00</td>\n",
              "      <td>De Baarsjes - Oud-West</td>\n",
              "      <td>Shared room</td>\n",
              "      <td>62.00</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>4.38</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "      <td>4.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.26</td>\n",
              "      <td>$2.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec2d4a54-406e-437e-b9f1-ea053f6dc06c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec2d4a54-406e-437e-b9f1-ea053f6dc06c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec2d4a54-406e-437e-b9f1-ea053f6dc06c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6f0f824-8ab0-4763-8282-8feb48acdd3d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6f0f824-8ab0-4763-8282-8feb48acdd3d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6f0f824-8ab0-4763-8282-8feb48acdd3d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list.head(2)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "zT7wuJ1KOx-7"
      },
      "id": "zT7wuJ1KOx-7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome, just like last week, our next step is to get an overview of the columns that are in this particular DataFrame 🧐."
      ],
      "metadata": {
        "id": "cP2eVhs0L0Gq"
      },
      "id": "cP2eVhs0L0Gq"
    },
    {
      "cell_type": "markdown",
      "id": "38b21f27-24ea-4622-8c2a-c4bb065b5d4e",
      "metadata": {
        "id": "38b21f27-24ea-4622-8c2a-c4bb065b5d4e"
      },
      "source": [
        "#### Task 2: Print column names, types, and non-null values\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/loading-inspect-dataset#corise_clcqs8s8c00002a6lgh770s18)\n",
        "\n",
        "Let's try and get an overview of the **Listings DataFrame**, called `df_list` with the [`info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)-command. This should show us some details about the columns in the DataFrame, like the column names, their data types, and the number of non-null values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list.info()"
      ],
      "metadata": {
        "id": "fy0TfpzJO9NE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7080e1f2-b64a-44ba-b66e-5996935a2215"
      },
      "id": "fy0TfpzJO9NE",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4817 entries, 0 to 6172\n",
            "Data columns (total 17 columns):\n",
            " #   Column                                Non-Null Count  Dtype   \n",
            "---  ------                                --------------  -----   \n",
            " 0   id                                    4817 non-null   int64   \n",
            " 1   host_acceptance_rate                  4817 non-null   float64 \n",
            " 2   neighbourhood                         4817 non-null   category\n",
            " 3   room_type                             4817 non-null   category\n",
            " 4   price_in_dollar                       4817 non-null   float64 \n",
            " 5   amenities                             4817 non-null   int64   \n",
            " 6   accommodates                          4817 non-null   int64   \n",
            " 7   host_is_superhost                     4817 non-null   bool    \n",
            " 8   has_availability                      4817 non-null   bool    \n",
            " 9   review_scores_rating                  4817 non-null   float64 \n",
            " 10  instant_bookable                      4817 non-null   bool    \n",
            " 11  number_of_reviews_l30d                4817 non-null   int64   \n",
            " 12  discount_per_5_days_booked            4817 non-null   float64 \n",
            " 13  discount_per_10_days_booked           4817 non-null   float64 \n",
            " 14  discount_per_30_and_more_days_booked  4817 non-null   float64 \n",
            " 15  host_reported_average_tip             4817 non-null   float64 \n",
            " 16  service_cost                          4817 non-null   category\n",
            "dtypes: bool(3), category(3), float64(7), int64(4)\n",
            "memory usage: 480.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list.info()"
      ],
      "metadata": {
        "id": "wOKwdo8gH9Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbcaa636-61be-4e8b-e43f-7152141a55b3"
      },
      "id": "wOKwdo8gH9Th",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4817 entries, 0 to 6172\n",
            "Data columns (total 17 columns):\n",
            " #   Column                                Non-Null Count  Dtype   \n",
            "---  ------                                --------------  -----   \n",
            " 0   id                                    4817 non-null   int64   \n",
            " 1   host_acceptance_rate                  4817 non-null   float64 \n",
            " 2   neighbourhood                         4817 non-null   category\n",
            " 3   room_type                             4817 non-null   category\n",
            " 4   price_in_dollar                       4817 non-null   float64 \n",
            " 5   amenities                             4817 non-null   int64   \n",
            " 6   accommodates                          4817 non-null   int64   \n",
            " 7   host_is_superhost                     4817 non-null   bool    \n",
            " 8   has_availability                      4817 non-null   bool    \n",
            " 9   review_scores_rating                  4817 non-null   float64 \n",
            " 10  instant_bookable                      4817 non-null   bool    \n",
            " 11  number_of_reviews_l30d                4817 non-null   int64   \n",
            " 12  discount_per_5_days_booked            4817 non-null   float64 \n",
            " 13  discount_per_10_days_booked           4817 non-null   float64 \n",
            " 14  discount_per_30_and_more_days_booked  4817 non-null   float64 \n",
            " 15  host_reported_average_tip             4817 non-null   float64 \n",
            " 16  service_cost                          4817 non-null   category\n",
            "dtypes: bool(3), category(3), float64(7), int64(4)\n",
            "memory usage: 480.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list.info()\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "HKTyyRvfO8ZM"
      },
      "id": "HKTyyRvfO8ZM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like last week we have a lot of attributes, of which a few we will drop. Some require processing while others don't. For example columns with the dtype **category** and **boolean** require some processing so that it can be used by Scikit-learn algorithms since most of these algorithms work best with numerical values."
      ],
      "metadata": {
        "id": "4ml13_W2eKvm"
      },
      "id": "4ml13_W2eKvm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3: Make a selection\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/loading-inspect-dataset#corise_cladmotdl002n3b6prslqdc79)\n",
        "\n",
        "There are a total of 17 columns. For readability purposes we are going to prematurely drop a lot of these columns. In practice you often drop columns at a later stage once you've determined their value. This is something we'll look into next week. For now we want you to drop the columns:\n",
        "\n",
        "- id (it has no meaning for the ML algorithm)\n",
        "- discount_per_5_days_booked\n",
        "- discount_per_10_days_booked\n",
        "- discount_per_30_and_more_days_booked"
      ],
      "metadata": {
        "id": "fM5DSYBMP6ZV"
      },
      "id": "fM5DSYBMP6ZV"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = df_list.drop(columns=[\"id\", \"discount_per_5_days_booked\", \"discount_per_10_days_booked\", \"discount_per_30_and_more_days_booked\"])"
      ],
      "metadata": {
        "id": "RWjB_nvCO_Me"
      },
      "id": "RWjB_nvCO_Me",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list_copy = df_list.drop(columns=['id', 'discount_per_5_days_booked',\n",
        "                                     'discount_per_10_days_booked', 'discount_per_30_and_more_days_booked'])\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "Wq-cIUOAPY1b"
      },
      "id": "Wq-cIUOAPY1b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the leftover information contained in the dataframe again."
      ],
      "metadata": {
        "id": "v_h9DU2PjkVe"
      },
      "id": "v_h9DU2PjkVe"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list.info()"
      ],
      "metadata": {
        "id": "FPyNu16IPA-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b2f303-9296-46b7-f2c4-30bdc0fece93"
      },
      "id": "FPyNu16IPA-r",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4817 entries, 0 to 6172\n",
            "Data columns (total 13 columns):\n",
            " #   Column                     Non-Null Count  Dtype   \n",
            "---  ------                     --------------  -----   \n",
            " 0   host_acceptance_rate       4817 non-null   float64 \n",
            " 1   neighbourhood              4817 non-null   category\n",
            " 2   room_type                  4817 non-null   category\n",
            " 3   price_in_dollar            4817 non-null   float64 \n",
            " 4   amenities                  4817 non-null   int64   \n",
            " 5   accommodates               4817 non-null   int64   \n",
            " 6   host_is_superhost          4817 non-null   bool    \n",
            " 7   has_availability           4817 non-null   bool    \n",
            " 8   review_scores_rating       4817 non-null   float64 \n",
            " 9   instant_bookable           4817 non-null   bool    \n",
            " 10  number_of_reviews_l30d     4817 non-null   int64   \n",
            " 11  host_reported_average_tip  4817 non-null   float64 \n",
            " 12  service_cost               4817 non-null   category\n",
            "dtypes: bool(3), category(3), float64(4), int64(3)\n",
            "memory usage: 329.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list.info()\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "stZz4J3APtbQ"
      },
      "id": "stZz4J3APtbQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are left with 3 boolean values and 3 categorical values, which need to be encoded.\n",
        "\n",
        "Now to determine our target variable, we have **no** labels assigned to this target variable; which is the combination of \"price_in_dollar\" and \"host_reported_average_tip\".\n",
        "\n",
        "Just like we saw on this weeks content, we want to first cluster these two numerical values into a few groups. This creates a new categorical variable called **listing_tipping_group**, which will be used as our target for the algorithm that we are going to make.\n",
        "\n"
      ],
      "metadata": {
        "id": "0DYmBpKtkOVN"
      },
      "id": "0DYmBpKtkOVN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 4: Visualize the two numerical variables\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/one-and-more-var#corise_clctcb6ll00062a76ap82hkt8)\n",
        "\n",
        "Create a scatter plot displaying the two numerical variables: \"price_in_dollar\" and \"host_reported_average_tip\"."
      ],
      "metadata": {
        "id": "a1qLOXOjV_tw"
      },
      "id": "a1qLOXOjV_tw"
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_list, x=\"price_in_dollar\", y=\"host_reported_average_tip\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wTiQHBsxVBAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "d268ae1f-33b9-450c-d9aa-ccd755a2a700"
      },
      "id": "wTiQHBsxVBAH",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"4fa0b479-b6fd-42a4-a439-a4bc03d92afb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4fa0b479-b6fd-42a4-a439-a4bc03d92afb\")) {                    Plotly.newPlot(                        \"4fa0b479-b6fd-42a4-a439-a4bc03d92afb\",                        [{\"hovertemplate\":\"price_in_dollar=%{x}\\u003cbr\\u003ehost_reported_average_tip=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[127.0,62.0,132.0,69.0,83.0,246.0,243.0,103.0,183.0,161.0,248.0,120.0,253.0,190.0,184.0,284.0,271.0,98.0,253.0,194.0,197.0,308.0,158.0,227.0,224.0,62.0,166.0,47.0,67.0,358.0,188.0,58.0,267.0,125.0,264.0,75.0,248.0,82.0,101.0,57.0,91.0,195.0,61.0,135.0,62.0,95.0,410.0,252.0,112.0,252.0,80.0,231.0,88.0,63.0,262.0,235.0,45.0,54.0,305.0,373.0,393.0,200.0,72.0,313.0,322.0,226.0,271.0,195.0,228.0,206.0,162.0,76.0,185.0,74.0,194.0,294.0,351.0,289.0,252.0,437.0,244.0,319.0,335.0,161.0,87.0,308.0,73.0,201.0,141.0,176.0,155.0,136.0,206.0,59.0,96.0,135.0,49.0,184.0,229.0,93.0,128.0,326.0,99.0,172.0,209.0,50.0,170.0,125.0,288.0,177.0,331.0,210.0,79.0,160.0,51.0,267.0,90.0,139.0,251.0,214.0,260.0,130.0,159.0,124.0,309.0,262.0,346.0,242.0,258.0,355.0,58.0,270.0,123.0,245.0,217.0,53.0,72.0,126.0,204.0,166.0,162.0,193.0,292.0,103.0,128.0,227.0,275.0,106.0,150.0,78.0,338.0,49.0,156.0,118.0,328.0,275.0,53.0,40.0,254.0,302.0,130.0,101.0,203.0,53.0,175.0,107.0,262.0,229.0,130.0,296.0,245.0,151.0,295.0,143.0,63.0,279.0,256.0,209.0,162.0,288.0,279.0,111.0,215.0,156.0,101.0,64.0,121.0,235.0,274.0,233.0,104.0,228.0,265.0,228.0,190.0,350.0,255.0,176.0,273.0,313.0,127.0,320.0,86.0,77.0,151.0,146.0,128.0,64.0,259.0,218.0,72.0,148.0,119.0,91.0,315.0,43.0,141.0,258.0,105.0,184.0,197.0,183.0,261.0,96.0,261.0,163.0,129.0,255.0,212.0,99.0,341.0,298.0,72.0,328.0,267.0,86.0,136.0,266.0,264.0,257.0,141.0,253.0,327.0,282.0,114.0,182.0,99.0,193.0,199.0,105.0,254.0,130.0,220.0,51.0,212.0,278.0,123.0,49.0,138.0,220.0,217.0,181.0,191.0,214.0,89.0,217.0,205.0,56.0,240.0,61.0,141.0,51.0,328.0,119.0,383.0,133.0,211.0,215.0,266.0,257.0,151.0,82.0,110.0,309.0,224.0,268.0,168.0,64.0,108.0,87.0,231.0,356.0,94.0,199.0,274.0,217.0,356.0,38.0,144.0,80.0,252.0,51.0,310.0,405.0,201.0,360.0,317.0,133.0,49.0,187.0,122.0,156.0,93.0,397.0,231.0,396.0,253.0,257.0,100.0,288.0,359.0,381.0,347.0,126.0,425.0,280.0,124.0,53.0,206.0,307.0,346.0,168.0,232.0,221.0,254.0,217.0,153.0,78.0,54.0,348.0,234.0,177.0,53.0,337.0,197.0,143.0,144.0,61.0,228.0,174.0,110.0,360.0,211.0,272.0,106.0,257.0,353.0,337.0,85.0,204.0,66.0,102.0,235.0,257.0,71.0,318.0,50.0,200.0,47.0,275.0,139.0,192.0,190.0,71.0,206.0,112.0,259.0,135.0,325.0,263.0,108.0,119.0,254.0,137.0,132.0,165.0,241.0,280.0,46.0,332.0,284.0,205.0,387.0,60.0,246.0,193.0,273.0,141.0,356.0,170.0,189.0,188.0,268.0,37.0,103.0,92.0,163.0,360.0,106.0,221.0,302.0,189.0,340.0,330.0,162.0,194.0,256.0,379.0,85.0,131.0,194.0,130.0,372.0,330.0,110.0,206.0,78.0,199.0,224.0,275.0,116.0,63.0,158.0,266.0,224.0,175.0,70.0,226.0,243.0,194.0,189.0,154.0,170.0,60.0,228.0,193.0,144.0,58.0,100.0,137.0,179.0,111.0,240.0,200.0,148.0,310.0,132.0,282.0,53.0,39.0,255.0,159.0,71.0,47.0,411.0,361.0,213.0,72.0,146.0,280.0,277.0,202.0,214.0,48.0,125.0,126.0,43.0,81.0,122.0,276.0,352.0,173.0,251.0,189.0,360.0,266.0,112.0,90.0,154.0,133.0,151.0,66.0,51.0,245.0,54.0,174.0,128.0,295.0,318.0,66.0,276.0,272.0,47.0,300.0,350.0,195.0,82.0,305.0,152.0,82.0,310.0,242.0,122.0,78.0,234.0,222.0,360.0,300.0,206.0,103.0,57.0,238.0,267.0,66.0,373.0,163.0,132.0,39.0,184.0,219.0,340.0,254.0,245.0,205.0,400.0,284.0,69.0,118.0,146.0,92.0,169.0,334.0,54.0,218.0,179.0,312.0,50.0,122.0,69.0,225.0,136.0,189.0,51.0,106.0,269.0,321.0,227.0,204.0,148.0,59.0,230.0,190.0,122.0,206.0,205.0,30.0,288.0,207.0,55.0,199.0,83.0,184.0,287.0,286.0,192.0,187.0,285.0,253.0,161.0,159.0,224.0,260.0,133.0,304.0,54.0,420.0,286.0,382.0,336.0,103.0,213.0,81.0,68.0,145.0,107.0,66.0,212.0,58.0,245.0,166.0,258.0,245.0,62.0,250.0,63.0,68.0,219.0,317.0,150.0,67.0,229.0,232.0,159.0,329.0,178.0,312.0,80.0,58.0,152.0,68.0,328.0,285.0,73.0,79.0,134.0,174.0,66.0,130.0,155.0,350.0,282.0,182.0,187.0,164.0,54.0,99.0,286.0,238.0,272.0,82.0,192.0,58.0,167.0,149.0,272.0,297.0,58.0,201.0,61.0,248.0,262.0,137.0,297.0,311.0,52.0,107.0,73.0,435.0,295.0,182.0,246.0,56.0,147.0,403.0,160.0,249.0,60.0,284.0,246.0,91.0,328.0,257.0,311.0,151.0,68.0,85.0,65.0,227.0,197.0,281.0,253.0,260.0,244.0,295.0,84.0,258.0,272.0,219.0,175.0,152.0,93.0,260.0,65.0,51.0,191.0,196.0,318.0,276.0,60.0,109.0,230.0,121.0,345.0,271.0,441.0,146.0,395.0,184.0,182.0,55.0,228.0,128.0,190.0,180.0,147.0,261.0,231.0,232.0,196.0,53.0,307.0,173.0,238.0,52.0,401.0,250.0,58.0,129.0,47.0,197.0,138.0,71.0,245.0,258.0,134.0,125.0,183.0,262.0,268.0,153.0,277.0,195.0,166.0,144.0,103.0,90.0,182.0,63.0,105.0,146.0,82.0,67.0,53.0,60.0,264.0,99.0,231.0,314.0,235.0,231.0,234.0,82.0,286.0,271.0,209.0,136.0,92.0,74.0,140.0,58.0,106.0,446.0,271.0,199.0,88.0,306.0,101.0,262.0,238.0,63.0,169.0,92.0,55.0,215.0,284.0,314.0,262.0,399.0,146.0,298.0,250.0,179.0,114.0,391.0,70.0,172.0,253.0,391.0,119.0,50.0,457.0,77.0,408.0,198.0,78.0,294.0,91.0,155.0,55.0,121.0,127.0,270.0,294.0,120.0,220.0,261.0,309.0,219.0,186.0,109.0,141.0,155.0,64.0,334.0,293.0,60.0,204.0,166.0,41.0,88.0,182.0,381.0,167.0,47.0,228.0,80.0,165.0,123.0,260.0,70.0,251.0,355.0,193.0,181.0,256.0,203.0,280.0,277.0,348.0,238.0,92.0,62.0,166.0,198.0,102.0,58.0,284.0,200.0,267.0,58.0,179.0,54.0,191.0,78.0,174.0,88.0,201.0,270.0,179.0,157.0,194.0,358.0,69.0,343.0,143.0,288.0,180.0,233.0,188.0,196.0,103.0,211.0,76.0,129.0,204.0,280.0,104.0,83.0,220.0,225.0,314.0,80.0,240.0,270.0,38.0,297.0,46.0,212.0,165.0,88.0,42.0,80.0,373.0,245.0,110.0,35.0,155.0,261.0,297.0,152.0,41.0,221.0,154.0,262.0,206.0,141.0,160.0,113.0,215.0,206.0,39.0,249.0,115.0,96.0,73.0,43.0,56.0,184.0,206.0,157.0,195.0,259.0,360.0,294.0,130.0,159.0,150.0,172.0,319.0,187.0,229.0,233.0,112.0,66.0,327.0,187.0,310.0,129.0,41.0,103.0,308.0,174.0,271.0,243.0,136.0,64.0,144.0,66.0,96.0,51.0,182.0,124.0,337.0,57.0,201.0,135.0,86.0,242.0,273.0,83.0,242.0,256.0,130.0,131.0,81.0,300.0,117.0,310.0,249.0,198.0,187.0,128.0,309.0,139.0,168.0,200.0,242.0,53.0,257.0,243.0,340.0,261.0,396.0,67.0,62.0,239.0,146.0,192.0,239.0,67.0,54.0,381.0,253.0,58.0,227.0,42.0,52.0,223.0,201.0,205.0,82.0,137.0,169.0,136.0,224.0,299.0,120.0,147.0,100.0,84.0,206.0,130.0,261.0,55.0,258.0,166.0,66.0,248.0,70.0,386.0,279.0,229.0,110.0,148.0,203.0,68.0,151.0,222.0,179.0,282.0,198.0,231.0,53.0,93.0,239.0,257.0,43.0,143.0,64.0,215.0,212.0,78.0,131.0,220.0,121.0,155.0,204.0,234.0,106.0,68.0,57.0,329.0,186.0,407.0,259.0,220.0,200.0,213.0,126.0,267.0,358.0,226.0,404.0,381.0,137.0,183.0,279.0,173.0,231.0,148.0,243.0,109.0,336.0,123.0,178.0,282.0,253.0,50.0,253.0,233.0,298.0,244.0,139.0,104.0,263.0,205.0,332.0,152.0,299.0,87.0,344.0,32.0,218.0,95.0,90.0,100.0,333.0,411.0,48.0,311.0,308.0,254.0,213.0,293.0,86.0,348.0,61.0,185.0,144.0,52.0,167.0,140.0,163.0,290.0,167.0,24.0,87.0,88.0,309.0,219.0,309.0,273.0,117.0,257.0,253.0,237.0,42.0,119.0,159.0,314.0,269.0,275.0,459.0,127.0,164.0,250.0,90.0,109.0,69.0,39.0,334.0,138.0,280.0,295.0,58.0,39.0,75.0,53.0,235.0,255.0,237.0,143.0,159.0,87.0,296.0,138.0,333.0,204.0,141.0,129.0,213.0,244.0,79.0,151.0,91.0,64.0,97.0,374.0,44.0,161.0,219.0,174.0,258.0,302.0,268.0,208.0,94.0,229.0,162.0,96.0,90.0,176.0,354.0,212.0,319.0,73.0,234.0,217.0,262.0,66.0,171.0,200.0,66.0,181.0,66.0,207.0,53.0,84.0,377.0,77.0,190.0,196.0,192.0,400.0,298.0,91.0,186.0,259.0,210.0,104.0,64.0,153.0,112.0,252.0,139.0,131.0,359.0,78.0,245.0,70.0,178.0,236.0,154.0,153.0,453.0,162.0,142.0,384.0,317.0,484.0,257.0,294.0,149.0,267.0,259.0,54.0,137.0,56.0,145.0,342.0,60.0,112.0,242.0,186.0,40.0,143.0,67.0,90.0,110.0,48.0,202.0,284.0,118.0,59.0,152.0,156.0,189.0,351.0,49.0,214.0,239.0,294.0,53.0,163.0,62.0,73.0,117.0,170.0,110.0,49.0,66.0,48.0,284.0,158.0,324.0,150.0,402.0,150.0,307.0,227.0,244.0,55.0,48.0,317.0,436.0,86.0,136.0,213.0,80.0,216.0,51.0,392.0,236.0,91.0,50.0,39.0,302.0,187.0,112.0,59.0,56.0,195.0,142.0,369.0,67.0,114.0,57.0,181.0,123.0,128.0,68.0,245.0,109.0,143.0,97.0,86.0,237.0,340.0,327.0,97.0,118.0,193.0,148.0,266.0,233.0,260.0,200.0,46.0,345.0,188.0,276.0,97.0,230.0,99.0,62.0,109.0,399.0,413.0,83.0,82.0,338.0,287.0,164.0,185.0,147.0,360.0,419.0,243.0,53.0,155.0,232.0,243.0,299.0,54.0,132.0,88.0,166.0,257.0,228.0,117.0,222.0,239.0,37.0,336.0,319.0,226.0,174.0,180.0,334.0,369.0,39.0,176.0,107.0,143.0,108.0,119.0,154.0,211.0,208.0,167.0,309.0,307.0,295.0,199.0,125.0,215.0,236.0,91.0,187.0,102.0,102.0,254.0,280.0,291.0,105.0,54.0,236.0,269.0,57.0,144.0,201.0,122.0,254.0,378.0,288.0,259.0,331.0,324.0,356.0,215.0,77.0,243.0,413.0,83.0,321.0,220.0,243.0,68.0,45.0,96.0,79.0,330.0,230.0,362.0,158.0,84.0,81.0,134.0,98.0,229.0,144.0,327.0,151.0,244.0,76.0,428.0,120.0,130.0,191.0,81.0,193.0,57.0,143.0,107.0,310.0,80.0,82.0,116.0,206.0,92.0,147.0,172.0,147.0,165.0,126.0,64.0,270.0,342.0,107.0,61.0,376.0,102.0,339.0,250.0,250.0,150.0,60.0,223.0,162.0,98.0,368.0,345.0,359.0,218.0,215.0,87.0,146.0,143.0,343.0,96.0,362.0,64.0,216.0,198.0,152.0,170.0,375.0,221.0,178.0,190.0,253.0,196.0,202.0,151.0,172.0,38.0,240.0,269.0,129.0,105.0,306.0,261.0,165.0,127.0,167.0,304.0,254.0,285.0,198.0,172.0,199.0,95.0,333.0,282.0,55.0,76.0,334.0,50.0,235.0,196.0,168.0,201.0,89.0,260.0,269.0,141.0,294.0,228.0,310.0,116.0,94.0,191.0,427.0,243.0,225.0,235.0,50.0,238.0,403.0,184.0,171.0,19.0,240.0,196.0,223.0,196.0,422.0,267.0,93.0,179.0,235.0,188.0,299.0,71.0,59.0,205.0,163.0,360.0,111.0,282.0,178.0,48.0,238.0,93.0,148.0,264.0,160.0,182.0,150.0,50.0,157.0,80.0,152.0,200.0,218.0,149.0,300.0,371.0,76.0,205.0,219.0,51.0,466.0,312.0,170.0,235.0,122.0,52.0,37.0,306.0,87.0,97.0,109.0,103.0,93.0,443.0,82.0,343.0,88.0,173.0,224.0,306.0,111.0,289.0,267.0,63.0,165.0,241.0,161.0,204.0,239.0,122.0,118.0,70.0,55.0,37.0,87.0,106.0,90.0,249.0,130.0,197.0,315.0,121.0,294.0,265.0,98.0,182.0,228.0,134.0,220.0,195.0,63.0,259.0,165.0,156.0,312.0,75.0,168.0,277.0,188.0,310.0,273.0,50.0,250.0,46.0,180.0,160.0,100.0,105.0,238.0,74.0,368.0,158.0,233.0,218.0,122.0,264.0,367.0,53.0,229.0,68.0,440.0,63.0,36.0,244.0,136.0,151.0,388.0,165.0,107.0,221.0,187.0,290.0,254.0,60.0,295.0,198.0,240.0,141.0,183.0,291.0,253.0,217.0,147.0,129.0,204.0,202.0,86.0,52.0,47.0,141.0,162.0,360.0,351.0,133.0,370.0,266.0,211.0,95.0,241.0,396.0,177.0,52.0,254.0,151.0,293.0,60.0,180.0,115.0,275.0,188.0,330.0,273.0,199.0,224.0,358.0,205.0,353.0,311.0,163.0,181.0,88.0,77.0,253.0,238.0,152.0,126.0,203.0,358.0,162.0,178.0,282.0,89.0,87.0,40.0,62.0,408.0,253.0,155.0,238.0,233.0,138.0,256.0,270.0,215.0,135.0,137.0,326.0,54.0,217.0,266.0,237.0,146.0,222.0,159.0,53.0,235.0,61.0,72.0,205.0,142.0,214.0,66.0,299.0,270.0,145.0,220.0,292.0,193.0,53.0,111.0,39.0,292.0,228.0,133.0,202.0,355.0,223.0,233.0,219.0,106.0,246.0,215.0,130.0,189.0,49.0,95.0,58.0,183.0,242.0,257.0,175.0,256.0,399.0,189.0,119.0,235.0,118.0,94.0,257.0,265.0,171.0,120.0,165.0,43.0,254.0,224.0,134.0,109.0,150.0,177.0,283.0,106.0,66.0,48.0,84.0,122.0,321.0,192.0,91.0,291.0,69.0,161.0,209.0,344.0,254.0,186.0,90.0,193.0,182.0,211.0,266.0,392.0,107.0,56.0,69.0,121.0,74.0,166.0,251.0,321.0,175.0,218.0,116.0,319.0,198.0,243.0,151.0,328.0,170.0,82.0,55.0,186.0,334.0,369.0,146.0,108.0,160.0,65.0,175.0,42.0,248.0,217.0,306.0,196.0,246.0,170.0,212.0,265.0,59.0,123.0,276.0,159.0,176.0,267.0,191.0,153.0,139.0,45.0,99.0,61.0,69.0,54.0,407.0,182.0,126.0,171.0,164.0,146.0,132.0,244.0,296.0,259.0,127.0,202.0,51.0,109.0,67.0,125.0,300.0,159.0,160.0,322.0,238.0,240.0,67.0,41.0,169.0,351.0,279.0,112.0,226.0,84.0,49.0,122.0,89.0,188.0,184.0,195.0,248.0,334.0,204.0,221.0,429.0,147.0,419.0,127.0,230.0,154.0,66.0,98.0,395.0,54.0,219.0,253.0,375.0,474.0,49.0,46.0,111.0,114.0,88.0,125.0,386.0,42.0,225.0,76.0,244.0,297.0,255.0,159.0,132.0,273.0,78.0,147.0,239.0,205.0,158.0,288.0,176.0,346.0,67.0,281.0,374.0,61.0,314.0,243.0,74.0,270.0,116.0,90.0,81.0,203.0,66.0,76.0,100.0,257.0,108.0,54.0,249.0,55.0,454.0,162.0,220.0,55.0,118.0,225.0,69.0,276.0,82.0,123.0,67.0,184.0,52.0,265.0,50.0,261.0,263.0,170.0,211.0,97.0,220.0,414.0,182.0,283.0,289.0,81.0,343.0,191.0,189.0,332.0,233.0,108.0,184.0,148.0,339.0,270.0,253.0,78.0,134.0,164.0,306.0,312.0,142.0,150.0,284.0,238.0,156.0,161.0,63.0,231.0,245.0,73.0,296.0,48.0,223.0,389.0,108.0,340.0,161.0,323.0,65.0,120.0,130.0,414.0,418.0,62.0,214.0,118.0,442.0,300.0,73.0,114.0,63.0,343.0,358.0,316.0,103.0,149.0,125.0,127.0,259.0,295.0,298.0,277.0,204.0,326.0,220.0,200.0,78.0,346.0,72.0,280.0,277.0,49.0,139.0,209.0,234.0,148.0,166.0,262.0,144.0,52.0,386.0,144.0,285.0,239.0,212.0,168.0,190.0,129.0,58.0,45.0,151.0,188.0,231.0,135.0,326.0,230.0,178.0,179.0,80.0,295.0,128.0,54.0,153.0,61.0,344.0,257.0,41.0,419.0,431.0,302.0,176.0,106.0,213.0,60.0,215.0,277.0,190.0,124.0,239.0,145.0,172.0,367.0,232.0,315.0,320.0,362.0,221.0,318.0,245.0,88.0,226.0,161.0,58.0,400.0,142.0,87.0,56.0,183.0,171.0,114.0,183.0,223.0,227.0,159.0,349.0,239.0,295.0,219.0,116.0,283.0,162.0,33.0,190.0,180.0,78.0,118.0,302.0,125.0,171.0,97.0,138.0,106.0,50.0,314.0,158.0,48.0,95.0,182.0,149.0,174.0,222.0,244.0,80.0,345.0,165.0,99.0,164.0,167.0,128.0,145.0,300.0,50.0,157.0,65.0,174.0,304.0,214.0,280.0,281.0,122.0,274.0,97.0,210.0,252.0,159.0,153.0,139.0,381.0,179.0,61.0,233.0,255.0,314.0,231.0,217.0,146.0,55.0,387.0,261.0,111.0,62.0,302.0,147.0,271.0,163.0,94.0,211.0,68.0,249.0,52.0,213.0,398.0,328.0,211.0,159.0,315.0,217.0,41.0,55.0,190.0,319.0,300.0,58.0,168.0,133.0,290.0,108.0,43.0,145.0,235.0,166.0,43.0,359.0,43.0,53.0,390.0,182.0,50.0,38.0,51.0,169.0,63.0,184.0,183.0,114.0,222.0,270.0,152.0,83.0,329.0,275.0,91.0,248.0,76.0,254.0,129.0,141.0,118.0,41.0,103.0,222.0,298.0,134.0,53.0,154.0,73.0,226.0,132.0,295.0,60.0,101.0,170.0,192.0,410.0,113.0,153.0,202.0,123.0,88.0,170.0,181.0,125.0,36.0,284.0,57.0,59.0,52.0,55.0,114.0,412.0,162.0,400.0,207.0,267.0,194.0,308.0,114.0,320.0,96.0,99.0,118.0,193.0,261.0,256.0,43.0,312.0,176.0,406.0,313.0,90.0,25.0,60.0,101.0,161.0,329.0,50.0,77.0,240.0,130.0,153.0,77.0,102.0,382.0,164.0,144.0,275.0,154.0,167.0,290.0,248.0,182.0,150.0,96.0,169.0,328.0,373.0,91.0,111.0,191.0,76.0,172.0,282.0,70.0,280.0,74.0,372.0,349.0,143.0,216.0,59.0,221.0,102.0,78.0,113.0,46.0,198.0,278.0,312.0,179.0,221.0,278.0,239.0,309.0,42.0,177.0,126.0,202.0,199.0,55.0,253.0,78.0,341.0,241.0,333.0,111.0,248.0,71.0,154.0,190.0,294.0,127.0,176.0,237.0,238.0,271.0,251.0,143.0,81.0,158.0,245.0,274.0,335.0,380.0,307.0,310.0,190.0,105.0,146.0,177.0,110.0,220.0,54.0,244.0,97.0,298.0,155.0,200.0,425.0,43.0,194.0,171.0,92.0,139.0,322.0,323.0,46.0,339.0,102.0,206.0,90.0,115.0,194.0,118.0,125.0,301.0,194.0,52.0,289.0,128.0,217.0,101.0,332.0,151.0,444.0,140.0,123.0,87.0,138.0,232.0,134.0,235.0,248.0,202.0,197.0,29.0,162.0,200.0,74.0,158.0,120.0,107.0,226.0,370.0,164.0,132.0,55.0,330.0,134.0,243.0,254.0,210.0,59.0,240.0,196.0,383.0,57.0,68.0,281.0,44.0,169.0,142.0,105.0,121.0,50.0,189.0,59.0,205.0,75.0,123.0,72.0,195.0,274.0,147.0,284.0,364.0,175.0,101.0,52.0,195.0,50.0,189.0,218.0,92.0,320.0,164.0,246.0,294.0,135.0,97.0,134.0,211.0,313.0,226.0,161.0,146.0,235.0,304.0,172.0,98.0,82.0,166.0,91.0,63.0,302.0,90.0,288.0,171.0,127.0,160.0,144.0,97.0,186.0,107.0,123.0,289.0,111.0,210.0,86.0,95.0,98.0,166.0,78.0,123.0,238.0,74.0,135.0,222.0,55.0,363.0,183.0,246.0,381.0,184.0,174.0,168.0,202.0,107.0,189.0,67.0,304.0,96.0,254.0,99.0,438.0,78.0,201.0,289.0,37.0,116.0,90.0,90.0,112.0,217.0,276.0,71.0,210.0,297.0,53.0,83.0,40.0,140.0,165.0,249.0,314.0,223.0,63.0,189.0,239.0,56.0,74.0,350.0,127.0,82.0,51.0,59.0,309.0,201.0,153.0,100.0,277.0,53.0,156.0,365.0,214.0,72.0,251.0,176.0,237.0,36.0,188.0,122.0,93.0,88.0,243.0,239.0,85.0,63.0,307.0,262.0,245.0,287.0,57.0,274.0,167.0,81.0,296.0,302.0,99.0,113.0,317.0,55.0,502.0,215.0,139.0,58.0,312.0,170.0,94.0,289.0,282.0,69.0,79.0,310.0,305.0,203.0,297.0,104.0,54.0,336.0,275.0,80.0,244.0,140.0,71.0,104.0,127.0,62.0,261.0,216.0,171.0,294.0,202.0,200.0,366.0,112.0,210.0,61.0,189.0,94.0,76.0,64.0,73.0,208.0,172.0,172.0,293.0,122.0,219.0,102.0,140.0,146.0,195.0,317.0,123.0,186.0,383.0,265.0,242.0,134.0,122.0,204.0,46.0,306.0,101.0,259.0,53.0,177.0,47.0,314.0,360.0,144.0,229.0,238.0,262.0,248.0,51.0,314.0,129.0,205.0,133.0,111.0,77.0,181.0,57.0,292.0,72.0,136.0,324.0,132.0,443.0,52.0,273.0,196.0,125.0,242.0,146.0,279.0,288.0,56.0,115.0,326.0,224.0,140.0,258.0,320.0,75.0,287.0,308.0,158.0,328.0,106.0,385.0,307.0,363.0,42.0,216.0,239.0,255.0,315.0,373.0,134.0,243.0,156.0,308.0,330.0,224.0,46.0,285.0,166.0,150.0,475.0,156.0,107.0,233.0,285.0,68.0,343.0,98.0,154.0,399.0,313.0,289.0,250.0,120.0,188.0,89.0,53.0,41.0,107.0,97.0,49.0,57.0,156.0,120.0,82.0,111.0,212.0,297.0,312.0,250.0,135.0,137.0,336.0,139.0,292.0,235.0,349.0,217.0,81.0,308.0,90.0,239.0,289.0,220.0,163.0,117.0,52.0,84.0,353.0,46.0,102.0,371.0,58.0,293.0,165.0,125.0,279.0,57.0,56.0,187.0,453.0,77.0,45.0,88.0,173.0,298.0,59.0,42.0,157.0,134.0,243.0,166.0,39.0,329.0,216.0,159.0,200.0,193.0,223.0,288.0,145.0,279.0,219.0,222.0,222.0,100.0,270.0,262.0,82.0,335.0,255.0,66.0,262.0,54.0,161.0,230.0,259.0,126.0,167.0,318.0,143.0,121.0,273.0,225.0,187.0,222.0,80.0,267.0,176.0,309.0,408.0,113.0,140.0,66.0,159.0,264.0,313.0,300.0,73.0,111.0,142.0,118.0,215.0,172.0,30.0,154.0,161.0,189.0,205.0,152.0,45.0,241.0,94.0,342.0,172.0,259.0,197.0,267.0,214.0,459.0,178.0,186.0,142.0,63.0,147.0,252.0,169.0,86.0,51.0,147.0,84.0,205.0,238.0,145.0,252.0,293.0,297.0,94.0,206.0,51.0,261.0,118.0,56.0,284.0,118.0,276.0,239.0,151.0,270.0,186.0,249.0,170.0,178.0,311.0,283.0,268.0,118.0,415.0,211.0,386.0,274.0,71.0,140.0,185.0,50.0,79.0,138.0,257.0,276.0,117.0,100.0,236.0,354.0,180.0,66.0,117.0,393.0,183.0,207.0,190.0,400.0,133.0,358.0,86.0,298.0,166.0,60.0,267.0,313.0,61.0,91.0,46.0,57.0,83.0,232.0,216.0,336.0,121.0,202.0,187.0,129.0,52.0,210.0,309.0,99.0,217.0,319.0,177.0,73.0,244.0,54.0,381.0,208.0,268.0,270.0,48.0,406.0,85.0,408.0,128.0,163.0,278.0,80.0,159.0,195.0,80.0,178.0,160.0,355.0,77.0,231.0,88.0,276.0,361.0,106.0,81.0,217.0,250.0,230.0,222.0,216.0,115.0,258.0,269.0,274.0,107.0,111.0,307.0,257.0,175.0,270.0,124.0,67.0,118.0,175.0,245.0,351.0,272.0,216.0,94.0,302.0,238.0,78.0,113.0,181.0,220.0,381.0,221.0,109.0,94.0,269.0,154.0,192.0,302.0,111.0,108.0,305.0,79.0,159.0,186.0,33.0,269.0,163.0,190.0,232.0,214.0,180.0,270.0,105.0,197.0,254.0,332.0,229.0,50.0,89.0,239.0,201.0,141.0,98.0,154.0,48.0,240.0,259.0,173.0,105.0,366.0,262.0,54.0,298.0,66.0,106.0,53.0,224.0,74.0,190.0,261.0,425.0,58.0,142.0,96.0,292.0,184.0,175.0,120.0,68.0,161.0,62.0,275.0,215.0,200.0,262.0,333.0,281.0,53.0,260.0,321.0,369.0,266.0,182.0,77.0,321.0,287.0,66.0,159.0,118.0,321.0,300.0,103.0,207.0,344.0,87.0,115.0,181.0,84.0,196.0,274.0,60.0,130.0,391.0,98.0,324.0,306.0,174.0,45.0,127.0,153.0,230.0,191.0,212.0,95.0,238.0,53.0,200.0,113.0,302.0,257.0,275.0,343.0,119.0,317.0,237.0,204.0,46.0,119.0,145.0,228.0,62.0,418.0,210.0,197.0,264.0,136.0,168.0,106.0,429.0,370.0,128.0,321.0,209.0,185.0,262.0,43.0,152.0,138.0,264.0,119.0,247.0,374.0,273.0,76.0,386.0,314.0,243.0,144.0,137.0,302.0,152.0,153.0,164.0,89.0,160.0,235.0,237.0,190.0,190.0,355.0,254.0,339.0,109.0,301.0,194.0,138.0,112.0,171.0,75.0,318.0,305.0,257.0,299.0,237.0,213.0,221.0,199.0,66.0,135.0,217.0,192.0,87.0,157.0,87.0,243.0,165.0,60.0,77.0,46.0,144.0,58.0,398.0,130.0,149.0,187.0,234.0,78.0,198.0,147.0,117.0,41.0,277.0,191.0,85.0,76.0,178.0,158.0,199.0,196.0,142.0,162.0,165.0,171.0,83.0,66.0,43.0,69.0,202.0,267.0,59.0,155.0,356.0,426.0,206.0,182.0,426.0,98.0,348.0,146.0,162.0,135.0,176.0,416.0,181.0,92.0,146.0,302.0,192.0,262.0,269.0,288.0,356.0,227.0,96.0,194.0,162.0,196.0,53.0,287.0,234.0,114.0,267.0,113.0,162.0,159.0,354.0,253.0,183.0,191.0,262.0,240.0,217.0,43.0,185.0,80.0,51.0,266.0,287.0,170.0,195.0,47.0,102.0,226.0,61.0,282.0,55.0,55.0,114.0,317.0,450.0,140.0,203.0,378.0,328.0,271.0,79.0,114.0,186.0,220.0,145.0,144.0,436.0,185.0,205.0,279.0,182.0,116.0,165.0,108.0,248.0,225.0,246.0,254.0,72.0,118.0,405.0,47.0,176.0,258.0,46.0,335.0,223.0,236.0,342.0,305.0,170.0,126.0,125.0,142.0,167.0,223.0,47.0,98.0,63.0,277.0,238.0,230.0,199.0,96.0,65.0,47.0,342.0,443.0,98.0,462.0,280.0,312.0,58.0,244.0,252.0,74.0,160.0,193.0,372.0,225.0,64.0,93.0,181.0,162.0,138.0,76.0,71.0,74.0,347.0,392.0,233.0,75.0,117.0,194.0,367.0,282.0,181.0,271.0,55.0,76.0,243.0,253.0,238.0,181.0,129.0,225.0,53.0,43.0,116.0,194.0,289.0,43.0,447.0,134.0,58.0,80.0,237.0,206.0,135.0,90.0,165.0,159.0,127.0,294.0,207.0,179.0,222.0,169.0,270.0,160.0,111.0,250.0,362.0,78.0,196.0,136.0,294.0,233.0,181.0,47.0,46.0,314.0,238.0,251.0,125.0,123.0,185.0,90.0,177.0,87.0,167.0,57.0,72.0,227.0,401.0,175.0,291.0,288.0,118.0,64.0,190.0,192.0,392.0,112.0,169.0,220.0,117.0,58.0,273.0,275.0,136.0,62.0,285.0,308.0,50.0,66.0,299.0,177.0,345.0,302.0,73.0,298.0,349.0,190.0,148.0,117.0,109.0,53.0,210.0,278.0,270.0,345.0,61.0,257.0,182.0,92.0,332.0,46.0,112.0,293.0,100.0,222.0,130.0,198.0,199.0,190.0,399.0,152.0,84.0,161.0,211.0,221.0,199.0,82.0,288.0,62.0,323.0,72.0,99.0,253.0,390.0,180.0,448.0,65.0,279.0,306.0,193.0,253.0,146.0,191.0,72.0,76.0,187.0,189.0,119.0,111.0,173.0,150.0,166.0,45.0,324.0,278.0,92.0,275.0,199.0,131.0,277.0,311.0,296.0,267.0,50.0,47.0,180.0,351.0,182.0,49.0,239.0,424.0,145.0,305.0,157.0,85.0,355.0,467.0,238.0,258.0,146.0,58.0,237.0,198.0,256.0,277.0,76.0,158.0,355.0,142.0,171.0,180.0,218.0,80.0,148.0,213.0,267.0,119.0,49.0,262.0,306.0,401.0,199.0,249.0,257.0,285.0,231.0,189.0,59.0,294.0,58.0,315.0,221.0,194.0,293.0,198.0,175.0,260.0,178.0,83.0,195.0,220.0,199.0,152.0,244.0,317.0,54.0,290.0,226.0,139.0,397.0,166.0,81.0,119.0,207.0,258.0,155.0,93.0,58.0,110.0,186.0,306.0,299.0,131.0,219.0,55.0,342.0,212.0,174.0,271.0,53.0,371.0,214.0,94.0,158.0,301.0,191.0,269.0,102.0,243.0,181.0,76.0,198.0,444.0,49.0,121.0,63.0,48.0,141.0,250.0,314.0,254.0,275.0,150.0,282.0,98.0,122.0,89.0,93.0,354.0,244.0,174.0,110.0,259.0,71.0,437.0,67.0,164.0,126.0,197.0,255.0,81.0,170.0,269.0,46.0,138.0,190.0,242.0,58.0,218.0,215.0,343.0,144.0,76.0,241.0,228.0,177.0,195.0,231.0,273.0,394.0,324.0,332.0,158.0,201.0,141.0,321.0,299.0,256.0,265.0,324.0,273.0,184.0,106.0,55.0,59.0,191.0,240.0,223.0,210.0,269.0,194.0,57.0,116.0,46.0,339.0,250.0,91.0,222.0,52.0,406.0,56.0,53.0,228.0,87.0,219.0,172.0,98.0,177.0,275.0,210.0,429.0,197.0,100.0,221.0,83.0,201.0,306.0,48.0,74.0,131.0,56.0,99.0,248.0,314.0,125.0,271.0,215.0,330.0,273.0,251.0,294.0,217.0,197.0,146.0,121.0,81.0,130.0,320.0,240.0,105.0,200.0,200.0,307.0,206.0,284.0,69.0,130.0,189.0,252.0,104.0,228.0,251.0,166.0,83.0,245.0,297.0,264.0,66.0,61.0,61.0,168.0,131.0,215.0,176.0,74.0,66.0,273.0,341.0,261.0,39.0,200.0,144.0,251.0,276.0,274.0,292.0,61.0,179.0,122.0,223.0,352.0,126.0,187.0,146.0,74.0,56.0,125.0,488.0,164.0,219.0,202.0,82.0,108.0,74.0,46.0,195.0,437.0,117.0,196.0,253.0,200.0,62.0,94.0,334.0,228.0,96.0,190.0,181.0,56.0,213.0,191.0,172.0,183.0,241.0,254.0,47.0,52.0,61.0,111.0,103.0,292.0,327.0,43.0,113.0,414.0,201.0,73.0,379.0,49.0,116.0,258.0,200.0,104.0,198.0,127.0,201.0,267.0,318.0,377.0,267.0,182.0,136.0,141.0,244.0,209.0,86.0,100.0,266.0,303.0,225.0,148.0,231.0,227.0,240.0,263.0,130.0,69.0,363.0,78.0,274.0,280.0,244.0,268.0,314.0,119.0,414.0,49.0,324.0,299.0,249.0,276.0,336.0,352.0,42.0,102.0,330.0,319.0,468.0,383.0,259.0,277.0,166.0,49.0,206.0,322.0,172.0,314.0,227.0,127.0,215.0,182.0,293.0,273.0,60.0,309.0,281.0,162.0,258.0,324.0,90.0,77.0,119.0,281.0,175.0,265.0,136.0,72.0,367.0,191.0,70.0,234.0,291.0,112.0,120.0,244.0,144.0,160.0,97.0,175.0,127.0,145.0,94.0,168.0,28.0,74.0,62.0,107.0,237.0,224.0,87.0,250.0,288.0,378.0,140.0,233.0,401.0,237.0,279.0,215.0,195.0,253.0,114.0,171.0,239.0,207.0,183.0,88.0,90.0,244.0,58.0,444.0,79.0,371.0,193.0,121.0,323.0,244.0,129.0,315.0,165.0,88.0,187.0,229.0,116.0,210.0,47.0,88.0,186.0,122.0,209.0,56.0,53.0,234.0,326.0,199.0,79.0,160.0,137.0,132.0,162.0,427.0,212.0,189.0,66.0,59.0,241.0,168.0,299.0,81.0,114.0,166.0,87.0,261.0,117.0,268.0,125.0,293.0,131.0,39.0,310.0,297.0,280.0,305.0,197.0,41.0,430.0,255.0,190.0,46.0,94.0,83.0,216.0,37.0,155.0,323.0,83.0,319.0,203.0,150.0,300.0,232.0,251.0,50.0,372.0,260.0,115.0,338.0,157.0,240.0,362.0,144.0,113.0,187.0,92.0,45.0,111.0,182.0,140.0,92.0,292.0,238.0,74.0,219.0,167.0,243.0,67.0,160.0,323.0,251.0,178.0,81.0,309.0,322.0,87.0,256.0,96.0,235.0,315.0,243.0,166.0,221.0,111.0,54.0,263.0,51.0,80.0,66.0,103.0,53.0,219.0,306.0,255.0,184.0,254.0,74.0,239.0,290.0,247.0,75.0,322.0,247.0,128.0,222.0,36.0,195.0,52.0,46.0,166.0,194.0,60.0,467.0,238.0,193.0,284.0,92.0,202.0,264.0,144.0,249.0,48.0,202.0,383.0,122.0,294.0,60.0,346.0,76.0,274.0,306.0,124.0,316.0,81.0,237.0,323.0,258.0,269.0,135.0,66.0,255.0,288.0,89.0,221.0,214.0,57.0,166.0,218.0,304.0,109.0,82.0,201.0,280.0,189.0,127.0,343.0,72.0,307.0,66.0,68.0,137.0,224.0,193.0,282.0,85.0,56.0,225.0,301.0,233.0,237.0,130.0,267.0,492.0,278.0,169.0,196.0,135.0,369.0,93.0,253.0,49.0,157.0,231.0,408.0,207.0,64.0,71.0,65.0,230.0,196.0,208.0,253.0,60.0,179.0,359.0,72.0,179.0,242.0,107.0,94.0,115.0,299.0,199.0,269.0,48.0,226.0,124.0,64.0,103.0,190.0,129.0,217.0,73.0,140.0,180.0,167.0,39.0,93.0,327.0,242.0,360.0,130.0,118.0,105.0,214.0,161.0,41.0,60.0,114.0,86.0,222.0,43.0,118.0,189.0,88.0,140.0,234.0,147.0,175.0,266.0,277.0,194.0,181.0,209.0,279.0,74.0,267.0,192.0,138.0,162.0,77.0,211.0,296.0,185.0,270.0,389.0,67.0,289.0,130.0,140.0,322.0,382.0,51.0,69.0,127.0,193.0,257.0,236.0,173.0,255.0,65.0,254.0,324.0,355.0,134.0,266.0,248.0,231.0,212.0,236.0,399.0,138.0,250.0,65.0,39.0,92.0,126.0,130.0,175.0,179.0,81.0,116.0,136.0,203.0,190.0,75.0,92.0,367.0,42.0,53.0,340.0,66.0,362.0,41.0,127.0,424.0,85.0,48.0,75.0,198.0,275.0,245.0,191.0,89.0,240.0,151.0,151.0,133.0,314.0,92.0,481.0,274.0,226.0,94.0,353.0,135.0,140.0,55.0,115.0,139.0,178.0,267.0,264.0,271.0,275.0,79.0,110.0,56.0,258.0,374.0,115.0,306.0,156.0,251.0,108.0,230.0,125.0,75.0,92.0,207.0,131.0,58.0,88.0,124.0,332.0,223.0,300.0,55.0,76.0,266.0,243.0,46.0,51.0,54.0,430.0,239.0,139.0,224.0,297.0,195.0,261.0,153.0,119.0,47.0,197.0,237.0,214.0,70.0,324.0,309.0,51.0,169.0,210.0,82.0,242.0,47.0,160.0,254.0,200.0,227.0,213.0,382.0,96.0,250.0,288.0,240.0,259.0,118.0,317.0,105.0,192.0,84.0,241.0,95.0,436.0,282.0,405.0,271.0,305.0,119.0,139.0,133.0,394.0,117.0,151.0,77.0,59.0,112.0,105.0,53.0,184.0,169.0,78.0,312.0,255.0,134.0,255.0,283.0,135.0,227.0,217.0,214.0,233.0,350.0,259.0,51.0,225.0,110.0,427.0,80.0,115.0,377.0,186.0,118.0,232.0,229.0,218.0,129.0,174.0,70.0,157.0,224.0,220.0,271.0,49.0,65.0,47.0,130.0,150.0,66.0,291.0,53.0,248.0,287.0,94.0,145.0,93.0,108.0,273.0,79.0,251.0,150.0,272.0,99.0,241.0,173.0,101.0,266.0,258.0,305.0,198.0,147.0,446.0,42.0,195.0,220.0,371.0,45.0,286.0,237.0,129.0,202.0,234.0,284.0,117.0,250.0,104.0,112.0,230.0,172.0,322.0,74.0,323.0,83.0,320.0,332.0,132.0,360.0,202.0,126.0,147.0,151.0,120.0,252.0,90.0,51.0,268.0,371.0,221.0,48.0,300.0,125.0,315.0,200.0,299.0,71.0,58.0,150.0,235.0,73.0,429.0,363.0,162.0,70.0,105.0,158.0,210.0,59.0,88.0,148.0,166.0,248.0,322.0,62.0,63.0,258.0,160.0,200.0,226.0,309.0,63.0,208.0,242.0,212.0,225.0,284.0,207.0,140.0,56.0,261.0,128.0,148.0,30.0,135.0,229.0,112.0,394.0,127.0,55.0,295.0,181.0,250.0,225.0,101.0,447.0,264.0,237.0,137.0,293.0,258.0,90.0,138.0,198.0,268.0,66.0,271.0,262.0,241.0,41.0,218.0,239.0,94.0,139.0,372.0,100.0,59.0,232.0,298.0,147.0,200.0,144.0,83.0,126.0,222.0,348.0,66.0,240.0,112.0,214.0,213.0,319.0,211.0,136.0,223.0,213.0,61.0,316.0,306.0,138.0,65.0,325.0,135.0,162.0,387.0,291.0,151.0,147.0,259.0,288.0,163.0,310.0,415.0,107.0,78.0,109.0,55.0,72.0,248.0,69.0,89.0,48.0,54.0,144.0,62.0,301.0,282.0,216.0,361.0,124.0,429.0,202.0,62.0,173.0,113.0,139.0,245.0,311.0,264.0,307.0,208.0,141.0,226.0,75.0,315.0,50.0,81.0,156.0,168.0,82.0,294.0,239.0,268.0,181.0,161.0,55.0,232.0,190.0,71.0,178.0,185.0,64.0,116.0,94.0,238.0,58.0,160.0,331.0,365.0,133.0,285.0,128.0,186.0,102.0,305.0,211.0,371.0,307.0,290.0,165.0,396.0,143.0,56.0,212.0,255.0,64.0,295.0,102.0,138.0,176.0,248.0,314.0,213.0,252.0,241.0,206.0,70.0,218.0,332.0,230.0,237.0,66.0,262.0,188.0,139.0,150.0,221.0,164.0,124.0,254.0,188.0,415.0,70.0,310.0,231.0,121.0,245.0,173.0,228.0,305.0,274.0,285.0,237.0,52.0,269.0,112.0,218.0,100.0,397.0,268.0,195.0,177.0,279.0,166.0,222.0,173.0,97.0,293.0,174.0,190.0,96.0,136.0,314.0,171.0,204.0,108.0,270.0,77.0,283.0,386.0,139.0,223.0,172.0,122.0,314.0,178.0,178.0,77.0,90.0,57.0,152.0,63.0,133.0,291.0,267.0,142.0,261.0,343.0],\"xaxis\":\"x\",\"y\":[1.03,1.26,9.98,0.6,0.8,16.63,13.76,0.27,13.62,11.45,14.94,17.18,12.62,13.9,15.71,27.68,30.84,16.48,0.0,15.1,16.4,25.97,15.26,31.74,15.5,1.6,12.85,1.29,0.13,28.4,18.13,1.39,32.16,16.47,29.46,1.94,13.76,0.98,13.67,1.26,0.47,9.82,1.8,16.74,0.75,0.5,31.78,16.38,0.01,10.91,1.03,31.18,0.0,0.88,33.23,10.43,0.55,0.39,28.67,34.29,29.4,14.94,0.39,29.5,31.29,15.92,29.06,13.34,12.57,15.68,12.58,0.37,14.44,1.33,15.46,29.84,29.73,31.51,13.51,27.3,12.18,28.26,32.89,13.91,14.38,30.76,0.54,13.62,18.47,17.42,13.08,14.18,13.02,2.67,0.5,10.43,1.67,13.94,27.62,12.54,13.25,30.2,3.25,14.6,16.95,0.65,15.21,0.53,30.07,13.24,30.82,16.55,0.7,14.52,0.07,27.97,11.38,11.57,12.05,12.0,14.42,2.11,12.7,14.84,29.39,32.62,30.25,31.17,14.1,31.96,0.81,27.66,2.28,11.29,17.03,0.45,1.35,0.56,16.9,14.45,11.47,15.28,33.0,0.58,1.23,29.79,29.37,14.07,12.06,0.0,31.02,1.58,13.38,12.29,30.59,26.82,0.0,0.15,14.96,24.19,0.16,13.45,10.38,0.34,10.9,12.79,26.32,9.41,0.85,30.79,13.37,14.74,29.47,10.88,0.41,28.41,30.7,11.37,12.25,30.23,28.75,14.46,14.54,15.58,9.92,0.73,2.13,14.63,29.85,27.02,0.34,15.66,31.16,9.44,16.92,31.19,13.85,13.78,26.39,29.68,17.18,25.99,0.75,2.76,14.38,16.9,15.2,0.41,14.8,16.31,0.0,13.15,0.09,1.27,31.98,1.0,16.21,28.52,1.85,12.13,17.46,15.28,31.62,15.49,33.06,12.27,0.59,16.16,13.48,13.3,31.06,31.1,1.06,29.12,31.38,14.35,15.35,31.33,32.9,29.03,11.06,17.73,28.48,30.39,13.73,15.1,16.34,16.48,14.26,1.11,32.82,0.91,15.83,0.29,30.63,31.25,19.29,1.2,14.83,32.69,13.08,17.35,10.97,29.1,14.83,13.27,12.26,2.2,13.07,1.08,14.42,1.16,30.64,1.37,34.48,16.2,33.1,14.68,29.87,13.09,14.09,16.06,0.77,30.66,29.57,29.72,0.0,0.33,1.31,2.48,14.93,29.93,12.21,12.11,29.26,12.5,28.62,1.65,0.0,0.77,16.19,2.18,32.16,30.45,14.3,32.82,33.0,13.58,1.92,14.89,2.22,12.23,0.88,33.09,14.31,32.28,24.87,12.94,0.0,31.61,31.83,29.18,31.27,14.49,32.81,28.07,1.22,2.82,13.2,29.41,32.34,14.44,11.87,15.64,14.43,12.92,14.17,2.74,0.94,33.09,16.47,11.48,0.41,28.74,12.37,14.52,10.87,0.18,13.81,15.79,0.95,28.79,14.66,26.32,14.94,31.71,31.48,28.41,0.91,13.63,1.43,0.8,10.85,12.99,0.34,28.6,0.75,15.03,0.79,30.94,12.23,11.24,14.3,0.49,11.67,0.44,31.79,14.21,28.24,30.29,0.8,16.94,14.2,14.07,15.81,14.02,31.86,29.57,0.43,30.35,35.62,14.74,29.79,0.65,13.55,13.51,27.98,12.68,27.49,17.78,12.41,13.17,28.75,0.03,1.59,13.49,15.39,28.94,1.37,33.63,28.53,14.22,28.18,29.42,12.39,14.23,16.44,32.37,1.41,13.42,14.91,0.0,30.74,31.44,1.96,13.43,0.0,16.3,15.67,31.93,1.1,1.43,14.24,29.82,12.97,13.15,1.6,14.02,25.75,16.09,14.97,17.24,15.47,1.73,12.67,14.19,12.0,0.0,0.19,15.32,17.23,0.0,28.37,14.65,12.51,30.59,16.85,31.46,2.51,0.3,14.64,14.74,0.67,1.78,30.34,26.97,25.96,0.69,13.86,32.47,27.85,15.51,28.31,2.42,16.74,0.37,0.48,3.45,0.0,30.52,29.24,17.36,28.55,15.29,30.35,28.69,0.58,0.81,12.05,10.16,14.06,1.38,0.02,27.93,0.04,14.43,0.8,27.81,27.44,1.3,26.2,31.64,0.76,28.98,29.97,15.73,1.56,26.49,14.28,0.38,27.45,28.66,14.64,2.37,29.86,13.61,29.93,29.26,11.76,13.51,0.81,14.31,31.91,1.13,31.61,15.12,17.54,2.76,14.94,14.28,32.48,13.5,28.14,12.01,28.18,28.67,0.47,10.42,16.65,1.98,15.01,30.98,0.06,32.07,0.0,29.97,0.37,0.84,0.48,28.63,14.71,12.2,2.63,1.32,30.45,29.68,30.5,15.65,12.93,0.0,0.0,14.56,1.7,10.93,18.14,3.99,31.07,12.08,1.23,12.93,0.72,16.4,27.22,27.08,14.07,11.79,30.96,13.58,14.89,16.68,13.91,10.08,13.16,30.44,1.23,30.11,29.59,29.37,27.28,1.98,15.6,1.02,1.33,13.61,0.72,0.0,16.73,0.91,13.2,13.93,28.57,10.68,2.47,17.22,0.53,1.31,14.54,29.7,15.56,0.84,16.59,21.1,12.2,31.8,15.66,32.02,1.2,0.11,14.47,0.0,32.55,33.61,1.2,0.97,16.4,13.63,1.93,15.36,16.52,34.03,31.02,14.79,13.83,14.91,0.71,2.04,30.89,12.86,34.46,15.38,16.13,2.42,0.0,12.21,26.15,29.48,1.39,14.58,1.14,27.09,28.77,13.99,32.92,30.0,1.27,15.02,1.85,31.15,30.21,13.68,11.04,0.9,17.27,31.55,14.69,17.3,2.21,29.93,15.25,2.1,28.52,15.42,31.99,10.47,1.13,0.6,2.31,17.22,17.66,31.96,13.98,12.15,14.86,30.39,0.07,10.44,32.35,11.8,13.86,13.78,0.42,14.52,0.14,0.48,13.2,15.01,28.5,30.03,2.12,13.35,11.84,15.27,31.15,29.49,30.95,12.21,31.58,13.13,9.88,1.53,14.24,1.2,14.67,8.56,14.01,31.24,15.52,14.33,13.8,1.48,27.39,14.19,12.42,2.3,29.7,15.51,0.58,0.0,1.48,11.86,19.31,1.51,13.62,29.3,13.18,0.82,13.51,28.36,30.15,11.5,32.87,13.23,16.02,14.48,15.46,0.2,12.51,0.43,0.37,18.14,17.27,0.5,1.69,1.45,26.8,2.68,16.69,28.56,15.93,30.46,28.69,0.68,30.31,29.96,16.28,17.24,0.0,1.88,0.0,0.06,1.9,31.4,27.4,17.17,1.09,27.66,16.24,26.64,15.4,2.19,13.31,1.06,1.43,16.48,32.38,32.72,28.36,33.19,15.65,26.2,10.96,12.14,1.88,26.43,1.79,15.08,14.61,31.63,0.68,0.02,25.22,1.64,30.16,14.49,1.55,31.85,2.72,12.83,2.29,1.0,0.6,28.93,31.78,0.8,12.59,29.5,29.65,12.5,13.88,0.9,0.0,10.74,1.05,27.74,29.45,0.0,16.04,13.55,2.53,0.07,9.91,31.44,15.28,1.92,13.9,3.11,13.27,10.61,29.86,0.36,14.91,30.28,11.44,14.88,13.15,13.59,28.25,27.88,34.97,10.76,0.41,1.86,11.93,14.83,2.18,0.75,25.9,11.86,31.53,1.13,12.69,0.28,12.24,1.76,14.45,0.0,14.82,31.95,16.06,16.34,11.01,31.49,0.4,32.43,14.87,25.79,13.32,12.57,14.56,14.83,10.44,9.31,1.06,0.73,16.79,28.93,1.86,0.52,26.49,30.56,28.0,0.07,13.81,28.22,0.95,28.16,2.09,15.35,15.32,0.82,2.82,1.66,31.11,13.96,0.71,0.94,13.95,30.97,30.86,14.54,1.49,17.47,9.81,29.67,10.56,12.12,10.82,17.41,13.58,12.58,2.23,14.3,13.86,0.0,3.17,2.18,1.9,13.99,19.11,17.11,15.4,12.74,29.0,30.24,1.79,13.39,10.76,13.72,28.77,13.59,15.33,13.5,14.64,2.19,30.38,13.83,29.82,0.29,1.99,0.96,29.13,13.38,29.8,12.99,14.98,2.18,14.54,2.54,1.41,0.22,15.24,13.34,33.36,0.18,12.9,12.62,15.15,16.6,29.85,14.54,11.62,16.31,0.24,13.77,0.0,33.79,2.27,28.14,10.16,15.62,12.93,15.64,28.56,14.43,13.49,14.47,14.68,1.03,32.86,11.06,33.33,31.8,30.7,0.84,0.47,14.63,14.8,16.13,14.42,0.6,1.21,32.08,30.24,2.5,11.23,4.1,1.7,15.63,14.22,19.44,0.27,13.62,11.96,14.7,16.56,31.39,15.61,15.01,19.35,12.83,11.13,0.04,31.41,0.21,33.44,10.9,2.02,12.44,0.0,33.53,29.72,27.53,1.18,12.38,14.88,0.69,14.62,10.5,13.94,28.67,14.94,32.24,0.6,14.41,32.0,14.87,0.07,18.42,0.26,13.78,31.57,0.79,11.44,16.63,1.32,15.87,16.02,13.49,0.04,0.42,1.93,31.61,12.65,31.39,14.37,14.99,15.96,15.07,0.42,30.67,33.55,12.92,31.81,27.64,11.89,11.32,32.46,16.51,15.51,16.3,13.88,1.47,33.92,1.33,13.58,27.22,12.03,0.82,11.16,15.5,27.73,33.02,9.91,0.51,27.05,16.35,31.42,11.39,27.26,2.08,30.25,0.72,17.65,2.14,0.0,1.68,26.73,28.45,2.1,30.65,29.55,9.51,12.03,29.05,3.36,33.13,1.84,14.48,0.0,0.66,14.91,14.49,12.75,29.49,14.11,0.0,0.0,1.18,28.85,31.82,29.46,32.66,1.41,28.35,11.78,14.1,1.22,1.94,13.85,29.2,29.19,29.43,30.11,1.26,16.28,13.38,1.82,15.93,0.8,1.7,30.43,14.7,29.95,32.69,0.44,1.09,0.89,1.76,9.52,13.94,13.3,11.43,15.82,1.34,29.12,13.86,31.55,17.48,13.74,13.01,11.35,14.81,0.96,14.12,0.0,2.04,1.75,29.51,0.17,12.87,9.08,15.54,13.68,35.17,31.14,14.44,0.81,31.34,15.98,1.34,17.36,13.84,28.24,15.03,28.31,3.06,34.23,14.27,34.45,2.27,14.04,0.0,0.59,17.29,0.06,15.76,2.37,1.11,28.58,0.08,12.95,15.59,11.37,32.49,30.13,13.87,15.95,16.6,11.32,0.27,0.0,0.0,0.0,13.91,12.14,9.53,31.34,1.94,13.15,1.83,13.18,10.97,18.01,12.97,32.51,13.45,14.32,28.31,34.09,30.13,30.95,28.18,13.08,29.3,0.0,0.87,16.34,0.07,0.0,33.73,0.33,0.59,13.43,14.41,0.35,11.37,0.78,0.12,0.82,0.19,15.38,32.4,1.56,0.66,19.16,0.0,19.45,29.59,1.85,32.34,31.15,32.89,1.39,12.34,0.97,0.01,1.31,14.56,0.2,0.01,0.75,0.68,30.2,12.51,27.36,0.0,33.69,13.12,29.9,10.55,31.3,3.62,0.76,29.88,33.7,1.06,12.58,29.65,0.0,11.49,2.35,29.3,12.47,13.75,1.56,1.5,31.87,14.35,0.0,1.28,0.37,13.91,9.55,30.08,1.71,0.44,0.02,15.48,12.28,0.0,0.55,15.51,14.26,14.01,9.29,14.7,10.56,28.36,28.59,16.87,0.65,15.98,0.0,32.09,14.25,31.53,15.03,1.47,30.23,11.93,29.93,2.62,13.85,15.48,2.01,11.87,27.84,28.34,0.15,1.15,32.95,30.2,12.91,15.26,9.95,27.41,29.92,16.61,1.58,12.61,14.11,13.71,31.39,0.94,14.02,1.7,12.58,14.74,30.9,1.18,11.43,12.12,0.82,31.88,29.79,16.06,10.01,14.52,26.08,27.46,0.68,11.32,15.06,12.92,0.93,1.59,15.53,30.52,18.15,11.93,29.53,29.55,28.13,13.88,0.25,32.37,14.01,0.0,9.57,12.52,16.62,13.1,26.92,31.83,0.0,0.72,13.87,30.51,0.2,13.81,11.88,0.0,11.65,29.79,32.9,13.15,30.97,28.56,29.24,16.36,0.03,14.12,28.64,1.04,30.32,13.84,14.13,0.0,0.92,13.06,1.57,29.77,28.56,28.06,16.18,13.69,2.16,14.13,0.73,33.04,15.96,33.09,13.65,15.52,1.23,30.73,0.29,0.27,13.36,0.57,11.83,0.37,11.86,0.49,32.93,0.17,0.11,16.31,14.05,0.05,0.0,13.78,13.06,12.9,0.61,0.96,32.25,32.64,0.89,1.69,33.77,0.66,30.65,31.03,10.17,15.15,0.52,12.65,11.78,12.27,28.03,27.87,31.11,16.02,12.87,0.0,15.48,14.38,30.6,1.12,30.73,2.89,13.6,12.32,0.0,14.11,30.15,12.81,18.32,16.12,31.4,13.53,11.87,12.4,15.74,2.01,12.69,28.75,0.0,13.69,30.07,30.84,17.74,1.2,13.44,30.38,13.16,29.98,12.31,11.51,12.11,0.35,29.29,31.49,1.08,0.57,29.34,0.38,14.07,14.82,12.71,0.0,0.11,13.22,30.72,18.71,30.23,0.0,29.42,1.6,2.92,13.68,28.09,12.07,12.02,11.01,0.89,31.38,28.01,11.96,16.26,0.0,16.86,8.35,30.62,15.29,32.26,29.41,0.39,12.12,12.44,13.15,30.87,1.91,1.65,10.78,14.65,31.03,12.62,34.06,12.32,0.44,12.63,0.15,15.54,32.51,12.93,18.61,14.29,0.5,13.38,1.1,13.77,15.53,0.0,15.33,30.27,28.99,0.0,14.4,28.4,0.81,27.8,29.16,15.44,14.11,1.14,1.29,1.17,33.35,0.55,16.33,0.41,16.96,0.0,29.65,0.68,29.92,14.71,14.39,15.65,27.33,14.03,31.39,31.14,1.26,13.75,10.57,14.95,13.53,10.65,0.95,1.67,2.58,1.43,1.94,0.0,2.18,12.67,15.91,0.66,15.84,29.34,0.01,30.95,28.51,0.03,16.66,11.72,15.59,16.1,12.9,2.5,27.85,17.3,16.74,32.69,1.67,14.51,32.16,9.42,30.29,26.32,0.18,32.34,0.18,12.06,15.91,0.98,0.73,15.85,3.06,27.01,17.25,12.95,30.46,1.42,31.78,30.82,2.4,34.26,0.03,25.7,1.28,0.59,29.35,16.86,10.65,32.74,13.34,0.87,28.23,13.86,33.55,13.05,0.01,30.89,10.67,16.03,12.75,0.0,29.52,12.3,12.86,11.91,2.1,15.99,10.67,1.25,0.24,2.07,16.33,15.31,31.48,30.13,15.37,32.56,27.35,13.33,0.79,16.4,31.11,11.75,0.1,15.01,13.04,30.02,0.41,14.65,1.93,25.65,14.55,27.74,27.93,14.38,12.62,31.51,13.15,28.92,32.96,13.28,10.62,1.41,0.99,12.15,15.68,13.32,2.08,15.05,27.0,15.04,16.16,31.64,14.01,1.03,3.71,0.23,30.2,28.35,15.82,32.8,14.72,10.87,12.23,28.46,10.43,12.25,15.29,31.35,2.04,13.32,24.67,12.88,17.26,13.57,0.0,0.96,10.1,0.15,0.76,0.0,12.64,13.29,0.6,31.4,29.61,16.85,10.68,28.28,13.6,1.14,0.64,2.03,30.51,13.82,12.15,13.84,31.82,28.7,13.65,14.4,0.42,11.04,15.65,10.08,19.28,2.01,0.13,3.06,12.49,15.2,13.83,0.0,31.45,29.11,8.48,0.15,8.87,0.97,0.55,14.51,32.78,16.13,0.48,11.3,1.18,14.49,15.07,12.01,0.0,11.88,12.3,32.37,1.69,1.97,0.05,1.06,11.7,29.09,16.44,0.42,31.86,1.54,15.23,15.58,30.49,11.98,14.12,1.2,14.35,14.5,15.89,26.61,29.95,0.83,2.62,1.85,15.74,1.29,13.36,13.68,32.15,14.34,31.99,0.61,32.08,16.42,15.21,15.45,32.23,13.01,0.04,1.5,10.18,32.05,29.82,11.8,0.53,12.78,1.38,15.79,2.62,15.34,26.13,31.0,12.8,16.95,14.02,29.64,31.4,0.12,2.3,29.76,10.06,10.78,30.06,18.49,13.91,11.81,0.18,1.14,0.67,0.04,1.44,29.89,12.78,0.2,16.64,14.17,14.95,15.57,17.67,30.23,13.18,12.6,11.68,1.75,0.79,0.0,0.1,29.03,15.18,13.14,30.72,12.76,13.5,0.62,1.15,13.72,29.52,30.17,11.82,13.98,1.8,0.0,16.63,0.15,12.53,0.0,15.09,10.32,29.77,11.98,7.97,30.76,14.77,27.32,1.92,15.0,10.92,0.34,0.45,32.47,1.68,10.25,12.39,29.92,29.61,2.09,0.87,0.41,0.59,15.23,1.94,32.14,1.12,12.28,0.0,10.49,32.67,14.62,0.0,17.68,30.52,1.56,15.2,13.16,19.0,14.15,27.44,15.93,30.05,0.87,28.25,32.35,0.39,28.45,15.28,2.14,24.67,2.36,1.03,1.25,16.32,1.15,0.19,15.17,10.46,1.71,2.62,12.3,0.35,29.42,13.99,11.46,0.85,1.28,28.54,0.31,30.14,2.4,0.34,1.32,14.83,0.86,31.57,1.04,29.2,29.15,15.2,9.65,12.54,18.72,32.36,12.83,28.64,31.61,0.87,32.02,17.03,13.07,27.11,16.56,2.4,17.25,12.52,28.12,28.79,17.49,1.6,11.56,0.0,31.15,30.11,16.17,11.15,30.59,27.97,13.13,16.42,2.49,13.68,29.5,2.34,30.21,0.32,11.47,29.47,0.29,29.21,13.21,28.8,1.24,2.19,17.19,30.85,28.85,0.95,29.32,13.57,29.78,27.99,2.31,0.13,0.38,27.34,31.39,25.79,0.86,10.75,1.24,1.51,15.38,31.9,33.53,31.03,15.68,32.37,16.08,13.52,0.36,31.83,2.63,31.32,32.14,1.05,15.11,12.41,14.0,13.79,11.25,27.76,13.69,0.72,30.36,12.24,28.22,13.99,13.78,11.98,12.79,0.83,2.35,1.27,15.15,15.17,15.44,0.0,28.73,31.56,14.27,16.29,1.08,31.64,0.21,1.14,14.58,0.44,29.17,31.37,0.5,28.57,31.41,29.45,14.84,11.42,27.41,0.0,15.07,35.03,13.83,0.45,0.0,14.98,16.37,28.26,16.96,30.64,27.18,28.98,29.76,31.4,13.13,16.18,16.72,10.07,0.53,32.87,12.68,0.32,1.36,0.0,15.04,0.43,13.1,11.3,16.9,13.65,31.38,30.62,30.87,28.68,1.29,29.23,14.8,1.4,13.12,15.01,0.72,0.44,28.8,13.58,15.09,0.0,17.63,0.91,0.26,29.02,15.36,1.3,14.07,15.06,12.33,14.72,13.9,10.91,0.0,27.87,14.72,2.78,14.46,16.41,0.0,13.56,28.12,1.11,14.58,0.0,16.2,31.95,14.67,30.97,29.35,0.1,24.62,0.41,13.02,15.71,13.96,17.03,12.81,29.42,15.78,1.08,13.26,13.26,29.13,14.15,12.23,15.19,0.79,29.39,28.98,0.98,0.62,30.4,12.14,28.3,12.22,1.54,31.56,0.45,8.85,1.36,12.81,31.49,32.66,13.15,15.41,31.12,17.77,1.54,0.54,10.84,31.13,29.89,1.23,16.8,15.6,31.33,0.0,2.31,14.28,20.89,12.12,1.77,29.34,0.09,0.97,28.7,14.7,2.88,1.65,1.43,15.45,0.19,13.68,15.92,16.81,13.45,28.79,11.75,13.53,29.77,31.11,1.89,13.89,0.42,31.62,0.57,13.37,0.13,0.61,0.53,12.77,30.66,14.09,1.14,12.14,1.76,13.12,17.74,30.26,0.7,0.21,14.75,14.73,32.61,14.92,11.86,15.96,0.32,2.15,13.12,12.73,0.8,0.82,29.92,2.36,0.04,0.13,1.75,1.9,29.3,0.0,34.06,13.73,31.22,11.86,32.88,14.29,29.02,0.0,1.32,1.16,12.85,26.98,16.64,0.12,33.44,11.76,31.18,31.52,14.64,0.0,1.13,2.25,13.64,27.98,0.5,2.76,12.97,15.88,12.79,0.68,15.81,33.51,12.18,16.35,30.15,12.16,14.53,27.35,29.8,14.28,15.66,1.15,0.0,29.61,33.45,0.29,2.54,12.59,0.54,14.41,31.02,0.0,29.03,2.82,29.45,27.15,14.52,17.66,1.59,11.13,16.02,0.73,0.95,0.43,13.05,31.91,29.23,12.82,14.75,28.78,31.8,32.54,0.0,14.83,0.85,16.29,13.23,0.55,14.01,1.43,29.37,28.93,31.76,2.2,31.12,0.3,15.5,13.46,29.06,14.34,15.95,16.69,13.62,29.0,30.97,15.54,1.66,17.36,13.61,30.83,28.92,30.0,31.53,30.69,17.62,1.88,15.08,16.18,2.2,16.48,0.29,15.53,0.49,27.42,13.86,13.15,31.38,0.53,10.21,12.54,1.67,13.67,32.49,31.01,2.02,28.96,1.25,12.52,0.29,0.16,17.91,0.35,1.06,29.3,13.62,1.0,31.46,1.09,13.51,2.0,29.13,13.64,32.28,15.33,1.87,0.73,16.17,13.9,15.16,14.93,11.37,18.01,14.47,0.04,13.88,14.22,0.9,13.55,18.96,14.0,13.63,30.0,11.0,16.1,0.97,30.3,17.77,12.06,30.17,12.98,0.07,15.36,15.89,33.57,0.73,1.22,25.43,1.27,14.45,16.64,1.48,2.09,0.81,15.0,0.05,0.0,0.64,0.57,1.26,0.0,31.83,13.01,27.02,31.79,17.02,15.03,0.35,16.36,0.17,15.31,13.24,1.37,28.13,13.08,13.99,29.91,15.76,15.88,13.57,10.66,28.7,24.97,12.07,12.51,25.23,31.36,16.61,12.95,0.73,9.04,0.0,1.02,29.38,0.58,29.45,9.93,14.58,11.5,13.17,1.54,15.76,1.09,1.24,31.07,1.51,13.6,0.0,1.89,0.0,16.17,0.49,0.0,15.67,1.95,12.85,14.41,1.28,29.71,12.57,32.31,30.03,14.1,14.98,12.98,12.9,2.6,11.92,0.62,30.17,15.3,14.16,1.12,31.16,1.04,13.77,34.58,1.27,2.56,1.16,0.86,0.12,16.35,29.1,1.51,11.83,32.54,1.57,0.82,0.69,17.8,15.59,13.03,29.54,26.95,0.06,14.92,10.54,1.73,0.94,29.87,2.6,3.19,1.72,0.77,26.99,15.68,15.24,0.77,30.43,0.0,15.72,28.71,14.9,1.5,13.06,13.16,11.43,0.0,11.65,0.4,0.92,1.2,15.86,11.81,0.36,2.43,33.75,27.71,16.38,30.63,0.98,32.12,12.58,0.71,34.16,29.61,0.12,0.71,27.65,2.02,30.51,13.8,14.01,2.5,31.3,0.0,15.87,31.63,29.32,0.0,1.29,27.62,28.59,15.58,28.94,1.26,0.43,25.68,32.34,0.65,14.31,10.17,1.06,15.56,1.84,1.67,29.35,13.0,13.5,27.51,12.53,15.53,28.56,0.9,13.74,0.8,12.78,0.44,2.31,0.57,2.2,14.62,11.05,11.7,30.84,16.55,11.62,1.21,14.12,11.98,10.75,30.0,0.99,12.04,31.38,27.1,30.29,15.81,1.18,0.0,0.74,32.1,0.61,11.97,0.62,17.62,0.79,29.02,29.09,16.59,11.24,10.46,30.38,11.38,1.07,33.05,2.29,12.64,14.01,17.54,1.6,14.38,0.88,26.96,1.28,12.69,29.69,16.14,27.03,0.06,29.67,12.5,1.55,14.92,14.12,28.69,31.97,0.58,1.76,29.33,29.17,0.0,12.61,30.37,0.0,28.74,31.99,12.24,27.57,1.5,29.55,29.43,28.13,1.39,17.64,10.69,13.17,28.68,30.75,11.24,11.7,15.22,28.54,26.06,12.16,0.71,30.54,14.95,15.51,28.87,11.71,0.82,12.04,25.84,0.23,33.17,1.48,16.63,31.85,32.7,25.98,12.83,12.21,14.39,13.11,1.71,2.42,16.52,0.23,2.19,1.07,14.05,10.7,1.71,13.93,12.8,30.82,27.41,12.12,18.08,11.41,28.48,13.46,29.02,24.95,31.03,13.11,0.0,28.07,2.47,12.7,30.16,13.11,12.83,1.47,2.19,13.32,29.58,0.37,2.03,32.74,1.66,27.64,11.74,1.06,27.24,0.78,0.26,15.95,29.2,0.93,0.0,3.4,14.76,30.61,1.28,0.72,10.35,14.1,15.28,16.58,1.35,33.01,14.29,15.14,12.89,16.33,12.9,28.82,10.61,29.75,28.92,31.94,10.55,13.77,29.87,31.74,2.14,35.02,28.89,2.24,31.8,1.8,15.13,10.67,14.25,2.9,14.14,28.4,14.98,14.08,27.98,10.83,11.94,11.06,2.44,31.94,12.59,33.64,29.26,3.18,13.11,0.16,11.04,28.94,32.67,28.62,0.04,1.12,12.08,0.21,9.9,14.29,2.93,13.25,12.7,10.5,10.3,15.72,0.94,17.47,14.0,28.05,12.82,14.31,13.98,35.35,12.94,28.45,13.11,15.78,13.41,1.54,15.1,10.85,0.0,0.64,0.39,14.5,0.44,16.47,28.03,18.26,12.01,31.86,29.66,0.0,13.98,2.28,30.89,0.59,0.65,30.36,2.34,29.52,14.26,11.99,24.81,12.51,32.04,13.7,12.88,30.7,34.64,28.95,0.62,31.04,16.03,28.21,31.32,1.15,0.0,17.28,1.52,0.59,16.58,29.85,34.15,1.69,0.26,16.72,29.6,13.32,0.2,12.89,27.24,12.47,14.7,17.71,28.9,12.84,29.78,1.29,33.24,16.28,2.12,29.01,30.08,2.23,0.69,0.04,1.6,11.7,28.84,12.61,31.68,1.56,19.54,15.17,0.96,0.18,16.08,34.41,1.14,27.17,30.76,13.47,0.94,28.78,1.1,29.97,12.04,27.08,34.54,1.21,30.8,1.89,32.05,15.48,11.56,31.32,1.61,15.4,11.71,0.22,15.23,15.22,30.03,1.06,15.22,0.33,30.59,30.26,2.86,1.2,13.53,13.89,33.56,13.46,16.44,1.11,14.43,29.69,26.37,0.33,1.57,27.28,15.19,14.5,29.18,0.61,0.86,14.21,17.39,11.3,29.13,27.97,9.62,1.7,29.28,13.91,1.29,1.24,12.31,19.08,30.19,15.77,1.37,16.61,32.14,15.51,15.65,27.2,1.4,0.21,29.01,0.35,13.54,14.3,1.37,31.88,14.14,15.78,15.58,14.97,10.93,32.34,12.96,12.47,14.89,29.66,15.23,1.59,0.0,16.45,12.33,13.32,16.34,10.84,2.72,12.38,13.63,15.35,0.35,28.4,25.69,0.71,28.47,0.0,14.22,1.02,13.95,0.36,13.47,31.06,30.41,2.21,16.71,1.4,29.91,11.52,13.69,0.65,1.2,11.23,0.66,30.8,14.43,13.96,30.13,30.61,30.56,0.33,14.89,30.95,32.01,30.25,14.51,0.25,28.91,24.88,0.0,16.0,0.0,29.02,28.95,0.55,12.92,31.58,0.69,0.0,11.85,0.45,14.0,33.64,1.41,1.49,30.54,2.39,27.94,33.11,12.96,3.83,13.36,14.59,31.06,14.88,15.55,0.31,14.55,3.71,14.13,15.49,29.52,14.0,29.8,28.8,1.15,28.53,33.43,10.84,2.27,0.5,14.72,11.21,0.91,29.5,9.87,13.55,27.9,16.74,12.58,1.24,30.31,29.21,0.5,29.21,15.09,19.18,28.56,1.7,10.79,14.37,30.25,0.05,13.69,30.9,28.2,0.28,28.23,32.29,13.75,0.0,16.66,31.29,14.49,13.42,13.62,12.5,14.35,15.12,14.73,15.42,14.43,30.38,30.58,28.81,0.14,29.88,14.68,12.76,0.21,13.4,0.57,29.01,31.59,14.34,29.32,29.56,12.48,31.14,14.8,1.48,13.75,29.8,14.47,1.57,11.48,1.35,28.91,12.86,2.93,0.84,0.24,14.36,0.49,31.54,0.0,12.23,14.67,17.24,0.09,17.08,16.5,1.76,2.23,31.36,14.72,0.0,2.43,13.98,15.48,17.6,14.65,15.8,14.4,11.28,11.8,1.34,1.45,1.36,0.37,16.35,28.6,1.49,15.47,27.9,30.11,0.0,13.28,29.67,0.26,32.21,12.73,12.32,12.85,13.52,26.06,18.89,13.25,14.28,30.27,12.87,28.68,30.6,29.15,30.65,32.33,0.0,14.5,16.32,9.64,2.68,29.54,13.82,3.33,28.42,1.71,17.32,0.0,30.76,10.41,12.68,13.17,29.81,27.83,12.47,0.41,14.66,1.06,0.0,31.09,31.61,16.01,12.75,0.66,15.68,14.58,0.19,28.87,0.29,0.0,15.26,26.54,27.94,10.77,0.0,33.1,32.41,29.43,0.69,0.0,13.15,14.76,13.25,10.37,28.52,14.73,14.94,30.07,14.26,1.7,16.71,0.76,28.39,12.98,13.35,27.57,0.0,14.91,31.07,0.17,12.07,17.9,0.69,29.01,13.54,11.54,31.83,29.54,13.57,2.5,2.12,14.94,14.63,32.51,0.14,0.36,2.11,28.46,10.03,14.68,14.06,1.0,0.94,0.04,29.44,27.29,18.11,34.25,32.47,31.88,1.18,15.98,13.14,0.58,14.17,14.14,27.5,16.44,2.17,0.07,15.07,11.05,13.36,1.19,0.48,0.51,27.86,29.77,12.06,0.61,0.0,13.94,32.96,29.6,13.85,28.26,2.16,0.36,12.34,15.52,16.45,17.8,13.93,11.56,0.79,0.04,1.78,16.91,28.89,0.01,30.03,14.55,0.42,1.41,32.88,13.36,14.79,14.27,15.89,12.04,11.04,28.42,14.48,14.02,15.41,11.64,29.39,0.0,12.87,14.0,30.02,2.85,18.5,12.95,27.06,0.0,11.49,0.34,0.95,27.94,13.34,12.44,0.55,0.37,12.78,12.79,12.37,0.08,14.36,1.05,0.77,11.18,28.17,0.0,30.61,33.33,0.78,0.62,11.74,10.78,29.64,1.69,11.43,13.73,1.11,1.94,29.76,30.11,14.86,1.41,31.52,29.67,1.57,0.71,30.61,14.9,29.33,27.96,0.75,31.67,28.78,15.22,13.54,0.0,1.2,2.11,15.0,30.16,35.01,29.71,0.7,14.5,13.26,0.98,31.8,1.43,2.74,27.78,1.04,15.36,14.44,8.25,15.14,14.22,27.07,12.08,10.97,11.14,14.52,13.93,12.14,0.38,31.52,1.97,31.81,2.1,1.64,11.45,31.59,14.39,27.7,2.37,28.93,31.02,13.96,14.45,14.82,10.81,0.61,0.5,13.15,15.13,0.7,2.13,10.81,9.39,10.89,0.28,26.91,31.28,1.98,30.94,14.41,10.21,30.17,29.2,28.38,32.4,3.23,2.61,9.51,29.21,14.61,0.73,11.9,32.4,13.01,27.98,12.06,0.39,29.41,32.13,15.54,11.26,16.55,1.17,14.38,11.97,16.78,31.31,2.38,12.47,33.09,12.69,14.32,15.8,14.01,1.84,10.49,29.96,27.87,1.14,2.51,27.61,28.7,31.2,15.56,13.83,11.09,28.88,13.72,0.0,0.57,31.39,0.85,28.82,12.69,14.27,28.5,15.07,15.22,29.77,16.25,9.96,12.97,31.88,15.01,14.02,19.79,29.4,0.77,30.64,12.4,14.23,29.12,16.7,0.3,18.53,15.51,31.43,16.46,1.16,0.51,0.53,0.0,29.71,31.06,14.19,31.48,0.0,30.72,15.67,17.03,29.56,1.89,31.51,14.47,1.66,20.33,32.26,7.86,0.0,0.83,13.75,19.89,0.87,14.06,30.15,0.0,0.25,0.51,1.5,12.62,14.0,31.41,12.9,30.65,10.53,28.58,1.45,2.29,1.77,13.3,30.23,11.78,13.09,0.0,14.45,1.62,30.0,1.6,16.05,0.0,15.02,14.77,1.82,13.56,31.17,0.54,14.63,13.2,11.72,1.81,9.36,19.0,33.53,14.44,0.64,30.25,12.96,12.73,13.16,13.87,34.0,30.33,28.85,26.21,17.69,11.75,14.43,33.54,30.58,16.83,31.99,24.38,30.9,13.08,1.16,0.27,0.61,13.79,14.32,15.56,17.74,27.85,14.8,1.36,0.94,1.04,23.78,14.51,2.93,11.56,1.31,30.7,0.65,1.34,14.65,1.05,12.63,13.56,0.0,9.61,30.67,11.59,28.28,0.0,0.81,11.14,1.56,13.91,30.06,0.34,0.48,14.11,0.66,12.01,27.3,31.93,0.14,28.23,32.41,29.18,27.94,16.18,27.16,31.27,15.33,15.95,16.19,0.37,1.48,28.88,14.26,1.16,13.96,13.15,27.99,15.94,32.99,1.75,1.79,11.62,30.7,0.0,30.62,32.08,17.61,1.68,15.47,28.93,30.29,0.0,1.72,1.52,13.85,17.96,12.61,12.11,3.09,0.2,31.55,30.25,30.57,1.07,13.63,11.02,27.63,30.43,30.62,29.94,1.33,14.99,0.77,13.78,29.04,1.2,13.8,0.0,1.25,0.68,13.97,30.34,16.29,12.67,16.34,2.06,11.73,0.47,1.28,12.51,27.72,1.43,13.46,11.61,16.22,0.88,0.67,28.25,14.65,0.8,16.61,15.62,1.12,17.29,13.12,16.63,14.44,11.36,17.37,0.69,0.12,0.01,0.94,1.91,30.17,29.74,0.78,0.7,31.37,18.49,0.32,31.2,2.82,0.01,15.84,8.97,0.97,13.72,0.23,16.41,28.05,27.93,32.85,25.76,15.59,12.32,12.94,13.56,11.7,0.95,3.05,28.93,29.61,13.3,15.39,28.28,29.83,14.4,0.0,1.66,0.55,31.23,0.96,27.77,27.88,11.58,32.41,32.24,17.22,28.21,2.39,27.13,28.29,16.19,29.82,33.34,30.28,0.58,1.28,29.0,31.59,29.15,28.25,30.4,32.23,12.74,0.0,11.27,33.75,13.54,32.61,12.02,0.0,13.02,9.63,31.92,31.89,2.02,30.51,30.88,11.83,31.23,31.08,1.43,0.54,1.41,33.2,13.81,30.25,10.69,0.96,27.63,13.68,1.24,13.04,30.85,15.86,1.67,13.82,14.55,12.07,0.95,13.93,13.62,12.89,0.38,16.61,0.0,1.79,1.45,15.16,29.9,15.45,1.55,17.21,32.8,28.4,14.22,16.45,32.35,14.96,30.35,27.13,14.94,26.29,14.78,12.48,17.68,15.54,15.49,0.99,2.5,29.83,2.1,29.63,0.76,24.93,15.96,0.67,30.17,13.62,0.0,27.98,13.49,1.9,14.77,10.54,0.64,10.99,1.82,0.36,16.65,0.79,12.29,0.81,0.1,15.72,29.8,16.02,1.01,15.9,11.72,14.86,15.13,29.47,14.78,13.52,2.07,3.8,13.93,15.09,30.92,0.6,0.98,11.88,1.86,30.86,13.84,32.56,16.16,30.86,13.91,0.34,30.57,34.77,29.37,32.36,12.17,2.71,30.87,15.84,16.7,0.39,1.2,2.86,12.77,0.14,15.65,31.94,15.54,27.41,0.0,11.3,30.63,31.59,13.67,0.95,32.34,13.16,13.14,30.15,12.99,15.79,30.19,12.99,1.06,12.86,1.18,0.64,0.0,15.02,15.8,12.88,28.42,14.51,0.23,29.62,11.07,13.46,1.08,10.65,27.42,13.16,13.19,0.69,31.0,27.62,13.21,31.1,0.94,18.97,26.93,0.0,13.55,31.05,11.0,1.11,29.53,1.63,1.26,0.19,1.04,0.65,14.57,30.54,13.95,0.0,14.45,0.74,11.95,32.06,17.43,1.44,33.33,15.81,2.2,14.28,0.72,11.99,0.52,0.59,12.78,15.53,2.01,35.37,30.34,15.48,28.58,11.87,13.71,33.67,11.75,13.61,0.71,19.04,35.17,0.69,31.7,0.21,27.37,1.04,32.02,29.75,0.91,31.29,2.17,14.74,32.45,12.15,31.6,17.79,1.03,12.98,27.38,0.28,11.52,13.54,0.97,9.54,17.63,30.25,0.53,1.1,16.47,30.01,12.53,0.73,31.04,0.82,29.12,2.09,1.85,14.59,10.0,16.57,29.27,1.33,0.16,12.86,29.36,15.74,13.23,0.19,33.38,28.93,31.36,13.15,13.59,13.39,30.93,2.03,32.49,0.48,15.64,12.6,33.93,14.83,0.54,1.41,0.49,14.53,14.28,12.14,12.31,0.89,0.0,27.17,1.03,16.14,31.6,2.45,0.07,10.77,29.33,17.26,31.53,0.3,18.34,2.9,1.67,3.16,13.68,0.0,10.14,1.81,17.03,12.47,14.14,0.71,1.61,27.94,16.38,29.49,10.93,1.42,2.03,16.3,15.21,0.0,1.12,14.92,12.44,15.4,1.67,14.1,12.02,0.0,14.42,15.02,16.96,14.79,28.95,29.64,12.77,13.24,12.25,29.59,0.38,28.0,13.55,16.22,12.21,2.23,16.85,28.11,11.73,30.24,30.96,1.22,30.72,12.72,14.81,27.51,27.65,0.71,1.34,1.04,11.19,14.16,12.01,13.82,14.97,3.24,14.47,31.52,31.13,0.0,28.54,28.17,13.51,15.55,11.99,27.69,13.92,13.1,0.0,0.83,13.42,11.09,1.32,15.81,12.94,0.93,16.43,15.31,16.49,15.71,1.09,0.0,31.69,0.14,0.35,31.7,1.82,28.38,0.33,14.61,30.83,2.26,2.19,1.09,11.02,30.55,15.21,13.06,1.93,15.03,14.58,13.11,18.31,30.18,13.69,31.29,32.03,15.31,13.76,32.16,17.56,12.51,0.47,12.28,11.14,14.49,27.04,29.45,27.57,31.82,3.41,14.94,0.4,12.05,28.54,2.45,33.7,16.01,16.98,18.26,14.26,1.77,1.94,0.14,11.21,10.55,2.13,1.43,1.36,31.71,30.33,30.11,1.18,1.19,28.17,14.29,1.22,0.91,1.19,30.51,12.01,18.07,12.86,31.59,8.18,32.11,15.43,0.79,1.07,15.56,15.38,13.15,1.17,29.17,37.54,1.49,16.67,14.23,1.31,11.98,0.13,15.94,13.61,15.21,10.44,11.05,33.6,1.37,13.66,30.57,13.71,15.4,1.66,25.53,0.0,12.14,0.0,29.53,2.48,30.31,33.2,28.87,26.2,29.32,13.51,11.97,11.89,32.56,0.69,11.97,0.48,0.4,0.05,2.67,0.67,15.81,17.28,0.22,29.78,28.24,10.17,35.24,0.0,14.4,14.57,32.12,12.46,13.54,33.5,15.23,2.03,27.94,15.66,31.58,0.37,0.5,33.8,13.02,0.0,13.86,16.15,27.04,11.54,13.48,0.09,12.77,13.09,8.6,32.68,2.34,0.54,1.18,1.23,12.67,2.21,30.28,1.67,11.84,29.63,2.16,14.48,0.48,2.49,30.64,2.2,14.91,11.64,31.5,2.21,29.24,13.65,0.45,30.52,10.9,30.35,14.29,13.84,30.0,0.92,12.02,11.32,30.72,1.14,0.0,13.99,0.06,16.32,10.85,31.47,0.24,14.83,1.06,0.36,30.57,11.38,26.31,3.44,31.02,12.45,31.01,30.93,13.35,34.03,17.17,17.72,13.61,13.37,0.78,13.67,2.82,1.69,30.93,33.81,11.82,0.93,29.79,16.17,32.63,15.08,34.62,1.97,0.83,13.13,12.72,0.29,29.34,30.42,12.88,2.51,0.12,12.89,0.0,0.28,1.15,15.44,16.33,12.48,29.74,2.11,2.79,14.69,13.87,11.36,16.05,28.06,1.01,13.21,15.9,15.0,17.91,29.88,16.03,14.16,1.55,32.78,0.63,13.28,0.53,11.9,16.03,1.11,28.88,0.73,0.46,30.82,13.54,34.42,10.83,14.68,28.92,32.4,13.94,15.34,30.5,17.3,17.99,13.59,14.77,31.79,1.55,28.82,31.19,13.76,0.27,15.21,30.94,0.0,12.12,29.31,0.42,0.98,13.48,27.07,14.35,14.21,14.58,1.26,13.16,12.34,32.77,2.13,14.06,0.29,15.06,12.7,30.22,14.27,15.91,31.94,15.6,1.0,28.32,29.64,14.03,3.17,32.21,15.81,13.75,28.93,31.86,12.72,16.48,27.88,30.06,15.01,27.86,31.05,1.23,0.57,14.12,0.88,1.82,15.06,2.22,12.96,0.69,0.06,14.85,1.71,29.24,28.31,14.41,30.98,0.49,30.49,13.73,0.1,13.32,15.44,13.99,15.45,29.91,29.58,33.43,18.15,12.18,13.17,0.38,32.68,1.68,1.11,14.06,17.01,1.01,27.61,32.83,28.36,12.33,15.05,0.56,17.43,10.09,2.85,14.19,16.9,1.81,0.08,14.94,13.06,1.95,0.0,30.59,28.44,14.01,28.97,0.12,12.41,0.0,29.07,11.43,31.16,27.14,31.57,12.1,33.29,11.73,0.68,14.26,15.94,0.99,29.27,0.56,15.06,11.06,15.94,30.24,12.26,14.69,15.06,16.59,0.18,13.14,30.83,10.86,16.24,2.42,33.48,14.64,13.08,12.2,15.09,13.51,12.44,29.94,14.66,31.17,1.44,31.06,11.7,2.11,13.45,17.91,29.82,31.19,31.47,31.15,14.68,0.02,30.02,0.48,12.54,0.14,29.31,32.23,13.89,15.69,30.09,14.5,13.27,13.95,2.94,29.35,0.0,14.86,1.67,16.84,31.34,16.46,14.93,2.6,25.96,0.07,30.79,28.93,14.81,14.81,12.65,0.62,29.4,11.27,11.52,0.79,1.76,0.77,11.85,0.31,14.28,29.35,28.89,13.96,31.48,31.01],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"price_in_dollar\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"host_reported_average_tip\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4fa0b479-b6fd-42a4-a439-a4bc03d92afb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_list, x=\"price_in_dollar\", y=\"host_reported_average_tip\")\n",
        "fig.show()\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "8RBWziuASbI5"
      },
      "id": "8RBWziuASbI5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 5: Define Three clusters via K-Means\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_cld3n6qss000s2a6ls9js7l7g)\n",
        "\n",
        "Just like in the unsupervised Section of Uplimit we are going to use the columns **price_in_dollar** and **host_reported_average_tip** to create a cluster variable. This we'll use later on! Let's start by importing the Kmeans, Pipeline and MinMaxScaler"
      ],
      "metadata": {
        "id": "8CYBS6_kZklh"
      },
      "id": "8CYBS6_kZklh"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "SZMT3CGUhV0c"
      },
      "id": "SZMT3CGUhV0c",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "_qBiCyFFSkkW"
      },
      "id": "_qBiCyFFSkkW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to create a pipeline which integrates the scaler and KMeans algorithm. Then we use fit_predict to create the KMean labels.\n",
        "\n",
        "*Make sure to set a seed for your algorithm/model.*"
      ],
      "metadata": {
        "id": "9hKe9397hWRa"
      },
      "id": "9hKe9397hWRa"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"scaler\", MinMaxScaler()) , # YOUR CODE HERE\n",
        "    (\"model\", KMeans(n_clusters=3, random_state=SEED)) # YOUR CODE HERE\n",
        "    ])\n",
        "\n",
        "Kmean_labels = pipeline.fit_predict(\n",
        "    df_list[[\"price_in_dollar\", \"host_reported_average_tip\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "WV77QCjGVA7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f82c426-e027-433f-991f-392b713468e8"
      },
      "id": "WV77QCjGVA7W",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", MinMaxScaler()),\n",
        "    (\"model\", KMeans(n_clusters=3, random_state=SEED))\n",
        "    ])\n",
        "\n",
        "Kmean_labels = pipeline.fit_predict(\n",
        "    df_list[[\"price_in_dollar\", \"host_reported_average_tip\"]]\n",
        ")\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "1HxEesA-Stvz"
      },
      "id": "1HxEesA-Stvz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since these labels are seen as numerical, when visualizing they will be regarded as numerical/continuous variables. However, these labels indicate three separate groups, meaning we want to change it into a categorical variable. For now we'll want you to convert these numerical labels into string variables."
      ],
      "metadata": {
        "id": "lwS0lLBug_Rk"
      },
      "id": "lwS0lLBug_Rk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the labels from numerical into categorical.\n",
        "#Kmean_labels = ... # YOUR CODE HERE\n",
        "Kmean_labels = [str(x) for x in Kmean_labels]"
      ],
      "metadata": {
        "id": "YSP0vOl0hA-m"
      },
      "id": "YSP0vOl0hA-m",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmean_labels"
      ],
      "metadata": {
        "id": "fa6wXAUyjx3_"
      },
      "id": "fa6wXAUyjx3_",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "Kmean_labels = [str(x) for x in Kmean_labels]\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "9ldnK_PES20E"
      },
      "id": "9ldnK_PES20E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, as a last check, let's confirm the labels were assigned as expected, by visualizing the scatterplot with the labels!"
      ],
      "metadata": {
        "id": "BYDFDPtyg-_B"
      },
      "id": "BYDFDPtyg-_B"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(\n",
        "    df_list, x=\"price_in_dollar\", y=\"host_reported_average_tip\", color=Kmean_labels\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TVlJ3JdKhBjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "79ba5e7d-cc6f-4ac6-83bd-5fe9396cc733"
      },
      "id": "TVlJ3JdKhBjZ",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"3c4cf981-0817-4227-9454-89c3574293f1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3c4cf981-0817-4227-9454-89c3574293f1\")) {                    Plotly.newPlot(                        \"3c4cf981-0817-4227-9454-89c3574293f1\",                        [{\"hovertemplate\":\"color=2\\u003cbr\\u003eprice_in_dollar=%{x}\\u003cbr\\u003ehost_reported_average_tip=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"2\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"2\",\"showlegend\":true,\"x\":[127.0,62.0,69.0,83.0,103.0,253.0,62.0,47.0,67.0,58.0,75.0,82.0,57.0,91.0,61.0,62.0,95.0,112.0,80.0,88.0,63.0,45.0,54.0,72.0,76.0,74.0,73.0,59.0,96.0,49.0,99.0,50.0,125.0,79.0,51.0,130.0,58.0,123.0,53.0,72.0,126.0,103.0,128.0,78.0,49.0,53.0,40.0,130.0,53.0,130.0,63.0,64.0,121.0,104.0,86.0,77.0,64.0,72.0,119.0,91.0,43.0,105.0,129.0,72.0,105.0,130.0,51.0,49.0,56.0,61.0,51.0,119.0,110.0,168.0,64.0,108.0,87.0,38.0,144.0,80.0,51.0,49.0,122.0,93.0,100.0,124.0,53.0,78.0,54.0,53.0,61.0,110.0,85.0,66.0,102.0,71.0,50.0,47.0,71.0,112.0,108.0,46.0,60.0,37.0,103.0,106.0,85.0,130.0,110.0,78.0,116.0,63.0,70.0,60.0,58.0,100.0,111.0,53.0,39.0,71.0,47.0,72.0,48.0,126.0,43.0,81.0,122.0,112.0,90.0,66.0,51.0,54.0,128.0,66.0,47.0,82.0,82.0,78.0,57.0,66.0,39.0,69.0,92.0,54.0,179.0,50.0,122.0,69.0,51.0,106.0,59.0,230.0,122.0,30.0,55.0,83.0,54.0,103.0,81.0,68.0,107.0,66.0,58.0,62.0,63.0,68.0,67.0,80.0,58.0,68.0,73.0,79.0,66.0,54.0,99.0,58.0,167.0,58.0,61.0,52.0,73.0,56.0,60.0,91.0,68.0,85.0,65.0,84.0,93.0,65.0,51.0,60.0,55.0,128.0,53.0,52.0,58.0,129.0,47.0,71.0,125.0,90.0,63.0,105.0,67.0,53.0,60.0,99.0,82.0,92.0,74.0,140.0,58.0,106.0,88.0,63.0,92.0,55.0,114.0,70.0,119.0,50.0,77.0,78.0,91.0,55.0,121.0,127.0,120.0,109.0,141.0,64.0,60.0,41.0,88.0,47.0,80.0,70.0,92.0,62.0,102.0,58.0,58.0,54.0,78.0,88.0,69.0,76.0,129.0,104.0,83.0,80.0,38.0,46.0,88.0,42.0,80.0,110.0,35.0,41.0,39.0,96.0,73.0,43.0,56.0,130.0,66.0,129.0,41.0,103.0,64.0,66.0,96.0,51.0,57.0,130.0,81.0,117.0,53.0,67.0,62.0,67.0,54.0,58.0,42.0,52.0,82.0,130.0,55.0,66.0,70.0,110.0,68.0,53.0,43.0,64.0,78.0,121.0,106.0,68.0,57.0,126.0,109.0,123.0,50.0,104.0,87.0,32.0,95.0,90.0,100.0,48.0,86.0,61.0,144.0,52.0,24.0,87.0,88.0,117.0,42.0,119.0,127.0,90.0,69.0,39.0,58.0,39.0,75.0,53.0,87.0,79.0,91.0,64.0,97.0,44.0,94.0,96.0,73.0,66.0,200.0,66.0,66.0,53.0,84.0,77.0,104.0,64.0,153.0,112.0,78.0,70.0,259.0,54.0,56.0,145.0,60.0,112.0,40.0,67.0,90.0,110.0,48.0,118.0,59.0,156.0,49.0,53.0,62.0,73.0,117.0,110.0,49.0,66.0,48.0,150.0,55.0,48.0,86.0,80.0,51.0,50.0,39.0,112.0,59.0,56.0,67.0,114.0,57.0,128.0,68.0,118.0,148.0,46.0,97.0,62.0,83.0,82.0,53.0,54.0,88.0,117.0,37.0,39.0,108.0,119.0,125.0,91.0,105.0,54.0,57.0,122.0,77.0,83.0,68.0,45.0,79.0,81.0,98.0,76.0,120.0,130.0,81.0,57.0,107.0,80.0,82.0,92.0,147.0,126.0,64.0,107.0,61.0,102.0,60.0,87.0,96.0,64.0,152.0,38.0,129.0,127.0,95.0,55.0,76.0,50.0,201.0,89.0,228.0,116.0,94.0,50.0,19.0,93.0,71.0,59.0,48.0,93.0,50.0,80.0,218.0,76.0,51.0,122.0,52.0,37.0,87.0,109.0,93.0,82.0,63.0,122.0,118.0,70.0,55.0,37.0,87.0,106.0,130.0,121.0,98.0,63.0,75.0,50.0,46.0,100.0,105.0,74.0,122.0,53.0,68.0,63.0,36.0,107.0,60.0,183.0,129.0,86.0,52.0,47.0,95.0,52.0,60.0,115.0,88.0,77.0,126.0,87.0,40.0,62.0,54.0,159.0,53.0,61.0,72.0,205.0,66.0,53.0,111.0,39.0,106.0,49.0,95.0,58.0,175.0,119.0,118.0,94.0,120.0,43.0,109.0,106.0,66.0,48.0,84.0,91.0,69.0,90.0,107.0,56.0,69.0,74.0,116.0,82.0,55.0,108.0,65.0,42.0,59.0,123.0,45.0,99.0,61.0,69.0,54.0,126.0,51.0,109.0,67.0,125.0,67.0,41.0,84.0,49.0,89.0,184.0,127.0,66.0,98.0,54.0,49.0,46.0,111.0,114.0,125.0,42.0,76.0,159.0,78.0,67.0,61.0,74.0,116.0,90.0,81.0,66.0,76.0,108.0,54.0,55.0,55.0,118.0,69.0,82.0,123.0,67.0,52.0,50.0,81.0,108.0,78.0,164.0,63.0,73.0,48.0,108.0,65.0,120.0,62.0,73.0,114.0,63.0,103.0,125.0,127.0,78.0,72.0,49.0,52.0,129.0,58.0,45.0,135.0,80.0,128.0,54.0,61.0,41.0,60.0,124.0,239.0,58.0,87.0,56.0,183.0,114.0,116.0,33.0,78.0,118.0,97.0,106.0,50.0,48.0,80.0,99.0,128.0,50.0,65.0,122.0,97.0,61.0,55.0,111.0,62.0,94.0,68.0,52.0,41.0,55.0,58.0,108.0,43.0,43.0,43.0,53.0,50.0,38.0,51.0,63.0,91.0,76.0,129.0,118.0,41.0,103.0,53.0,73.0,60.0,101.0,123.0,88.0,125.0,36.0,57.0,59.0,52.0,55.0,114.0,162.0,96.0,99.0,118.0,43.0,25.0,60.0,101.0,50.0,77.0,77.0,96.0,169.0,91.0,111.0,76.0,70.0,74.0,59.0,78.0,113.0,46.0,42.0,126.0,55.0,78.0,111.0,71.0,81.0,105.0,110.0,54.0,97.0,43.0,92.0,46.0,102.0,90.0,115.0,118.0,125.0,52.0,128.0,101.0,123.0,87.0,29.0,74.0,55.0,59.0,57.0,68.0,44.0,105.0,121.0,50.0,59.0,205.0,75.0,123.0,72.0,195.0,52.0,50.0,92.0,82.0,91.0,63.0,90.0,97.0,107.0,123.0,111.0,86.0,95.0,98.0,78.0,123.0,74.0,55.0,107.0,67.0,99.0,78.0,37.0,116.0,90.0,90.0,112.0,71.0,53.0,83.0,40.0,63.0,56.0,74.0,127.0,82.0,51.0,59.0,100.0,53.0,72.0,36.0,122.0,93.0,88.0,85.0,63.0,57.0,81.0,99.0,113.0,55.0,58.0,170.0,69.0,79.0,104.0,54.0,80.0,71.0,127.0,62.0,112.0,61.0,94.0,76.0,64.0,73.0,102.0,123.0,122.0,204.0,46.0,101.0,53.0,47.0,51.0,129.0,77.0,57.0,72.0,52.0,125.0,56.0,115.0,140.0,75.0,106.0,42.0,46.0,107.0,68.0,98.0,53.0,41.0,97.0,49.0,57.0,82.0,81.0,90.0,117.0,52.0,46.0,102.0,58.0,125.0,57.0,56.0,77.0,45.0,88.0,59.0,42.0,39.0,82.0,66.0,54.0,126.0,80.0,113.0,66.0,73.0,111.0,118.0,30.0,45.0,63.0,169.0,86.0,51.0,84.0,94.0,51.0,118.0,56.0,118.0,118.0,71.0,140.0,50.0,79.0,117.0,100.0,66.0,86.0,60.0,61.0,91.0,46.0,57.0,121.0,129.0,52.0,99.0,73.0,54.0,48.0,85.0,80.0,80.0,77.0,88.0,106.0,81.0,115.0,107.0,111.0,124.0,67.0,94.0,78.0,113.0,109.0,111.0,108.0,79.0,33.0,50.0,89.0,48.0,105.0,54.0,66.0,53.0,74.0,58.0,96.0,120.0,68.0,62.0,53.0,77.0,66.0,118.0,103.0,87.0,115.0,84.0,60.0,130.0,98.0,45.0,95.0,53.0,119.0,46.0,119.0,62.0,106.0,128.0,43.0,119.0,76.0,144.0,109.0,112.0,75.0,66.0,87.0,87.0,60.0,77.0,46.0,58.0,130.0,78.0,117.0,41.0,85.0,76.0,83.0,66.0,43.0,69.0,59.0,206.0,98.0,96.0,53.0,114.0,113.0,159.0,43.0,80.0,51.0,47.0,61.0,55.0,55.0,203.0,79.0,114.0,116.0,108.0,72.0,47.0,46.0,126.0,125.0,47.0,98.0,63.0,96.0,65.0,47.0,58.0,74.0,64.0,93.0,76.0,71.0,74.0,75.0,117.0,55.0,76.0,53.0,43.0,116.0,43.0,58.0,80.0,160.0,78.0,233.0,47.0,46.0,125.0,123.0,87.0,57.0,72.0,175.0,118.0,64.0,112.0,117.0,58.0,62.0,50.0,66.0,73.0,117.0,109.0,53.0,61.0,92.0,46.0,112.0,100.0,82.0,62.0,72.0,99.0,65.0,72.0,76.0,119.0,111.0,45.0,92.0,50.0,47.0,49.0,85.0,58.0,76.0,80.0,119.0,49.0,189.0,59.0,58.0,54.0,81.0,93.0,58.0,110.0,186.0,55.0,53.0,94.0,269.0,102.0,76.0,49.0,121.0,63.0,48.0,98.0,122.0,89.0,110.0,71.0,67.0,126.0,81.0,46.0,58.0,76.0,106.0,55.0,59.0,57.0,116.0,46.0,91.0,52.0,56.0,53.0,87.0,98.0,197.0,100.0,83.0,48.0,74.0,56.0,125.0,81.0,130.0,105.0,69.0,130.0,104.0,83.0,66.0,61.0,61.0,74.0,66.0,39.0,61.0,122.0,126.0,146.0,74.0,56.0,82.0,74.0,46.0,117.0,62.0,94.0,96.0,56.0,47.0,52.0,61.0,111.0,103.0,43.0,113.0,73.0,49.0,116.0,104.0,127.0,86.0,100.0,263.0,130.0,69.0,78.0,49.0,42.0,102.0,49.0,127.0,60.0,90.0,77.0,119.0,72.0,70.0,120.0,97.0,94.0,28.0,74.0,62.0,87.0,88.0,90.0,58.0,79.0,121.0,129.0,88.0,116.0,47.0,88.0,122.0,56.0,53.0,79.0,66.0,59.0,81.0,114.0,87.0,39.0,41.0,46.0,94.0,83.0,37.0,203.0,50.0,113.0,92.0,45.0,111.0,74.0,67.0,81.0,96.0,243.0,54.0,51.0,80.0,66.0,103.0,53.0,184.0,74.0,75.0,128.0,36.0,52.0,46.0,60.0,48.0,122.0,60.0,76.0,124.0,81.0,66.0,89.0,57.0,109.0,82.0,127.0,72.0,66.0,68.0,85.0,56.0,130.0,93.0,49.0,64.0,71.0,65.0,60.0,179.0,72.0,107.0,94.0,48.0,124.0,64.0,103.0,129.0,73.0,39.0,93.0,118.0,105.0,41.0,60.0,43.0,88.0,74.0,77.0,67.0,51.0,69.0,127.0,65.0,134.0,65.0,39.0,130.0,81.0,75.0,92.0,42.0,53.0,66.0,41.0,85.0,48.0,75.0,89.0,55.0,79.0,56.0,115.0,125.0,75.0,92.0,58.0,88.0,124.0,55.0,76.0,46.0,51.0,54.0,119.0,47.0,70.0,51.0,82.0,47.0,96.0,118.0,105.0,84.0,95.0,117.0,77.0,59.0,112.0,105.0,53.0,78.0,283.0,51.0,80.0,115.0,118.0,70.0,49.0,65.0,47.0,130.0,66.0,53.0,94.0,93.0,108.0,79.0,99.0,101.0,42.0,45.0,286.0,129.0,117.0,104.0,112.0,74.0,120.0,90.0,51.0,48.0,71.0,58.0,73.0,70.0,105.0,210.0,59.0,88.0,62.0,63.0,63.0,56.0,128.0,30.0,112.0,127.0,55.0,66.0,41.0,94.0,100.0,59.0,83.0,66.0,112.0,61.0,65.0,107.0,78.0,55.0,72.0,69.0,48.0,54.0,62.0,124.0,62.0,75.0,50.0,81.0,82.0,55.0,71.0,64.0,116.0,58.0,160.0,128.0,102.0,56.0,64.0,102.0,70.0,66.0,70.0,121.0,52.0,112.0,100.0,97.0,174.0,96.0,108.0,77.0,122.0,77.0,90.0,57.0,63.0],\"xaxis\":\"x\",\"y\":[1.03,1.26,0.6,0.8,0.27,0.0,1.6,1.29,0.13,1.39,1.94,0.98,1.26,0.47,1.8,0.75,0.5,0.01,1.03,0.0,0.88,0.55,0.39,0.39,0.37,1.33,0.54,2.67,0.5,1.67,3.25,0.65,0.53,0.7,0.07,2.11,0.81,2.28,0.45,1.35,0.56,0.58,1.23,0.0,1.58,0.0,0.15,0.16,0.34,0.85,0.41,0.73,2.13,0.34,0.75,2.76,0.41,0.0,0.09,1.27,1.0,1.85,0.59,1.06,1.11,0.91,0.29,1.2,2.2,1.08,1.16,1.37,0.77,0.0,0.33,1.31,2.48,1.65,0.0,0.77,2.18,1.92,2.22,0.88,0.0,1.22,2.82,2.74,0.94,0.41,0.18,0.95,0.91,1.43,0.8,0.34,0.75,0.79,0.49,0.44,0.8,0.43,0.65,0.03,1.59,1.37,1.41,0.0,1.96,0.0,1.1,1.43,1.6,1.73,0.0,0.19,0.0,2.51,0.3,0.67,1.78,0.69,2.42,0.37,0.48,3.45,0.0,0.58,0.81,1.38,0.02,0.04,0.8,1.3,0.76,1.56,0.38,2.37,0.81,1.13,2.76,0.47,1.98,0.06,0.0,0.37,0.84,0.48,2.63,1.32,0.0,0.0,1.7,3.99,1.23,0.72,1.23,1.98,1.02,1.33,0.72,0.0,0.91,2.47,0.53,1.31,0.84,1.2,0.11,0.0,1.2,0.97,1.93,0.71,2.04,2.42,0.0,1.39,1.14,1.27,1.85,0.9,2.21,2.1,1.13,0.6,2.31,0.07,0.42,0.14,0.48,2.12,1.53,1.2,1.48,2.3,0.58,0.0,1.48,1.51,0.82,0.2,0.43,0.37,0.5,1.69,1.45,2.68,0.68,0.0,1.88,0.0,0.06,1.9,1.09,2.19,1.06,1.43,1.88,1.79,0.68,0.02,1.64,1.55,2.72,2.29,1.0,0.6,0.8,0.9,0.0,1.05,0.0,2.53,0.07,1.92,3.11,0.36,0.41,1.86,2.18,0.75,1.13,0.28,1.76,0.0,0.4,1.06,0.73,1.86,0.52,0.07,0.95,2.09,0.82,2.82,1.66,0.71,0.94,1.49,2.23,0.0,3.17,2.18,1.9,1.79,2.19,0.29,1.99,0.96,2.18,2.54,1.41,0.22,0.18,0.24,0.0,2.27,1.03,0.84,0.47,0.6,1.21,2.5,4.1,1.7,0.27,0.04,0.21,2.02,0.0,1.18,0.69,0.6,0.07,0.26,0.79,1.32,0.04,0.42,1.93,0.42,1.47,1.33,0.82,0.51,2.08,0.72,2.14,0.0,1.68,2.1,3.36,1.84,0.0,0.66,0.0,0.0,1.18,1.41,1.22,1.94,1.26,1.82,0.8,1.7,0.44,1.09,0.89,1.76,1.34,0.96,0.0,2.04,1.75,0.17,0.81,1.34,3.06,2.27,0.0,0.59,0.06,2.37,1.11,0.08,0.27,0.0,0.0,0.0,1.94,1.83,0.0,0.87,0.07,0.0,0.33,0.59,0.35,0.78,0.12,0.82,0.19,1.56,0.66,0.0,1.85,1.39,0.97,0.01,1.31,0.2,0.01,0.75,0.68,0.0,3.62,0.76,1.06,0.0,2.35,1.56,1.5,0.0,1.28,0.37,1.71,0.44,0.02,0.0,0.55,0.65,0.0,1.47,2.62,2.01,0.15,1.15,1.58,0.94,1.7,1.18,0.82,0.68,0.93,1.59,0.25,0.0,0.0,0.72,0.2,0.0,0.03,1.04,0.0,0.92,1.57,2.16,0.73,1.23,0.29,0.27,0.57,0.37,0.49,0.17,0.11,0.05,0.0,0.61,0.96,0.89,1.69,0.66,0.52,0.0,1.12,2.89,0.0,2.01,0.0,1.2,0.35,1.08,0.57,0.38,0.0,0.11,0.0,1.6,2.92,0.89,0.0,0.39,1.91,1.65,0.44,0.15,0.5,1.1,0.0,0.0,0.81,1.14,1.29,1.17,0.55,0.41,0.0,0.68,1.26,0.95,1.67,2.58,1.43,1.94,0.0,2.18,0.66,0.01,0.03,2.5,1.67,0.18,0.18,0.98,0.73,3.06,1.42,2.4,0.03,1.28,0.59,0.87,0.01,0.0,2.1,1.25,0.24,2.07,0.79,0.1,0.41,1.93,1.41,0.99,2.08,1.03,3.71,0.23,2.04,0.0,0.96,0.15,0.76,0.0,0.6,1.14,0.64,2.03,0.42,2.01,0.13,3.06,0.0,0.15,0.97,0.55,0.48,1.18,0.0,1.69,1.97,0.05,1.06,0.42,1.54,1.2,0.83,2.62,1.85,1.29,0.61,0.04,1.5,0.53,1.38,2.62,0.12,2.3,0.18,1.14,0.67,0.04,1.44,0.2,1.75,0.79,0.0,0.1,0.62,1.15,1.8,0.0,0.15,0.0,1.92,0.34,0.45,1.68,2.09,0.87,0.41,0.59,1.94,1.12,0.0,0.0,1.56,0.87,0.39,2.14,2.36,1.03,1.25,1.15,0.19,1.71,2.62,0.35,0.85,1.28,0.31,2.4,0.34,1.32,0.86,1.04,0.87,2.4,1.6,0.0,2.49,2.34,0.32,0.29,1.24,2.19,0.95,2.31,0.13,0.38,0.86,1.24,1.51,0.36,2.63,1.05,0.72,0.83,2.35,1.27,0.0,1.08,0.21,1.14,0.44,0.5,0.0,0.45,0.0,0.53,0.32,1.36,0.0,0.43,1.29,1.4,0.72,0.44,0.0,0.91,0.26,1.3,0.0,2.78,0.0,1.11,0.0,0.1,0.41,1.08,0.79,0.98,0.62,1.54,0.45,1.36,1.54,0.54,1.23,0.0,2.31,1.77,0.09,0.97,2.88,1.65,1.43,0.19,1.89,0.42,0.57,0.13,0.61,0.53,1.14,1.76,0.7,0.21,0.32,2.15,0.8,0.82,2.36,0.04,0.13,1.75,1.9,0.0,0.0,1.32,1.16,0.12,0.0,1.13,2.25,0.5,2.76,0.68,1.15,0.0,0.29,2.54,0.54,0.0,2.82,1.59,0.73,0.95,0.43,0.0,0.85,0.55,1.43,2.2,0.3,1.66,1.88,2.2,0.29,0.49,0.53,1.67,2.02,1.25,0.29,0.16,0.35,1.06,1.0,1.09,2.0,1.87,0.73,0.04,0.9,0.97,0.07,0.73,1.22,1.27,1.48,2.09,0.81,0.05,0.0,0.64,0.57,1.26,0.0,0.35,0.17,1.37,0.73,0.0,1.02,0.58,1.54,1.09,1.24,1.51,0.0,1.89,0.0,0.49,0.0,1.95,1.28,2.6,0.62,1.12,1.04,1.27,2.56,1.16,0.86,0.12,1.51,1.57,0.82,0.69,0.06,1.73,0.94,2.6,3.19,1.72,0.77,0.77,0.0,1.5,0.0,0.4,0.92,1.2,0.36,2.43,0.98,0.71,0.12,0.71,2.02,2.5,0.0,0.0,1.29,1.26,0.43,0.65,1.06,1.84,1.67,0.9,0.8,0.44,2.31,0.57,2.2,1.21,0.99,1.18,0.0,0.74,0.61,0.62,0.79,1.07,2.29,1.6,0.88,1.28,0.06,1.55,0.58,1.76,0.0,0.0,1.5,1.39,0.71,0.82,0.23,1.48,1.71,2.42,0.23,2.19,1.07,1.71,0.0,2.47,1.47,2.19,0.37,2.03,1.66,1.06,0.78,0.26,0.93,0.0,3.4,1.28,0.72,1.35,2.14,2.24,1.8,2.9,2.44,3.18,0.16,0.04,1.12,0.21,2.93,0.94,1.54,0.0,0.64,0.39,0.44,0.0,2.28,0.59,0.65,2.34,0.62,1.15,0.0,1.52,0.59,1.69,0.26,0.2,1.29,2.12,2.23,0.69,0.04,1.6,1.56,0.96,0.18,1.14,0.94,1.1,1.21,1.89,1.61,0.22,1.06,0.33,2.86,1.2,1.11,0.33,1.57,0.61,0.86,1.7,1.29,1.24,1.37,1.4,0.21,0.35,1.37,1.59,0.0,2.72,0.35,0.71,0.0,1.02,0.36,2.21,1.4,0.65,1.2,0.66,0.33,0.25,0.0,0.0,0.55,0.69,0.0,0.45,1.41,1.49,2.39,3.83,0.31,3.71,1.15,2.27,0.5,0.91,1.24,0.5,1.7,0.05,0.28,0.0,0.14,0.21,0.57,1.48,1.57,1.35,2.93,0.84,0.24,0.49,0.0,0.09,1.76,2.23,0.0,2.43,1.34,1.45,1.36,0.37,1.49,0.0,0.26,0.0,2.68,3.33,1.71,0.0,0.41,1.06,0.0,0.66,0.19,0.29,0.0,0.0,0.69,0.0,1.7,0.76,0.0,0.17,0.69,2.5,2.12,0.14,0.36,2.11,1.0,0.94,0.04,1.18,0.58,2.17,0.07,1.19,0.48,0.51,0.61,0.0,2.16,0.36,0.79,0.04,1.78,0.01,0.42,1.41,0.0,2.85,0.0,0.34,0.95,0.55,0.37,0.08,1.05,0.77,0.0,0.78,0.62,1.69,1.11,1.94,1.41,1.57,0.71,0.75,0.0,1.2,2.11,0.7,0.98,1.43,2.74,1.04,0.38,1.97,2.1,1.64,2.37,0.61,0.5,0.7,2.13,0.28,1.98,3.23,2.61,0.73,0.39,1.17,2.38,1.84,1.14,2.51,0.0,0.57,0.85,0.77,0.3,1.16,0.51,0.53,0.0,0.0,1.89,1.66,0.0,0.83,0.87,0.0,0.25,0.51,1.5,1.45,2.29,1.77,0.0,1.62,1.6,0.0,1.82,0.54,1.81,0.64,1.16,0.27,0.61,1.36,0.94,1.04,2.93,1.31,0.65,1.34,1.05,0.0,0.0,0.81,1.56,0.34,0.48,0.66,0.14,0.37,1.48,1.16,1.75,1.79,0.0,1.68,0.0,1.72,1.52,3.09,0.2,1.07,1.33,0.77,1.2,0.0,1.25,0.68,2.06,0.47,1.28,1.43,0.88,0.67,0.8,1.12,0.69,0.12,0.01,0.94,1.91,0.78,0.7,0.32,2.82,0.01,0.97,0.23,0.95,3.05,0.0,1.66,0.55,0.96,2.39,0.58,1.28,0.0,0.0,2.02,1.43,0.54,1.41,0.96,1.24,1.67,0.95,0.38,0.0,1.79,1.45,1.55,0.99,2.5,2.1,0.76,0.67,0.0,1.9,0.64,1.82,0.36,0.79,0.81,0.1,1.01,2.07,3.8,0.6,0.98,1.86,0.34,2.71,0.39,1.2,2.86,0.14,0.0,0.95,1.06,1.18,0.64,0.0,0.23,1.08,0.69,0.94,0.0,1.11,1.63,1.26,0.19,1.04,0.65,0.0,0.74,1.44,2.2,0.72,0.52,0.59,2.01,0.71,0.69,0.21,1.04,0.91,2.17,1.03,0.28,0.97,0.53,1.1,0.73,0.82,2.09,1.85,1.33,0.16,0.19,2.03,0.48,0.54,1.41,0.49,0.89,0.0,1.03,2.45,0.07,0.3,2.9,1.67,3.16,0.0,1.81,0.71,1.61,1.42,2.03,0.0,1.12,1.67,0.0,0.38,2.23,1.22,0.71,1.34,1.04,3.24,0.0,0.0,0.83,1.32,0.93,1.09,0.0,0.14,0.35,1.82,0.33,2.26,2.19,1.09,1.93,0.47,3.41,0.4,2.45,1.77,1.94,0.14,2.13,1.43,1.36,1.18,1.19,1.22,0.91,1.19,0.79,1.07,1.17,1.49,1.31,0.13,1.37,1.66,0.0,0.0,2.48,0.69,0.48,0.4,0.05,2.67,0.67,0.22,0.0,2.03,0.37,0.5,0.0,0.09,2.34,0.54,1.18,1.23,2.21,1.67,2.16,0.48,2.49,2.2,2.21,0.45,0.92,1.14,0.0,0.06,0.24,1.06,0.36,3.44,0.78,2.82,1.69,0.93,1.97,0.83,0.29,2.51,0.12,0.0,0.28,1.15,2.11,2.79,1.01,1.55,0.63,0.53,1.11,0.73,0.46,1.55,0.27,0.0,0.42,0.98,1.26,2.13,0.29,1.0,3.17,1.23,0.57,0.88,1.82,2.22,0.69,0.06,1.71,0.49,0.1,0.38,1.68,1.11,1.01,0.56,2.85,1.81,0.08,1.95,0.0,0.12,0.0,0.68,0.99,0.56,0.18,2.42,1.44,2.11,0.02,0.48,0.14,2.94,0.0,1.67,2.6,0.07,0.62,0.79,1.76,0.77,0.31],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"color=0\\u003cbr\\u003eprice_in_dollar=%{x}\\u003cbr\\u003ehost_reported_average_tip=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"0\",\"showlegend\":true,\"x\":[132.0,246.0,243.0,183.0,161.0,248.0,120.0,253.0,190.0,184.0,98.0,194.0,197.0,158.0,224.0,166.0,188.0,125.0,248.0,101.0,195.0,135.0,252.0,252.0,235.0,200.0,226.0,195.0,228.0,206.0,162.0,185.0,194.0,252.0,244.0,161.0,87.0,201.0,141.0,176.0,155.0,136.0,206.0,135.0,184.0,93.0,128.0,172.0,209.0,170.0,177.0,210.0,160.0,90.0,139.0,251.0,214.0,260.0,159.0,124.0,258.0,245.0,217.0,204.0,166.0,162.0,193.0,106.0,150.0,156.0,118.0,254.0,101.0,203.0,175.0,107.0,229.0,245.0,151.0,143.0,209.0,162.0,111.0,215.0,156.0,101.0,235.0,228.0,228.0,190.0,255.0,176.0,127.0,151.0,146.0,128.0,259.0,218.0,148.0,141.0,184.0,197.0,183.0,96.0,163.0,255.0,212.0,99.0,86.0,136.0,141.0,253.0,114.0,182.0,99.0,193.0,199.0,220.0,123.0,138.0,217.0,181.0,191.0,89.0,217.0,205.0,240.0,141.0,133.0,215.0,257.0,151.0,82.0,231.0,94.0,199.0,217.0,252.0,201.0,133.0,187.0,156.0,231.0,257.0,126.0,206.0,168.0,232.0,221.0,254.0,217.0,153.0,234.0,177.0,197.0,143.0,144.0,228.0,174.0,211.0,106.0,204.0,235.0,257.0,200.0,139.0,192.0,190.0,206.0,135.0,119.0,254.0,137.0,132.0,165.0,205.0,246.0,193.0,141.0,170.0,189.0,188.0,92.0,163.0,189.0,162.0,194.0,256.0,131.0,194.0,206.0,199.0,224.0,158.0,224.0,175.0,226.0,194.0,189.0,154.0,170.0,228.0,193.0,144.0,137.0,179.0,200.0,148.0,132.0,255.0,159.0,146.0,202.0,125.0,173.0,189.0,154.0,133.0,151.0,174.0,195.0,152.0,122.0,222.0,206.0,103.0,238.0,163.0,132.0,184.0,219.0,254.0,205.0,118.0,146.0,169.0,136.0,189.0,204.0,148.0,190.0,206.0,205.0,207.0,199.0,184.0,192.0,187.0,253.0,161.0,159.0,224.0,260.0,133.0,213.0,145.0,212.0,245.0,166.0,245.0,250.0,219.0,150.0,229.0,232.0,159.0,178.0,152.0,134.0,174.0,130.0,155.0,182.0,187.0,164.0,238.0,82.0,192.0,149.0,201.0,137.0,107.0,182.0,246.0,147.0,160.0,249.0,246.0,257.0,151.0,227.0,197.0,253.0,260.0,244.0,258.0,219.0,175.0,152.0,260.0,191.0,196.0,109.0,230.0,121.0,146.0,184.0,182.0,228.0,190.0,180.0,147.0,231.0,232.0,196.0,173.0,238.0,250.0,197.0,138.0,245.0,134.0,183.0,153.0,195.0,166.0,144.0,103.0,182.0,146.0,82.0,231.0,235.0,209.0,136.0,199.0,101.0,238.0,169.0,215.0,146.0,250.0,179.0,172.0,253.0,198.0,155.0,220.0,219.0,186.0,155.0,204.0,166.0,182.0,167.0,228.0,165.0,123.0,251.0,193.0,181.0,256.0,203.0,238.0,166.0,198.0,200.0,179.0,191.0,174.0,201.0,179.0,157.0,194.0,143.0,180.0,233.0,188.0,196.0,103.0,211.0,204.0,240.0,212.0,165.0,245.0,155.0,152.0,221.0,154.0,206.0,141.0,160.0,113.0,215.0,206.0,249.0,115.0,184.0,206.0,157.0,195.0,259.0,159.0,150.0,172.0,187.0,229.0,233.0,112.0,187.0,174.0,243.0,136.0,144.0,182.0,124.0,201.0,135.0,86.0,242.0,83.0,242.0,256.0,131.0,249.0,198.0,187.0,128.0,139.0,168.0,200.0,242.0,243.0,239.0,146.0,192.0,239.0,227.0,223.0,201.0,205.0,137.0,169.0,136.0,224.0,120.0,147.0,100.0,84.0,206.0,166.0,248.0,148.0,203.0,151.0,222.0,179.0,198.0,93.0,257.0,143.0,215.0,131.0,220.0,155.0,204.0,234.0,186.0,259.0,220.0,200.0,213.0,226.0,137.0,183.0,173.0,231.0,148.0,243.0,178.0,253.0,253.0,233.0,139.0,205.0,152.0,218.0,254.0,213.0,185.0,167.0,140.0,163.0,167.0,253.0,237.0,159.0,164.0,250.0,109.0,138.0,235.0,255.0,237.0,143.0,159.0,138.0,204.0,141.0,129.0,213.0,244.0,151.0,161.0,219.0,174.0,258.0,208.0,162.0,90.0,176.0,212.0,217.0,171.0,181.0,207.0,190.0,196.0,192.0,91.0,186.0,259.0,210.0,252.0,139.0,131.0,245.0,178.0,236.0,154.0,153.0,162.0,142.0,149.0,137.0,242.0,186.0,143.0,202.0,152.0,189.0,163.0,170.0,158.0,150.0,227.0,136.0,216.0,236.0,91.0,187.0,195.0,142.0,181.0,123.0,245.0,109.0,143.0,97.0,86.0,237.0,97.0,193.0,233.0,200.0,188.0,230.0,99.0,109.0,164.0,185.0,147.0,243.0,155.0,232.0,243.0,132.0,166.0,257.0,222.0,239.0,226.0,174.0,180.0,176.0,107.0,143.0,154.0,208.0,167.0,199.0,236.0,187.0,102.0,102.0,254.0,236.0,144.0,201.0,254.0,259.0,215.0,243.0,220.0,243.0,96.0,158.0,84.0,134.0,144.0,151.0,244.0,191.0,193.0,143.0,116.0,206.0,172.0,147.0,165.0,250.0,150.0,223.0,162.0,98.0,218.0,215.0,146.0,143.0,216.0,198.0,170.0,221.0,178.0,190.0,196.0,202.0,151.0,172.0,240.0,105.0,165.0,167.0,254.0,198.0,172.0,199.0,235.0,196.0,168.0,260.0,141.0,191.0,243.0,225.0,235.0,184.0,171.0,240.0,196.0,196.0,179.0,235.0,188.0,205.0,163.0,111.0,178.0,238.0,148.0,160.0,182.0,150.0,157.0,152.0,200.0,149.0,205.0,170.0,235.0,97.0,103.0,88.0,173.0,224.0,111.0,165.0,241.0,161.0,204.0,239.0,90.0,249.0,197.0,182.0,228.0,134.0,220.0,195.0,165.0,156.0,168.0,188.0,180.0,160.0,238.0,158.0,233.0,136.0,151.0,165.0,187.0,254.0,198.0,240.0,141.0,253.0,217.0,147.0,204.0,202.0,141.0,162.0,133.0,211.0,241.0,177.0,254.0,151.0,180.0,188.0,199.0,224.0,205.0,163.0,181.0,253.0,238.0,152.0,203.0,162.0,178.0,89.0,155.0,233.0,138.0,256.0,215.0,135.0,137.0,217.0,237.0,146.0,222.0,235.0,142.0,214.0,145.0,220.0,193.0,228.0,133.0,202.0,233.0,219.0,246.0,215.0,130.0,189.0,183.0,242.0,257.0,189.0,235.0,257.0,171.0,165.0,254.0,224.0,134.0,150.0,177.0,122.0,192.0,161.0,209.0,254.0,186.0,193.0,182.0,211.0,121.0,166.0,251.0,175.0,198.0,243.0,151.0,170.0,186.0,146.0,160.0,175.0,248.0,196.0,246.0,170.0,159.0,176.0,191.0,153.0,139.0,182.0,171.0,164.0,146.0,132.0,244.0,259.0,127.0,202.0,159.0,160.0,238.0,240.0,169.0,112.0,226.0,122.0,188.0,195.0,248.0,204.0,221.0,147.0,230.0,154.0,219.0,253.0,88.0,225.0,244.0,255.0,132.0,147.0,239.0,205.0,158.0,176.0,243.0,203.0,100.0,257.0,249.0,162.0,220.0,184.0,170.0,211.0,97.0,220.0,182.0,191.0,189.0,233.0,184.0,148.0,253.0,134.0,142.0,150.0,156.0,161.0,231.0,223.0,161.0,130.0,118.0,149.0,259.0,204.0,220.0,200.0,139.0,209.0,234.0,148.0,166.0,144.0,144.0,239.0,212.0,168.0,190.0,151.0,188.0,231.0,178.0,179.0,153.0,176.0,106.0,215.0,190.0,145.0,172.0,232.0,245.0,88.0,226.0,161.0,142.0,171.0,183.0,223.0,227.0,159.0,162.0,190.0,180.0,125.0,171.0,138.0,158.0,95.0,182.0,149.0,174.0,222.0,244.0,165.0,164.0,167.0,145.0,157.0,174.0,214.0,210.0,252.0,159.0,153.0,139.0,179.0,233.0,255.0,231.0,217.0,146.0,147.0,163.0,249.0,213.0,211.0,159.0,217.0,190.0,168.0,133.0,145.0,235.0,166.0,182.0,169.0,184.0,183.0,114.0,222.0,152.0,83.0,248.0,141.0,222.0,134.0,154.0,226.0,132.0,170.0,192.0,113.0,153.0,202.0,170.0,181.0,207.0,194.0,114.0,193.0,256.0,176.0,90.0,161.0,240.0,130.0,153.0,102.0,164.0,144.0,154.0,167.0,182.0,150.0,191.0,172.0,143.0,216.0,221.0,102.0,198.0,179.0,221.0,177.0,202.0,199.0,253.0,154.0,190.0,127.0,176.0,237.0,238.0,143.0,158.0,245.0,190.0,146.0,177.0,220.0,244.0,155.0,200.0,194.0,171.0,139.0,206.0,194.0,194.0,217.0,151.0,140.0,138.0,232.0,134.0,235.0,248.0,202.0,197.0,162.0,200.0,158.0,120.0,107.0,226.0,164.0,132.0,134.0,243.0,210.0,240.0,196.0,169.0,142.0,189.0,147.0,175.0,101.0,195.0,189.0,218.0,164.0,246.0,135.0,97.0,134.0,211.0,161.0,146.0,172.0,98.0,166.0,171.0,127.0,160.0,144.0,186.0,210.0,166.0,238.0,135.0,222.0,183.0,184.0,174.0,168.0,202.0,189.0,96.0,254.0,201.0,217.0,210.0,140.0,165.0,249.0,189.0,239.0,201.0,153.0,156.0,214.0,251.0,176.0,237.0,188.0,243.0,239.0,245.0,167.0,215.0,139.0,94.0,203.0,244.0,140.0,104.0,216.0,171.0,202.0,200.0,210.0,189.0,208.0,172.0,172.0,122.0,219.0,140.0,146.0,195.0,186.0,134.0,259.0,177.0,144.0,229.0,238.0,248.0,205.0,133.0,111.0,181.0,136.0,132.0,196.0,242.0,146.0,258.0,158.0,216.0,239.0,255.0,134.0,243.0,156.0,224.0,166.0,150.0,156.0,233.0,154.0,250.0,120.0,188.0,89.0,107.0,156.0,120.0,111.0,212.0,250.0,135.0,137.0,139.0,217.0,239.0,220.0,163.0,84.0,165.0,187.0,173.0,157.0,134.0,243.0,166.0,216.0,159.0,200.0,193.0,223.0,145.0,222.0,100.0,161.0,230.0,259.0,167.0,143.0,121.0,225.0,187.0,222.0,176.0,140.0,159.0,142.0,215.0,172.0,154.0,161.0,189.0,205.0,152.0,241.0,94.0,172.0,259.0,197.0,214.0,178.0,186.0,142.0,147.0,252.0,147.0,205.0,145.0,252.0,206.0,239.0,151.0,186.0,170.0,178.0,211.0,185.0,138.0,236.0,180.0,117.0,183.0,207.0,190.0,133.0,166.0,83.0,216.0,202.0,187.0,210.0,177.0,208.0,128.0,163.0,159.0,195.0,178.0,160.0,231.0,217.0,250.0,222.0,216.0,258.0,257.0,175.0,118.0,175.0,245.0,216.0,238.0,181.0,220.0,221.0,94.0,154.0,192.0,159.0,186.0,163.0,190.0,232.0,214.0,180.0,105.0,197.0,254.0,229.0,239.0,201.0,141.0,98.0,154.0,240.0,259.0,173.0,106.0,224.0,190.0,142.0,184.0,175.0,161.0,215.0,200.0,260.0,182.0,159.0,207.0,181.0,196.0,174.0,127.0,153.0,191.0,212.0,238.0,200.0,113.0,257.0,204.0,145.0,228.0,210.0,197.0,136.0,168.0,209.0,185.0,152.0,138.0,247.0,243.0,137.0,152.0,153.0,164.0,89.0,160.0,235.0,237.0,190.0,190.0,194.0,138.0,171.0,257.0,213.0,199.0,135.0,192.0,157.0,165.0,144.0,149.0,187.0,234.0,198.0,147.0,191.0,178.0,158.0,199.0,196.0,142.0,162.0,165.0,171.0,202.0,155.0,182.0,146.0,162.0,135.0,176.0,181.0,92.0,146.0,192.0,194.0,162.0,196.0,234.0,162.0,253.0,183.0,191.0,217.0,185.0,170.0,195.0,102.0,226.0,114.0,140.0,186.0,220.0,145.0,144.0,185.0,205.0,182.0,165.0,225.0,246.0,118.0,176.0,258.0,223.0,236.0,170.0,142.0,167.0,238.0,230.0,199.0,98.0,244.0,252.0,160.0,193.0,225.0,181.0,162.0,138.0,233.0,194.0,181.0,243.0,253.0,238.0,181.0,129.0,225.0,194.0,134.0,206.0,135.0,90.0,165.0,159.0,127.0,207.0,179.0,222.0,169.0,111.0,250.0,196.0,136.0,181.0,238.0,251.0,185.0,90.0,177.0,167.0,227.0,190.0,192.0,169.0,220.0,136.0,177.0,190.0,148.0,210.0,257.0,182.0,222.0,130.0,198.0,199.0,190.0,152.0,84.0,161.0,211.0,221.0,199.0,253.0,180.0,193.0,253.0,146.0,191.0,187.0,189.0,173.0,150.0,166.0,199.0,131.0,180.0,182.0,239.0,145.0,157.0,238.0,258.0,146.0,237.0,198.0,256.0,158.0,142.0,171.0,180.0,218.0,148.0,199.0,249.0,257.0,231.0,221.0,194.0,198.0,175.0,178.0,83.0,195.0,199.0,152.0,244.0,226.0,139.0,166.0,119.0,207.0,155.0,131.0,212.0,174.0,214.0,158.0,191.0,243.0,181.0,198.0,141.0,250.0,254.0,150.0,93.0,244.0,174.0,259.0,164.0,197.0,255.0,170.0,138.0,190.0,242.0,218.0,215.0,144.0,228.0,177.0,195.0,231.0,158.0,201.0,141.0,256.0,184.0,191.0,240.0,223.0,210.0,194.0,250.0,222.0,228.0,219.0,172.0,177.0,210.0,221.0,201.0,131.0,99.0,251.0,197.0,146.0,121.0,240.0,200.0,200.0,206.0,189.0,166.0,245.0,168.0,131.0,215.0,176.0,200.0,144.0,179.0,223.0,187.0,125.0,164.0,219.0,202.0,108.0,195.0,196.0,253.0,200.0,228.0,190.0,181.0,213.0,191.0,172.0,183.0,241.0,254.0,201.0,258.0,200.0,198.0,201.0,182.0,136.0,141.0,244.0,209.0,225.0,148.0,240.0,244.0,119.0,249.0,166.0,206.0,172.0,227.0,215.0,182.0,162.0,175.0,136.0,191.0,234.0,112.0,244.0,144.0,160.0,175.0,127.0,145.0,168.0,107.0,224.0,250.0,140.0,233.0,237.0,195.0,114.0,171.0,239.0,207.0,183.0,193.0,244.0,165.0,187.0,229.0,210.0,186.0,209.0,234.0,199.0,160.0,137.0,132.0,162.0,212.0,189.0,241.0,168.0,166.0,117.0,125.0,131.0,197.0,255.0,190.0,216.0,155.0,83.0,150.0,251.0,260.0,115.0,157.0,240.0,144.0,187.0,182.0,140.0,92.0,238.0,167.0,243.0,160.0,251.0,178.0,87.0,235.0,166.0,111.0,219.0,255.0,254.0,239.0,247.0,247.0,222.0,195.0,166.0,194.0,193.0,92.0,202.0,144.0,249.0,202.0,237.0,258.0,135.0,255.0,221.0,214.0,166.0,218.0,201.0,189.0,137.0,224.0,193.0,225.0,233.0,237.0,169.0,196.0,135.0,157.0,231.0,207.0,230.0,196.0,208.0,253.0,179.0,115.0,199.0,226.0,190.0,217.0,140.0,180.0,167.0,242.0,130.0,214.0,161.0,114.0,86.0,222.0,118.0,189.0,140.0,234.0,147.0,175.0,194.0,181.0,209.0,192.0,138.0,162.0,211.0,185.0,130.0,140.0,193.0,257.0,236.0,173.0,255.0,254.0,231.0,212.0,236.0,138.0,250.0,92.0,126.0,175.0,179.0,116.0,136.0,203.0,190.0,127.0,198.0,245.0,191.0,240.0,151.0,151.0,133.0,92.0,226.0,94.0,135.0,140.0,115.0,139.0,178.0,110.0,258.0,156.0,251.0,108.0,230.0,207.0,131.0,243.0,239.0,139.0,224.0,195.0,153.0,197.0,237.0,214.0,169.0,210.0,242.0,160.0,254.0,200.0,227.0,213.0,250.0,240.0,259.0,192.0,119.0,139.0,133.0,151.0,184.0,169.0,134.0,135.0,227.0,214.0,233.0,259.0,110.0,186.0,232.0,229.0,129.0,174.0,157.0,224.0,220.0,150.0,248.0,145.0,251.0,150.0,173.0,258.0,198.0,147.0,195.0,220.0,237.0,202.0,234.0,250.0,172.0,83.0,132.0,202.0,126.0,147.0,151.0,252.0,221.0,125.0,200.0,150.0,235.0,162.0,158.0,148.0,166.0,248.0,258.0,160.0,200.0,226.0,208.0,242.0,212.0,225.0,207.0,140.0,148.0,135.0,229.0,181.0,225.0,101.0,237.0,137.0,258.0,90.0,138.0,198.0,241.0,218.0,139.0,232.0,147.0,200.0,144.0,126.0,222.0,240.0,214.0,213.0,211.0,136.0,213.0,138.0,135.0,162.0,151.0,147.0,163.0,109.0,248.0,89.0,144.0,216.0,202.0,173.0,113.0,139.0,245.0,208.0,141.0,226.0,156.0,168.0,181.0,161.0,232.0,190.0,178.0,185.0,94.0,238.0,133.0,186.0,211.0,165.0,143.0,212.0,255.0,138.0,176.0,248.0,213.0,252.0,241.0,206.0,218.0,230.0,237.0,188.0,139.0,150.0,221.0,164.0,124.0,188.0,231.0,245.0,173.0,237.0,218.0,195.0,177.0,166.0,222.0,173.0,190.0,136.0,171.0,204.0,139.0,223.0,172.0,178.0,178.0,152.0,133.0,142.0],\"xaxis\":\"x\",\"y\":[9.98,16.63,13.76,13.62,11.45,14.94,17.18,12.62,13.9,15.71,16.48,15.1,16.4,15.26,15.5,12.85,18.13,16.47,13.76,13.67,9.82,16.74,16.38,10.91,10.43,14.94,15.92,13.34,12.57,15.68,12.58,14.44,15.46,13.51,12.18,13.91,14.38,13.62,18.47,17.42,13.08,14.18,13.02,10.43,13.94,12.54,13.25,14.6,16.95,15.21,13.24,16.55,14.52,11.38,11.57,12.05,12.0,14.42,12.7,14.84,14.1,11.29,17.03,16.9,14.45,11.47,15.28,14.07,12.06,13.38,12.29,14.96,13.45,10.38,10.9,12.79,9.41,13.37,14.74,10.88,11.37,12.25,14.46,14.54,15.58,9.92,14.63,15.66,9.44,16.92,13.85,13.78,17.18,14.38,16.9,15.2,14.8,16.31,13.15,16.21,12.13,17.46,15.28,15.49,12.27,16.16,13.48,13.3,14.35,15.35,11.06,17.73,13.73,15.1,16.34,16.48,14.26,15.83,19.29,14.83,13.08,17.35,10.97,14.83,13.27,12.26,13.07,14.42,16.2,14.68,13.09,14.09,16.06,14.93,12.21,12.11,12.5,16.19,14.3,13.58,14.89,12.23,14.31,12.94,14.49,13.2,14.44,11.87,15.64,14.43,12.92,14.17,16.47,11.48,12.37,14.52,10.87,13.81,15.79,14.66,14.94,13.63,10.85,12.99,15.03,12.23,11.24,14.3,11.67,14.21,16.94,14.2,14.07,15.81,14.02,14.74,13.55,13.51,12.68,17.78,12.41,13.17,13.49,15.39,14.22,12.39,14.23,16.44,13.42,14.91,13.43,16.3,15.67,14.24,12.97,13.15,14.02,16.09,14.97,17.24,15.47,12.67,14.19,12.0,15.32,17.23,14.65,12.51,16.85,14.64,14.74,13.86,15.51,16.74,17.36,15.29,12.05,10.16,14.06,14.43,15.73,14.28,14.64,13.61,11.76,13.51,14.31,15.12,17.54,14.94,14.28,13.5,12.01,10.42,16.65,15.01,14.71,12.2,15.65,12.93,14.56,10.93,18.14,12.08,12.93,16.4,14.07,11.79,13.58,14.89,16.68,13.91,10.08,13.16,15.6,13.61,16.73,13.2,13.93,10.68,17.22,14.54,15.56,16.59,21.1,12.2,15.66,14.47,16.4,13.63,15.36,16.52,14.79,13.83,14.91,12.86,15.38,16.13,12.21,14.58,13.99,15.02,13.68,11.04,17.27,14.69,17.3,15.25,15.42,10.47,17.22,17.66,13.98,12.15,14.86,10.44,11.8,13.86,13.78,14.52,13.2,15.01,13.35,11.84,15.27,12.21,13.13,9.88,14.24,14.67,8.56,14.01,15.52,14.33,13.8,14.19,12.42,15.51,11.86,19.31,13.62,13.18,13.51,11.5,13.23,16.02,14.48,15.46,12.51,18.14,17.27,16.69,15.93,16.28,17.24,17.17,16.24,15.4,13.31,16.48,15.65,10.96,12.14,15.08,14.61,14.49,12.83,12.59,12.5,13.88,10.74,16.04,13.55,9.91,15.28,13.9,13.27,10.61,14.91,11.44,14.88,13.15,13.59,10.76,11.93,14.83,11.86,12.69,12.24,14.45,14.82,16.06,16.34,11.01,14.87,13.32,12.57,14.56,14.83,10.44,9.31,16.79,13.81,15.35,15.32,13.96,13.95,14.54,17.47,9.81,10.56,12.12,10.82,17.41,13.58,12.58,14.3,13.86,13.99,19.11,17.11,15.4,12.74,13.39,10.76,13.72,13.59,15.33,13.5,14.64,13.83,13.38,12.99,14.98,14.54,15.24,13.34,12.9,12.62,15.15,16.6,14.54,11.62,16.31,13.77,10.16,15.62,12.93,15.64,14.43,13.49,14.47,14.68,11.06,14.63,14.8,16.13,14.42,11.23,15.63,14.22,19.44,13.62,11.96,14.7,16.56,15.61,15.01,19.35,12.83,11.13,10.9,12.44,12.38,14.88,14.62,10.5,13.94,14.94,14.41,14.87,18.42,13.78,11.44,16.63,15.87,16.02,13.49,12.65,14.37,14.99,15.96,15.07,12.92,11.89,11.32,16.51,15.51,16.3,13.88,13.58,12.03,11.16,15.5,9.91,16.35,11.39,17.65,9.51,12.03,14.48,14.91,14.49,12.75,14.11,11.78,14.1,13.85,16.28,13.38,15.93,14.7,9.52,13.94,13.3,11.43,15.82,13.86,17.48,13.74,13.01,11.35,14.81,14.12,12.87,9.08,15.54,13.68,14.44,15.98,17.36,13.84,15.03,14.27,14.04,17.29,15.76,12.95,15.59,11.37,13.87,15.95,16.6,11.32,13.91,12.14,9.53,13.15,13.18,10.97,18.01,12.97,13.45,14.32,13.08,16.34,13.43,14.41,11.37,15.38,19.16,19.45,12.34,14.56,12.51,13.12,10.55,12.58,11.49,12.47,13.75,14.35,13.91,9.55,15.48,12.28,15.51,14.26,14.01,9.29,14.7,10.56,16.87,15.98,14.25,15.03,11.93,13.85,15.48,11.87,12.91,15.26,9.95,16.61,12.61,14.11,13.71,14.02,12.58,14.74,11.43,12.12,16.06,10.01,14.52,11.32,15.06,12.92,15.53,18.15,11.93,13.88,14.01,9.57,12.52,16.62,13.1,13.87,13.81,11.88,11.65,13.15,16.36,14.12,13.84,14.13,13.06,16.18,13.69,14.13,15.96,13.65,15.52,13.36,11.83,11.86,16.31,14.05,13.78,13.06,12.9,10.17,15.15,12.65,11.78,12.27,16.02,12.87,15.48,14.38,13.6,12.32,14.11,12.81,18.32,16.12,13.53,11.87,12.4,15.74,12.69,13.69,17.74,13.44,13.16,12.31,11.51,12.11,14.07,14.82,12.71,13.22,18.71,13.68,12.07,12.02,11.01,11.96,16.26,16.86,8.35,15.29,12.12,12.44,13.15,10.78,14.65,12.62,12.32,12.63,15.54,12.93,18.61,14.29,13.38,13.77,15.53,15.33,14.4,15.44,14.11,16.33,16.96,14.71,14.39,15.65,14.03,13.75,10.57,14.95,13.53,10.65,12.67,15.91,15.84,16.66,11.72,15.59,16.1,12.9,17.3,16.74,14.51,9.42,12.06,15.91,15.85,17.25,12.95,16.86,10.65,13.34,13.86,13.05,10.67,16.03,12.75,12.3,12.86,11.91,15.99,10.67,16.33,15.31,15.37,13.33,16.4,11.75,15.01,13.04,14.65,14.55,14.38,12.62,13.15,13.28,10.62,12.15,15.68,13.32,15.05,15.04,16.16,14.01,15.82,14.72,10.87,12.23,10.43,12.25,15.29,13.32,12.88,17.26,13.57,10.1,12.64,13.29,16.85,10.68,13.6,13.82,12.15,13.84,13.65,14.4,11.04,15.65,10.08,19.28,12.49,15.2,13.83,8.48,8.87,14.51,16.13,11.3,14.49,15.07,12.01,11.88,12.3,11.7,16.44,15.23,15.58,11.98,14.12,14.35,14.5,15.89,15.74,13.36,13.68,14.34,16.42,15.21,15.45,13.01,10.18,11.8,12.78,15.79,15.34,12.8,16.95,14.02,10.06,10.78,18.49,13.91,11.81,12.78,16.64,14.17,14.95,15.57,17.67,13.18,12.6,11.68,15.18,13.14,12.76,13.5,13.72,11.82,13.98,16.63,12.53,15.09,10.32,11.98,7.97,14.77,15.0,10.92,10.25,12.39,15.23,12.28,10.49,14.62,17.68,15.2,13.16,19.0,14.15,15.93,15.28,16.32,15.17,10.46,12.3,13.99,11.46,14.83,15.2,9.65,12.54,18.72,12.83,17.03,13.07,16.56,17.25,12.52,17.49,11.56,16.17,11.15,13.13,16.42,13.68,11.47,13.21,17.19,13.57,10.75,15.38,15.68,16.08,13.52,15.11,12.41,14.0,13.79,11.25,13.69,12.24,13.99,13.78,11.98,12.79,15.15,15.17,15.44,14.27,16.29,14.58,14.84,11.42,15.07,13.83,14.98,16.37,16.96,13.13,16.18,16.72,10.07,12.68,15.04,13.1,11.3,16.9,13.65,14.8,13.12,15.01,13.58,15.09,17.63,15.36,14.07,15.06,12.33,14.72,13.9,10.91,14.72,14.46,16.41,13.56,14.58,16.2,14.67,13.02,15.71,13.96,17.03,12.81,15.78,13.26,13.26,14.15,12.23,15.19,12.14,12.22,8.85,12.81,13.15,15.41,17.77,10.84,16.8,15.6,14.28,20.89,12.12,14.7,15.45,13.68,15.92,16.81,13.45,11.75,13.53,13.89,13.37,12.77,14.09,12.14,13.12,17.74,14.75,14.73,14.92,11.86,15.96,13.12,12.73,13.73,11.86,14.29,12.85,16.64,11.76,14.64,13.64,12.97,15.88,12.79,15.81,12.18,16.35,12.16,14.53,14.28,15.66,12.59,14.41,14.52,17.66,11.13,16.02,13.05,12.82,14.75,14.83,16.29,13.23,14.01,15.5,13.46,14.34,15.95,16.69,13.62,15.54,17.36,13.61,17.62,15.08,16.18,16.48,15.53,13.86,13.15,10.21,12.54,13.67,12.52,17.91,13.62,13.51,13.64,15.33,16.17,13.9,15.16,14.93,11.37,18.01,14.47,13.88,14.22,13.55,18.96,14.0,13.63,11.0,16.1,17.77,12.06,12.98,15.36,15.89,14.45,16.64,15.0,13.01,17.02,15.03,16.36,15.31,13.24,13.08,13.99,15.76,15.88,13.57,10.66,12.07,12.51,16.61,12.95,9.04,9.93,14.58,11.5,13.17,15.76,13.6,16.17,15.67,12.85,14.41,12.57,14.1,14.98,12.98,12.9,11.92,15.3,14.16,13.77,16.35,11.83,17.8,15.59,13.03,14.92,10.54,15.68,15.24,15.72,14.9,13.06,13.16,11.43,11.65,15.86,11.81,16.38,12.58,13.8,14.01,15.87,15.58,14.31,10.17,15.56,13.0,13.5,12.53,15.53,13.74,12.78,14.62,11.05,11.7,16.55,11.62,14.12,11.98,10.75,12.04,15.81,11.97,17.62,16.59,11.24,10.46,11.38,12.64,14.01,17.54,14.38,12.69,16.14,12.5,14.92,14.12,12.61,12.24,17.64,10.69,13.17,11.24,11.7,15.22,12.16,14.95,15.51,11.71,12.04,16.63,12.83,12.21,14.39,13.11,16.52,14.05,10.7,13.93,12.8,12.12,18.08,11.41,13.46,13.11,12.7,13.11,12.83,13.32,11.74,15.95,14.76,10.35,14.1,15.28,16.58,14.29,15.14,12.89,16.33,12.9,10.61,10.55,13.77,15.13,10.67,14.25,14.14,14.98,14.08,10.83,11.94,11.06,12.59,13.11,11.04,12.08,9.9,14.29,13.25,12.7,10.5,10.3,15.72,17.47,14.0,12.82,14.31,13.98,12.94,13.11,15.78,13.41,15.1,10.85,14.5,16.47,18.26,12.01,13.98,14.26,11.99,12.51,13.7,12.88,16.03,17.28,16.58,16.72,13.32,12.89,12.47,14.7,17.71,12.84,16.28,11.7,12.61,19.54,15.17,16.08,13.47,12.04,15.48,11.56,15.4,11.71,15.23,15.22,15.22,13.53,13.89,13.46,16.44,14.43,15.19,14.5,14.21,17.39,11.3,9.62,13.91,12.31,19.08,15.77,16.61,15.51,15.65,13.54,14.3,14.14,15.78,15.58,14.97,10.93,12.96,12.47,14.89,15.23,16.45,12.33,13.32,16.34,10.84,12.38,13.63,15.35,14.22,13.95,13.47,16.71,11.52,13.69,11.23,14.43,13.96,14.89,14.51,16.0,12.92,11.85,14.0,12.96,13.36,14.59,14.88,15.55,14.55,14.13,15.49,14.0,10.84,14.72,11.21,9.87,13.55,16.74,12.58,15.09,19.18,10.79,14.37,13.69,13.75,16.66,14.49,13.42,13.62,12.5,14.35,15.12,14.73,15.42,14.43,14.68,12.76,13.4,14.34,12.48,14.8,13.75,14.47,11.48,12.86,14.36,12.23,14.67,17.24,17.08,16.5,14.72,13.98,15.48,17.6,14.65,15.8,14.4,11.28,11.8,16.35,15.47,13.28,12.73,12.32,12.85,13.52,18.89,13.25,14.28,12.87,14.5,16.32,9.64,13.82,17.32,10.41,12.68,13.17,12.47,14.66,16.01,12.75,15.68,14.58,15.26,10.77,13.15,14.76,13.25,10.37,14.73,14.94,14.26,16.71,12.98,13.35,14.91,12.07,17.9,13.54,11.54,13.57,14.94,14.63,10.03,14.68,14.06,18.11,15.98,13.14,14.17,14.14,16.44,15.07,11.05,13.36,12.06,13.94,13.85,12.34,15.52,16.45,17.8,13.93,11.56,16.91,14.55,13.36,14.79,14.27,15.89,12.04,11.04,14.48,14.02,15.41,11.64,12.87,14.0,18.5,12.95,11.49,13.34,12.44,12.78,12.79,12.37,14.36,11.18,11.74,10.78,11.43,13.73,14.86,14.9,15.22,13.54,15.0,14.5,13.26,15.36,14.44,8.25,15.14,14.22,12.08,10.97,11.14,14.52,13.93,12.14,11.45,14.39,13.96,14.45,14.82,10.81,13.15,15.13,10.81,9.39,10.89,14.41,10.21,9.51,14.61,11.9,13.01,12.06,15.54,11.26,16.55,14.38,11.97,16.78,12.47,12.69,14.32,15.8,14.01,10.49,15.56,13.83,11.09,13.72,12.69,14.27,15.07,15.22,16.25,9.96,12.97,15.01,14.02,19.79,12.4,14.23,16.7,18.53,15.51,16.46,14.19,15.67,17.03,14.47,20.33,7.86,13.75,19.89,14.06,12.62,14.0,12.9,10.53,13.3,11.78,13.09,14.45,16.05,15.02,14.77,13.56,14.63,13.2,11.72,9.36,19.0,14.44,12.96,12.73,13.16,13.87,17.69,11.75,14.43,16.83,13.08,13.79,14.32,15.56,17.74,14.8,14.51,11.56,14.65,12.63,13.56,9.61,11.59,11.14,13.91,14.11,12.01,16.18,15.33,15.95,16.19,14.26,13.96,13.15,15.94,11.62,17.61,15.47,13.85,17.96,12.61,12.11,13.63,11.02,14.99,13.78,13.8,13.97,16.29,12.67,16.34,11.73,12.51,13.46,11.61,16.22,14.65,16.61,15.62,17.29,13.12,16.63,14.44,11.36,17.37,18.49,15.84,8.97,13.72,16.41,15.59,12.32,12.94,13.56,11.7,13.3,15.39,14.4,11.58,17.22,16.19,12.74,11.27,13.54,12.02,13.02,9.63,11.83,13.81,10.69,13.68,13.04,15.86,13.82,14.55,12.07,13.93,13.62,12.89,16.61,15.16,15.45,17.21,14.22,16.45,14.96,14.94,14.78,12.48,17.68,15.54,15.49,15.96,13.62,13.49,14.77,10.54,10.99,16.65,12.29,15.72,16.02,15.9,11.72,14.86,15.13,14.78,13.52,13.93,15.09,11.88,13.84,16.16,13.91,12.17,15.84,16.7,12.77,15.65,15.54,11.3,13.67,13.16,13.14,12.99,15.79,12.99,12.86,15.02,15.8,12.88,14.51,11.07,13.46,10.65,13.16,13.19,13.21,18.97,13.55,11.0,14.57,13.95,14.45,11.95,17.43,15.81,14.28,11.99,12.78,15.53,15.48,11.87,13.71,11.75,13.61,19.04,14.74,12.15,17.79,12.98,11.52,13.54,9.54,17.63,16.47,12.53,14.59,10.0,16.57,12.86,15.74,13.23,13.15,13.59,13.39,15.64,12.6,14.83,14.53,14.28,12.14,12.31,16.14,10.77,17.26,18.34,13.68,10.14,17.03,12.47,14.14,16.38,10.93,16.3,15.21,14.92,12.44,15.4,14.1,12.02,14.42,15.02,16.96,14.79,12.77,13.24,12.25,13.55,16.22,12.21,16.85,11.73,12.72,14.81,11.19,14.16,12.01,13.82,14.97,14.47,13.51,15.55,11.99,13.92,13.1,13.42,11.09,15.81,12.94,16.43,15.31,16.49,15.71,14.61,11.02,15.21,13.06,15.03,14.58,13.11,18.31,13.69,15.31,13.76,17.56,12.51,12.28,11.14,14.49,14.94,12.05,16.01,16.98,18.26,14.26,11.21,10.55,14.29,12.01,18.07,12.86,8.18,15.43,15.56,15.38,13.15,16.67,14.23,11.98,15.94,13.61,15.21,10.44,11.05,13.66,13.71,15.4,12.14,13.51,11.97,11.89,11.97,15.81,17.28,10.17,14.4,14.57,12.46,13.54,15.23,15.66,13.02,13.86,16.15,11.54,13.48,12.77,13.09,8.6,12.67,11.84,14.48,14.91,11.64,13.65,10.9,14.29,13.84,12.02,11.32,13.99,16.32,10.85,14.83,11.38,12.45,13.35,17.17,17.72,13.61,13.37,13.67,11.82,16.17,15.08,13.13,12.72,12.88,12.89,15.44,16.33,12.48,14.69,13.87,11.36,16.05,13.21,15.9,15.0,17.91,16.03,14.16,13.28,11.9,16.03,13.54,10.83,14.68,13.94,15.34,17.3,17.99,13.59,14.77,13.76,15.21,12.12,13.48,14.35,14.21,14.58,13.16,12.34,14.06,15.06,12.7,14.27,15.91,15.6,14.03,15.81,13.75,12.72,16.48,15.01,14.12,15.06,12.96,14.85,14.41,13.73,13.32,15.44,13.99,15.45,18.15,12.18,13.17,14.06,17.01,12.33,15.05,17.43,10.09,14.19,16.9,14.94,13.06,14.01,12.41,11.43,12.1,11.73,14.26,15.94,15.06,11.06,15.94,12.26,14.69,15.06,16.59,13.14,10.86,16.24,14.64,13.08,12.2,15.09,13.51,12.44,14.66,11.7,13.45,17.91,14.68,12.54,13.89,15.69,14.5,13.27,13.95,14.86,16.84,16.46,14.93,14.81,14.81,12.65,11.27,11.52,11.85,14.28,13.96],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"color=1\\u003cbr\\u003eprice_in_dollar=%{x}\\u003cbr\\u003ehost_reported_average_tip=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"1\",\"showlegend\":true,\"x\":[284.0,271.0,308.0,227.0,358.0,267.0,264.0,410.0,231.0,262.0,305.0,373.0,393.0,313.0,322.0,271.0,294.0,351.0,289.0,437.0,319.0,335.0,308.0,229.0,326.0,288.0,331.0,267.0,309.0,262.0,346.0,242.0,355.0,270.0,292.0,227.0,275.0,338.0,328.0,275.0,302.0,262.0,296.0,295.0,279.0,256.0,288.0,279.0,274.0,233.0,265.0,350.0,273.0,313.0,320.0,315.0,258.0,261.0,261.0,341.0,298.0,328.0,267.0,266.0,264.0,257.0,327.0,282.0,254.0,212.0,278.0,220.0,214.0,328.0,383.0,211.0,266.0,309.0,224.0,268.0,356.0,274.0,356.0,310.0,405.0,360.0,317.0,397.0,396.0,253.0,288.0,359.0,381.0,347.0,425.0,280.0,307.0,346.0,348.0,337.0,360.0,272.0,257.0,353.0,337.0,318.0,275.0,259.0,325.0,263.0,241.0,280.0,332.0,284.0,387.0,273.0,356.0,268.0,360.0,221.0,302.0,340.0,330.0,379.0,372.0,330.0,275.0,266.0,243.0,240.0,310.0,282.0,411.0,361.0,213.0,280.0,277.0,214.0,276.0,352.0,251.0,360.0,266.0,245.0,295.0,318.0,276.0,272.0,300.0,350.0,305.0,310.0,242.0,234.0,360.0,300.0,267.0,373.0,340.0,245.0,400.0,284.0,334.0,218.0,312.0,225.0,269.0,321.0,227.0,288.0,287.0,286.0,285.0,304.0,420.0,286.0,382.0,336.0,258.0,317.0,329.0,312.0,328.0,285.0,350.0,282.0,286.0,272.0,272.0,297.0,248.0,262.0,297.0,311.0,435.0,295.0,403.0,284.0,328.0,311.0,281.0,295.0,272.0,318.0,276.0,345.0,271.0,441.0,395.0,261.0,307.0,401.0,258.0,262.0,268.0,277.0,264.0,314.0,231.0,234.0,286.0,271.0,446.0,271.0,306.0,262.0,284.0,314.0,262.0,399.0,298.0,391.0,391.0,457.0,408.0,294.0,270.0,294.0,261.0,309.0,334.0,293.0,381.0,260.0,355.0,280.0,277.0,348.0,284.0,267.0,270.0,358.0,343.0,288.0,280.0,220.0,225.0,314.0,270.0,297.0,373.0,261.0,297.0,262.0,360.0,294.0,319.0,327.0,310.0,308.0,271.0,337.0,273.0,300.0,310.0,309.0,257.0,340.0,261.0,396.0,381.0,253.0,299.0,261.0,258.0,386.0,279.0,229.0,282.0,231.0,239.0,212.0,329.0,407.0,267.0,358.0,404.0,381.0,279.0,336.0,282.0,298.0,244.0,263.0,332.0,299.0,344.0,333.0,411.0,311.0,308.0,293.0,348.0,290.0,309.0,219.0,309.0,273.0,257.0,314.0,269.0,275.0,459.0,334.0,280.0,295.0,296.0,333.0,374.0,302.0,268.0,229.0,354.0,319.0,234.0,262.0,377.0,400.0,298.0,359.0,453.0,384.0,317.0,484.0,257.0,294.0,267.0,342.0,284.0,351.0,214.0,239.0,294.0,284.0,324.0,402.0,307.0,244.0,317.0,436.0,213.0,392.0,302.0,369.0,340.0,327.0,266.0,260.0,345.0,276.0,399.0,413.0,338.0,287.0,360.0,419.0,299.0,228.0,336.0,319.0,334.0,369.0,211.0,309.0,307.0,295.0,215.0,280.0,291.0,269.0,378.0,288.0,331.0,324.0,356.0,413.0,321.0,330.0,230.0,362.0,229.0,327.0,428.0,310.0,270.0,342.0,376.0,339.0,250.0,368.0,345.0,359.0,343.0,362.0,375.0,253.0,269.0,306.0,261.0,304.0,285.0,333.0,282.0,334.0,269.0,294.0,310.0,427.0,238.0,403.0,223.0,422.0,267.0,299.0,360.0,282.0,264.0,300.0,371.0,219.0,466.0,312.0,306.0,443.0,343.0,306.0,289.0,267.0,315.0,294.0,265.0,259.0,312.0,277.0,310.0,273.0,250.0,368.0,218.0,264.0,367.0,229.0,440.0,244.0,388.0,221.0,290.0,295.0,291.0,360.0,351.0,370.0,266.0,396.0,293.0,275.0,330.0,273.0,358.0,353.0,311.0,358.0,282.0,408.0,253.0,238.0,270.0,326.0,266.0,299.0,270.0,292.0,292.0,355.0,223.0,256.0,399.0,265.0,283.0,321.0,291.0,344.0,266.0,392.0,321.0,218.0,319.0,328.0,334.0,369.0,217.0,306.0,212.0,265.0,276.0,267.0,407.0,296.0,300.0,322.0,351.0,279.0,334.0,429.0,419.0,395.0,375.0,474.0,386.0,297.0,273.0,288.0,346.0,281.0,374.0,314.0,270.0,454.0,225.0,276.0,265.0,261.0,263.0,414.0,283.0,289.0,343.0,332.0,339.0,270.0,306.0,312.0,284.0,238.0,245.0,296.0,389.0,340.0,323.0,414.0,418.0,214.0,442.0,300.0,343.0,358.0,316.0,295.0,298.0,277.0,326.0,346.0,280.0,277.0,262.0,386.0,285.0,326.0,230.0,295.0,344.0,257.0,419.0,431.0,302.0,213.0,277.0,367.0,315.0,320.0,362.0,221.0,318.0,400.0,349.0,239.0,295.0,219.0,283.0,302.0,314.0,345.0,300.0,304.0,280.0,281.0,274.0,381.0,314.0,387.0,261.0,302.0,271.0,211.0,398.0,328.0,315.0,319.0,300.0,290.0,359.0,390.0,270.0,329.0,275.0,254.0,298.0,295.0,410.0,284.0,412.0,400.0,267.0,308.0,320.0,261.0,312.0,406.0,313.0,329.0,382.0,275.0,290.0,248.0,328.0,373.0,282.0,280.0,372.0,349.0,278.0,312.0,278.0,239.0,309.0,341.0,241.0,333.0,248.0,294.0,271.0,251.0,274.0,335.0,380.0,307.0,310.0,298.0,425.0,322.0,323.0,339.0,301.0,289.0,332.0,444.0,370.0,330.0,254.0,383.0,281.0,274.0,284.0,364.0,320.0,294.0,313.0,226.0,235.0,304.0,302.0,288.0,289.0,363.0,246.0,381.0,304.0,438.0,289.0,276.0,297.0,314.0,223.0,350.0,309.0,277.0,365.0,307.0,262.0,287.0,274.0,296.0,302.0,317.0,502.0,312.0,289.0,282.0,310.0,305.0,297.0,336.0,275.0,261.0,294.0,366.0,293.0,317.0,383.0,265.0,242.0,306.0,314.0,360.0,262.0,314.0,292.0,324.0,443.0,273.0,279.0,288.0,326.0,224.0,320.0,287.0,308.0,328.0,385.0,307.0,363.0,315.0,373.0,308.0,330.0,285.0,475.0,285.0,343.0,399.0,313.0,289.0,297.0,312.0,336.0,292.0,235.0,349.0,308.0,289.0,353.0,371.0,293.0,279.0,453.0,298.0,329.0,288.0,279.0,219.0,222.0,270.0,262.0,335.0,255.0,262.0,318.0,273.0,267.0,309.0,408.0,264.0,313.0,300.0,342.0,267.0,459.0,238.0,293.0,297.0,261.0,284.0,276.0,270.0,249.0,311.0,283.0,268.0,415.0,386.0,274.0,257.0,276.0,354.0,393.0,400.0,358.0,298.0,267.0,313.0,232.0,336.0,309.0,217.0,319.0,244.0,381.0,268.0,270.0,406.0,408.0,278.0,355.0,276.0,361.0,230.0,269.0,274.0,307.0,270.0,351.0,272.0,302.0,381.0,269.0,302.0,305.0,269.0,270.0,332.0,366.0,262.0,298.0,261.0,425.0,292.0,275.0,262.0,333.0,281.0,321.0,369.0,266.0,321.0,287.0,321.0,300.0,344.0,274.0,391.0,324.0,306.0,230.0,302.0,275.0,343.0,317.0,237.0,418.0,264.0,429.0,370.0,321.0,262.0,264.0,374.0,273.0,386.0,314.0,302.0,355.0,254.0,339.0,301.0,318.0,305.0,299.0,237.0,221.0,217.0,243.0,398.0,277.0,267.0,356.0,426.0,426.0,348.0,416.0,302.0,262.0,269.0,288.0,356.0,227.0,287.0,267.0,354.0,262.0,240.0,266.0,287.0,282.0,317.0,450.0,378.0,328.0,271.0,436.0,279.0,248.0,254.0,405.0,335.0,342.0,305.0,223.0,277.0,342.0,443.0,462.0,280.0,312.0,372.0,347.0,392.0,367.0,282.0,271.0,289.0,447.0,237.0,294.0,270.0,362.0,294.0,314.0,401.0,291.0,288.0,392.0,273.0,275.0,285.0,308.0,299.0,345.0,302.0,298.0,349.0,278.0,270.0,345.0,332.0,293.0,399.0,288.0,323.0,390.0,448.0,279.0,306.0,324.0,278.0,275.0,277.0,311.0,296.0,267.0,351.0,424.0,305.0,355.0,467.0,277.0,355.0,213.0,267.0,262.0,306.0,401.0,285.0,294.0,315.0,293.0,260.0,220.0,317.0,290.0,397.0,258.0,306.0,299.0,219.0,342.0,271.0,371.0,301.0,444.0,314.0,275.0,282.0,354.0,437.0,269.0,343.0,241.0,273.0,394.0,324.0,332.0,321.0,299.0,265.0,324.0,273.0,269.0,339.0,406.0,275.0,429.0,306.0,248.0,314.0,271.0,215.0,330.0,273.0,294.0,217.0,320.0,307.0,284.0,252.0,228.0,251.0,297.0,264.0,273.0,341.0,261.0,251.0,276.0,274.0,292.0,352.0,488.0,437.0,334.0,292.0,327.0,414.0,379.0,267.0,318.0,377.0,267.0,266.0,303.0,231.0,227.0,363.0,274.0,280.0,268.0,314.0,414.0,324.0,299.0,276.0,336.0,352.0,330.0,319.0,468.0,383.0,259.0,277.0,322.0,314.0,293.0,273.0,309.0,281.0,258.0,324.0,281.0,265.0,367.0,291.0,237.0,288.0,378.0,401.0,279.0,215.0,253.0,244.0,444.0,371.0,323.0,315.0,326.0,427.0,299.0,261.0,268.0,293.0,310.0,297.0,280.0,305.0,430.0,323.0,319.0,300.0,232.0,372.0,338.0,362.0,292.0,219.0,323.0,309.0,322.0,256.0,315.0,221.0,263.0,306.0,290.0,322.0,467.0,238.0,284.0,264.0,383.0,294.0,346.0,274.0,306.0,316.0,323.0,269.0,288.0,304.0,280.0,343.0,307.0,282.0,301.0,267.0,492.0,278.0,369.0,253.0,408.0,359.0,242.0,299.0,269.0,327.0,360.0,266.0,277.0,279.0,267.0,296.0,270.0,389.0,289.0,322.0,382.0,324.0,355.0,266.0,248.0,399.0,367.0,340.0,362.0,424.0,275.0,314.0,481.0,274.0,353.0,267.0,264.0,271.0,275.0,374.0,306.0,332.0,223.0,300.0,266.0,430.0,297.0,261.0,324.0,309.0,382.0,288.0,317.0,241.0,436.0,282.0,405.0,271.0,305.0,394.0,312.0,255.0,255.0,217.0,350.0,225.0,427.0,377.0,218.0,271.0,291.0,287.0,273.0,272.0,241.0,266.0,305.0,446.0,371.0,284.0,230.0,322.0,323.0,320.0,332.0,360.0,268.0,371.0,300.0,315.0,299.0,429.0,363.0,322.0,309.0,284.0,261.0,394.0,295.0,250.0,447.0,264.0,293.0,268.0,271.0,262.0,239.0,372.0,298.0,348.0,319.0,223.0,316.0,306.0,325.0,387.0,291.0,259.0,288.0,310.0,415.0,301.0,282.0,361.0,429.0,311.0,264.0,307.0,315.0,294.0,239.0,268.0,331.0,365.0,285.0,305.0,371.0,307.0,290.0,396.0,295.0,314.0,332.0,262.0,254.0,415.0,310.0,228.0,305.0,274.0,285.0,269.0,397.0,268.0,279.0,293.0,314.0,270.0,283.0,386.0,314.0,291.0,267.0,261.0,343.0],\"xaxis\":\"x\",\"y\":[27.68,30.84,25.97,31.74,28.4,32.16,29.46,31.78,31.18,33.23,28.67,34.29,29.4,29.5,31.29,29.06,29.84,29.73,31.51,27.3,28.26,32.89,30.76,27.62,30.2,30.07,30.82,27.97,29.39,32.62,30.25,31.17,31.96,27.66,33.0,29.79,29.37,31.02,30.59,26.82,24.19,26.32,30.79,29.47,28.41,30.7,30.23,28.75,29.85,27.02,31.16,31.19,26.39,29.68,25.99,31.98,28.52,31.62,33.06,31.06,31.1,29.12,31.38,31.33,32.9,29.03,28.48,30.39,32.82,30.63,31.25,32.69,29.1,30.64,34.48,33.1,29.87,30.66,29.57,29.72,29.93,29.26,28.62,32.16,30.45,32.82,33.0,33.09,32.28,24.87,31.61,31.83,29.18,31.27,32.81,28.07,29.41,32.34,33.09,28.74,28.79,26.32,31.71,31.48,28.41,28.6,30.94,31.79,28.24,30.29,31.86,29.57,30.35,35.62,29.79,27.98,27.49,28.75,28.94,33.63,28.53,28.18,29.42,32.37,30.74,31.44,31.93,29.82,25.75,28.37,30.59,31.46,30.34,26.97,25.96,32.47,27.85,28.31,30.52,29.24,28.55,30.35,28.69,27.93,27.81,27.44,26.2,31.64,28.98,29.97,26.49,27.45,28.66,29.86,29.93,29.26,31.91,31.61,32.48,28.14,28.18,28.67,30.98,32.07,29.97,28.63,30.45,29.68,30.5,31.07,27.22,27.08,30.96,30.44,30.11,29.59,29.37,27.28,28.57,29.7,31.8,32.02,32.55,33.61,34.03,31.02,30.89,34.46,26.15,29.48,27.09,28.77,32.92,30.0,31.15,30.21,31.55,29.93,28.52,31.99,31.96,30.39,32.35,28.5,30.03,31.15,29.49,30.95,31.58,31.24,27.39,29.7,29.3,28.36,30.15,32.87,26.8,28.56,30.46,28.69,30.31,29.96,31.4,27.4,27.66,26.64,32.38,32.72,28.36,33.19,26.2,26.43,31.63,25.22,30.16,31.85,28.93,31.78,29.5,29.65,27.74,29.45,31.44,29.86,30.28,28.25,27.88,34.97,25.9,31.53,31.95,31.49,32.43,25.79,28.93,26.49,30.56,28.0,28.22,28.16,31.11,30.97,30.86,29.67,29.0,30.24,28.77,30.38,29.82,29.13,29.8,33.36,29.85,33.79,28.14,28.56,32.86,33.33,31.8,30.7,32.08,30.24,31.39,31.41,33.44,33.53,29.72,27.53,28.67,32.24,32.0,31.57,31.61,31.39,30.67,33.55,31.81,27.64,32.46,33.92,27.22,27.73,33.02,27.05,31.42,27.26,30.25,26.73,28.45,30.65,29.55,29.05,33.13,29.49,28.85,31.82,29.46,32.66,28.35,29.2,29.19,29.43,30.11,30.43,29.95,32.69,29.12,31.55,29.51,35.17,31.14,31.34,28.24,28.31,34.23,34.45,28.58,32.49,30.13,31.34,32.51,28.31,34.09,30.13,30.95,28.18,29.3,33.73,32.4,29.59,32.34,31.15,32.89,30.2,27.36,33.69,29.9,31.3,29.88,33.7,29.65,29.3,31.87,30.08,28.36,28.59,32.09,31.53,30.23,29.93,27.84,28.34,32.95,30.2,27.41,29.92,31.39,30.9,31.88,29.79,26.08,27.46,30.52,29.53,29.55,28.13,32.37,26.92,31.83,30.51,29.79,32.9,30.97,28.56,29.24,28.64,30.32,29.77,28.56,28.06,33.04,33.09,30.73,32.93,32.25,32.64,33.77,30.65,31.03,28.03,27.87,31.11,30.6,30.73,30.15,31.4,28.75,30.07,30.84,30.38,29.98,29.29,31.49,29.34,30.72,30.23,29.42,28.09,31.38,28.01,30.62,32.26,29.41,30.87,31.03,34.06,32.51,30.27,28.99,28.4,27.8,29.16,33.35,29.65,29.92,27.33,31.39,31.14,29.34,30.95,28.51,27.85,32.69,32.16,30.29,26.32,32.34,27.01,30.46,31.78,30.82,34.26,25.7,29.35,32.74,28.23,33.55,30.89,29.52,31.48,30.13,32.56,27.35,31.11,30.02,25.65,27.74,27.93,31.51,28.92,32.96,27.0,31.64,30.2,28.35,32.8,28.46,31.35,24.67,31.4,29.61,28.28,30.51,31.82,28.7,31.45,29.11,32.78,32.37,29.09,31.86,30.49,26.61,29.95,32.15,31.99,32.08,32.23,32.05,29.82,26.13,31.0,29.64,31.4,29.76,30.06,29.89,30.23,29.03,30.72,29.52,30.17,29.77,30.76,27.32,32.47,29.92,29.61,32.14,32.67,30.52,27.44,30.05,28.25,32.35,28.45,24.67,29.42,28.54,30.14,31.57,29.2,29.15,32.36,28.64,31.61,32.02,27.11,28.12,28.79,31.15,30.11,30.59,27.97,29.5,30.21,29.47,29.21,28.8,30.85,28.85,29.32,29.78,27.99,27.34,31.39,25.79,31.9,33.53,31.03,32.37,31.83,31.32,32.14,27.76,30.36,28.22,28.73,31.56,31.64,29.17,31.37,28.57,31.41,29.45,27.41,35.03,28.26,30.64,27.18,28.98,29.76,31.4,32.87,31.38,30.62,30.87,28.68,29.23,28.8,29.02,27.87,28.12,31.95,30.97,29.35,24.62,29.42,29.13,29.39,28.98,30.4,28.3,31.56,31.49,32.66,31.12,31.13,29.89,31.33,29.34,28.7,28.79,29.77,31.11,31.62,30.66,30.26,32.61,29.92,29.3,34.06,31.22,32.88,29.02,26.98,33.44,31.18,31.52,27.98,33.51,30.15,27.35,29.8,29.61,33.45,31.02,29.03,29.45,27.15,31.91,29.23,28.78,31.8,32.54,29.37,28.93,31.76,31.12,29.06,29.0,30.97,30.83,28.92,30.0,31.53,30.69,27.42,31.38,32.49,31.01,28.96,29.3,31.46,29.13,32.28,30.0,30.3,30.17,33.57,25.43,31.83,27.02,31.79,28.13,29.91,28.7,24.97,25.23,31.36,29.38,29.45,31.07,29.71,32.31,30.03,30.17,31.16,34.58,29.1,32.54,29.54,26.95,29.87,26.99,30.43,28.71,33.75,27.71,30.63,32.12,34.16,29.61,27.65,30.51,31.3,31.63,29.32,27.62,28.59,28.94,25.68,32.34,29.35,27.51,28.56,30.84,30.0,31.38,27.1,30.29,32.1,29.02,29.09,30.38,33.05,26.96,29.69,27.03,29.67,28.69,31.97,29.33,29.17,30.37,28.74,31.99,27.57,29.55,29.43,28.13,28.68,30.75,28.54,26.06,30.54,28.87,25.84,33.17,31.85,32.7,25.98,30.82,27.41,28.48,29.02,24.95,31.03,28.07,30.16,29.58,32.74,27.64,27.24,29.2,30.61,33.01,28.82,29.75,28.92,31.94,29.87,31.74,35.02,28.89,31.8,28.4,27.98,31.94,33.64,29.26,28.94,32.67,28.62,28.05,35.35,28.45,28.03,31.86,29.66,30.89,30.36,29.52,24.81,32.04,30.7,34.64,28.95,31.04,28.21,31.32,29.85,34.15,29.6,27.24,28.9,29.78,33.24,29.01,30.08,28.84,31.68,34.41,27.17,30.76,28.78,29.97,27.08,34.54,30.8,32.05,31.32,30.03,30.59,30.26,33.56,29.69,26.37,27.28,29.18,29.13,27.97,29.28,30.19,32.14,27.2,29.01,31.88,32.34,29.66,28.4,25.69,28.47,31.06,30.41,29.91,30.8,30.13,30.61,30.56,30.95,32.01,30.25,28.91,24.88,29.02,28.95,31.58,33.64,30.54,27.94,33.11,31.06,29.52,29.8,28.8,28.53,33.43,29.5,27.9,30.31,29.21,29.21,28.56,30.25,30.9,28.2,28.23,32.29,31.29,30.38,30.58,28.81,29.88,29.01,31.59,29.32,29.56,31.14,29.8,28.91,31.54,31.36,28.6,27.9,30.11,29.67,32.21,26.06,30.27,28.68,30.6,29.15,30.65,32.33,29.54,28.42,30.76,29.81,27.83,31.09,31.61,28.87,26.54,27.94,33.1,32.41,29.43,28.52,30.07,28.39,27.57,31.07,29.01,31.83,29.54,32.51,28.46,29.44,27.29,34.25,32.47,31.88,27.5,27.86,29.77,32.96,29.6,28.26,28.89,30.03,32.88,28.42,29.39,30.02,27.06,27.94,28.17,30.61,33.33,29.64,29.76,30.11,31.52,29.67,30.61,29.33,27.96,31.67,28.78,30.16,35.01,29.71,31.8,27.78,27.07,31.52,31.81,31.59,27.7,28.93,31.02,26.91,31.28,30.94,30.17,29.2,28.38,32.4,29.21,32.4,27.98,29.41,32.13,31.31,33.09,29.96,27.87,27.61,28.7,31.2,28.88,31.39,28.82,28.5,29.77,31.88,29.4,30.64,29.12,31.43,29.71,31.06,31.48,30.72,29.56,31.51,32.26,30.15,31.41,30.65,28.58,30.23,30.0,31.17,33.53,30.25,34.0,30.33,28.85,26.21,33.54,30.58,31.99,24.38,30.9,27.85,23.78,30.7,30.67,28.28,30.06,27.3,31.93,28.23,32.41,29.18,27.94,27.16,31.27,28.88,27.99,32.99,30.7,30.62,32.08,28.93,30.29,31.55,30.25,30.57,27.63,30.43,30.62,29.94,29.04,30.34,27.72,28.25,30.17,29.74,31.37,31.2,28.05,27.93,32.85,25.76,28.93,29.61,28.28,29.83,31.23,27.77,27.88,32.41,32.24,28.21,27.13,28.29,29.82,33.34,30.28,29.0,31.59,29.15,28.25,30.4,32.23,33.75,32.61,31.92,31.89,30.51,30.88,31.23,31.08,33.2,30.25,27.63,30.85,29.9,32.8,28.4,32.35,30.35,27.13,26.29,29.83,29.63,24.93,30.17,27.98,29.8,29.47,30.92,30.86,32.56,30.86,30.57,34.77,29.37,32.36,30.87,31.94,27.41,30.63,31.59,32.34,30.15,30.19,28.42,29.62,27.42,31.0,27.62,31.1,26.93,31.05,29.53,30.54,32.06,33.33,35.37,30.34,28.58,33.67,35.17,31.7,27.37,32.02,29.75,31.29,32.45,31.6,27.38,30.25,30.01,31.04,29.12,29.27,29.36,33.38,28.93,31.36,30.93,32.49,33.93,27.17,31.6,29.33,31.53,27.94,29.49,28.95,29.64,29.59,28.0,28.11,30.24,30.96,30.72,27.51,27.65,31.52,31.13,28.54,28.17,27.69,31.69,31.7,28.38,30.83,30.55,30.18,31.29,32.03,32.16,27.04,29.45,27.57,31.82,28.54,33.7,31.71,30.33,30.11,28.17,30.51,31.59,32.11,29.17,37.54,33.6,30.57,25.53,29.53,30.31,33.2,28.87,26.2,29.32,32.56,29.78,28.24,35.24,32.12,33.5,27.94,31.58,33.8,27.04,32.68,30.28,29.63,30.64,31.5,29.24,30.52,30.35,30.0,30.72,31.47,30.57,26.31,31.02,31.01,30.93,34.03,30.93,33.81,29.79,32.63,34.62,29.34,30.42,29.74,28.06,29.88,32.78,28.88,30.82,34.42,28.92,32.4,30.5,31.79,28.82,31.19,30.94,29.31,27.07,32.77,30.22,31.94,28.32,29.64,32.21,28.93,31.86,27.88,30.06,27.86,31.05,29.24,28.31,30.98,30.49,29.91,29.58,33.43,32.68,27.61,32.83,28.36,30.59,28.44,28.97,29.07,31.16,27.14,31.57,33.29,29.27,30.24,30.83,33.48,29.94,31.17,31.06,29.82,31.19,31.47,31.15,30.02,29.31,32.23,30.09,29.35,31.34,25.96,30.79,28.93,29.4,29.35,28.89,31.48,31.01],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"price_in_dollar\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"host_reported_average_tip\"}},\"legend\":{\"title\":{\"text\":\"color\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3c4cf981-0817-4227-9454-89c3574293f1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "fig = px.scatter(\n",
        "    df_list, x=\"price_in_dollar\", y=\"host_reported_average_tip\", color=Kmean_labels\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "sx4DVfbUS_n8"
      },
      "id": "sx4DVfbUS_n8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 6: Reassign as a new column to the Dataset\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_clc1wb7gf00023b6oaqcg6tn1)\n",
        "\n",
        "Let's add this list as a column to our dataset **df_list** using the name **listing_tipping_group**."
      ],
      "metadata": {
        "id": "ySBsjR3baGZ1"
      },
      "id": "ySBsjR3baGZ1"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[\"listing_tipping_group\"] = Kmean_labels"
      ],
      "metadata": {
        "id": "oVboBZZUZzbn"
      },
      "id": "oVboBZZUZzbn",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmean_labels"
      ],
      "metadata": {
        "id": "_TbpSiPaiWfc"
      },
      "id": "_TbpSiPaiWfc",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list[\"listing_tipping_group\"] = Kmean_labels\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "PaFqVgC3TNmV"
      },
      "id": "PaFqVgC3TNmV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 7: Manually split up Three into Four clusters\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_clc1wb7gf00023b6oaqcg6tn1)\n",
        "\n",
        "As you might've noticed, there are also listings that receive no tips at all. We want to recognize these listings as a separate group. Let's use Pandas for that to overwrite labels which have received no tips to the number 3."
      ],
      "metadata": {
        "id": "syM3H3c8Z0AL"
      },
      "id": "syM3H3c8Z0AL"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list.loc[df_list[\"host_reported_average_tip\"] == 0.00, \"listing_tipping_group\"] = \"3\"\n",
        "#df_list[df_list[\"listing_tipping_group\"] == 3]"
      ],
      "metadata": {
        "id": "neoxPM2QVA0Z"
      },
      "id": "neoxPM2QVA0Z",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list.loc[df_list[\"host_reported_average_tip\"] == 0.00, \"listing_tipping_group\"] = \"3\"\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "Qn9rvA97TaKL"
      },
      "id": "Qn9rvA97TaKL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make our labels a bit more \"expressive\", let's use a [`replace()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) operation to make the values like:\n",
        "\n",
        "- \"0\" to \"average tip\"\n",
        "- \"1\" to \"high tip\"\n",
        "- \"2\" to \"low tip\"\n",
        "- \"3\" to \"no tip\"\n"
      ],
      "metadata": {
        "id": "lJqVlKz_kx2f"
      },
      "id": "lJqVlKz_kx2f"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[\"listing_tipping_group\"] = df_list[\"listing_tipping_group\"].replace({\"0\": \"average tip\", \"1\": \"high tip\", \"2\": \"low tip\", \"3\": \"no tip\"})\n"
      ],
      "metadata": {
        "id": "5pRME2tSkxq9"
      },
      "id": "5pRME2tSkxq9",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list[\"listing_tipping_group\"] = df_list[\"listing_tipping_group\"].replace({\"0\": \"average tip\", \"1\": \"high tip\", \"2\": \"low tip\", \"3\": \"no tip\"})\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "GViulRfGTsm4"
      },
      "id": "GViulRfGTsm4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome, now let's encode these labels back into numbers, haha! You might think why did we do it in the first place? Well, the Scikit-learn encoder has a way to easily transform these labels back and forth between numbers and label names. So when we need labels, we just turn it back using a Scikit-learn function, which is easier than constantly having to replace values, like we did above (or remembering what the values meant)."
      ],
      "metadata": {
        "id": "Mimyr6L_l_Lo"
      },
      "id": "Mimyr6L_l_Lo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 8: Encode our Label\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_clc2384f100073b6ozta1negh)\n",
        "\n",
        "For the model/algorithm that we are building, we use the **listing_tipping_group** as our target ($y$) label. Let's assign this column as our *y* variable and encode it with a [`LabelEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ],
      "metadata": {
        "id": "8gzIdMIQbrU7"
      },
      "id": "8gzIdMIQbrU7"
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_list[\"listing_tipping_group\"]"
      ],
      "metadata": {
        "id": "YysQRzbubdfI"
      },
      "id": "YysQRzbubdfI",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "y = df_list[\"listing_tipping_group\"]\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "2nMCfMC-T6Le"
      },
      "id": "2nMCfMC-T6Le"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's encode it! Make sure to use ravel."
      ],
      "metadata": {
        "id": "sGeIPKv4oX53"
      },
      "id": "sGeIPKv4oX53"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import ravel  # change column-vector to 1d-array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_enc = LabelEncoder()\n",
        "y = label_enc.fit_transform(ravel(y))"
      ],
      "metadata": {
        "id": "QvaATIIpoXSN"
      },
      "id": "QvaATIIpoXSN",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from numpy import ravel  # change column-vector to 1d-array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_enc = LabelEncoder()\n",
        "y = label_enc.fit_transform(ravel(y))\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "F3r_b52aUDze"
      },
      "id": "F3r_b52aUDze"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and changing back one of the encoded labels by using [`inverse_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.inverse_transform). Input a value between 0 to 3 and see what comes back.\n",
        "\n",
        "*You might notice that the numbers don't coincide with what the numbers originally were when we generated the three clusters, this is okay and shouldn't influence our results*"
      ],
      "metadata": {
        "id": "HgLDwo-IogVD"
      },
      "id": "HgLDwo-IogVD"
    },
    {
      "cell_type": "code",
      "source": [
        "label_enc.inverse_transform(np.array([0, 1 ,3]))"
      ],
      "metadata": {
        "id": "UFAneZhjofEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451a5585-36dd-4b5c-d637-a73521642223"
      },
      "id": "UFAneZhjofEl",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['average tip', 'high tip', 'no tip'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 9: Set Booleans to Int\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_clc2384f100073b6ozta1negh)\n",
        "\n",
        "Taking a closer look at our dataset, reveals that we have three columns that are boolean, which we want to convert to integer."
      ],
      "metadata": {
        "id": "DkarfKyFbTZt"
      },
      "id": "DkarfKyFbTZt"
    },
    {
      "cell_type": "code",
      "source": [
        "list(df_list.select_dtypes(include=[\"bool\"]).columns)"
      ],
      "metadata": {
        "id": "yXrPaEipbsGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad0cc96-a946-4f46-b450-fe5d671a9e13"
      },
      "id": "yXrPaEipbsGb",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['host_is_superhost', 'has_availability', 'instant_bookable']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go ahead and convert it into int!"
      ],
      "metadata": {
        "id": "kRRlOts2soH1"
      },
      "id": "kRRlOts2soH1"
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[\"host_is_superhost\"] = df_list[\"host_is_superhost\"].astype(int)\n",
        "df_list[\"has_availability\"] = df_list[\"has_availability\"].astype(int)\n",
        "df_list[\"instant_bookable\"] = df_list[\"instant_bookable\"].astype(int)"
      ],
      "metadata": {
        "id": "JlqwqyYgsn6Y"
      },
      "id": "JlqwqyYgsn6Y",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "df_list[\"host_is_superhost\"] = df_list[\"host_is_superhost\"].astype(int)\n",
        "df_list[\"has_availability\"] = df_list[\"has_availability\"].astype(int)\n",
        "df_list[\"instant_bookable\"] = df_list[\"instant_bookable\"].astype(int)\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "yK-DXDcOUvob"
      },
      "id": "yK-DXDcOUvob"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 10: Split our Dataset\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/load-inspect-split#corise_clcut65qu00072a73x6glgvcr)\n",
        "\n",
        "Before we split our dataset we drop **price_in_dollar** and **host_reported_average_tip** since these were used to create **listing_tipping_group**."
      ],
      "metadata": {
        "id": "sF6hjGS7aP7Q"
      },
      "id": "sF6hjGS7aP7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = (\n",
        "    df_list[[\"host_acceptance_rate\", \"host_is_superhost\",\n",
        "             \"has_availability\", \"number_of_reviews_l30d\",\n",
        "             \"neighbourhood\", \"room_type\",\n",
        "             \"accommodates\", \"review_scores_rating\",\n",
        "             \"instant_bookable\", \"service_cost\"]] , # YOUR CODE HERE\n",
        "    df_list[\"listing_tipping_group\"] # YOUR CODE HERE\n",
        ")"
      ],
      "metadata": {
        "id": "LMFBCVidaPYh"
      },
      "id": "LMFBCVidaPYh",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "X, y = (\n",
        "    df_list[[\"host_acceptance_rate\", \"host_is_superhost\",\n",
        "             \"has_availability\", \"number_of_reviews_l30d\",\n",
        "             \"neighbourhood\", \"room_type\",\n",
        "             \"accommodates\", \"review_scores_rating\",\n",
        "             \"instant_bookable\", \"service_cost\"]],\n",
        "        df_list[[\"listing_tipping_group\"]]\n",
        "        )\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "fX5ypWVAU-5A"
      },
      "id": "fX5ypWVAU-5A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now that we split up the dataset into X and y, let's split it also up into training, validation and test set."
      ],
      "metadata": {
        "id": "oquiXp8vt6Pe"
      },
      "id": "oquiXp8vt6Pe"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_validation_test_split(\n",
        "    X, y, train_ratio: float, validation_ratio: float, test_ratio: float\n",
        "):\n",
        "    # Split up dataset into train and test, of which we split up the test.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=(1 - train_ratio), random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Split up test into two (validation and test).\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_test,\n",
        "        y_test,\n",
        "        test_size=(test_ratio / (test_ratio + validation_ratio)),\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    # Return the splits\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "# Splits according to ratio of 80/10/10\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
        "    X, y, 0.75, 0.15, 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ZlyNjnMPt6FZ"
      },
      "id": "ZlyNjnMPt6FZ",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 11: Encode the Numerical Variables\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/load-inspect-split#corise_clcut70vz00082a73zc30553d)\n",
        "\n",
        "Now the real interesting part starts where we turn the different numerical variable ranges to the same scale (From 0 to 1) by using a [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). This way, algorithms that are sensitive to scale will not regard some features more important than others, purely because of a scale that was initially much bigger.\n",
        "\n",
        "So let's prepare a pipeline for the numerical columns!"
      ],
      "metadata": {
        "id": "nSLIwYcCbgFB"
      },
      "id": "nSLIwYcCbgFB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the numerical columns\n",
        "numerical_cols_X = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Numerical pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    (\"scaler\", MinMaxScaler())\n",
        "])"
      ],
      "metadata": {
        "id": "Gm6caBe6Z_wq"
      },
      "id": "Gm6caBe6Z_wq",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the numerical columns\n",
        "numerical_cols_X = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Numerical pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "9wqFBrc4VJ_3"
      },
      "id": "9wqFBrc4VJ_3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 12: Encode the Categorical Variables\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/load-inspect-split#corise_clcut70vz00082a73zc30553d)\n",
        "\n",
        "Now, categorical variables often don't need scaling but they do need proper encoding. For this we'll use [`OneHotEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to achieve that.\n",
        "\n",
        "Make such a pipeline!"
      ],
      "metadata": {
        "id": "6i8mOqbRcKZA"
      },
      "id": "6i8mOqbRcKZA"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Select the categorical columns\n",
        "categorical_cols_X = X_train.select_dtypes(include=[\"category\"]).columns\n",
        "\n",
        "# Categorical pipeline: containing only one encoder\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "])"
      ],
      "metadata": {
        "id": "EHHniYnXcJ5U"
      },
      "id": "EHHniYnXcJ5U",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Select the categorical columns\n",
        "categorical_cols_X = X_train.select_dtypes(include=[\"category\"]).columns\n",
        "\n",
        "# Categorical pipeline: containing only one encoder\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "])\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "ipLfEHlDVd-r"
      },
      "id": "ipLfEHlDVd-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 13: Combine the Pipelines in ColumnTransformer\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/load-inspect-split#corise_clcut70vz00082a73zc30553d)\n",
        "\n",
        "Let's put the ColumnTransformer to good use and combine our two pipelines into one variable called *preprocessor*."
      ],
      "metadata": {
        "id": "yFQUdrjUyy1u"
      },
      "id": "yFQUdrjUyy1u"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Combine the two pipelines into one ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('cat', cat_pipeline, categorical_cols_X),  # Notify Transformer which cols to use.\n",
        "    ('num', num_pipeline, numerical_cols_X)  # Notify Transformer which cols to use.\n",
        "])"
      ],
      "metadata": {
        "id": "SS2unsysyyqw"
      },
      "id": "SS2unsysyyqw",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Combine the two pipelines into one ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('cat', cat_pipeline, categorical_cols_X),  # Notify Transformer which cols to use.\n",
        "    ('num', num_pipeline, numerical_cols_X)  # Notify Transformer which cols to use.\n",
        "])\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "0PZDcmRjWKPC"
      },
      "id": "0PZDcmRjWKPC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 14: Select our Model\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/scikit-advanced#corise_clbtolj4u00033b6ontw6x38q)\n",
        "\n",
        "In this week's project we are going to be using a [SVM classifier](https://youtu.be/_YPScrckx28) to make predictions! The implementation is called [`SVC()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and with pipelines is just as simple as plugging in other models like we see in [this example](https://corise.com/course/intermediate-python-for-data-science/v2/module/unsupervised-learning#corise_clc2384f100073b6ozta1negh).\n",
        "\n",
        "*Make sure to set the seed of the SVM/SVC algorithm/model.*"
      ],
      "metadata": {
        "id": "-b5o5X0DcSXe"
      },
      "id": "-b5o5X0DcSXe"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Combine the preprocesser with the Algorithm/model\n",
        "pipeline = Pipeline([\n",
        "    (\"pre\", preprocessor), # YOUR CODE HERE\n",
        "    (\"model\", SVC(random_state=SEED) # YOUR CODE HERE\n",
        "    )\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "ASCMurpFZ_q7"
      },
      "id": "ASCMurpFZ_q7",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Combine the preprocesser with the Algorithm/model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', SVC(random_state=SEED)\n",
        "    )\n",
        "])\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "WSRSQTFgWRkB"
      },
      "id": "WSRSQTFgWRkB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 15: Train the Model\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/unsupervised-learning#corise_cld3n6qss000s2a6ls9js7l7g)"
      ],
      "metadata": {
        "id": "hOm-iAxAwux4"
      },
      "id": "hOm-iAxAwux4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final pipeline on the training set.\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "dFJ5AymEwv9U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "857d522e-6858-44b9-c1aa-b025b062c78b"
      },
      "id": "dFJ5AymEwv9U",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning:\n",
            "\n",
            "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('pre',\n",
              "                 ColumnTransformer(transformers=[('cat',\n",
              "                                                  Pipeline(steps=[('ohe',\n",
              "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
              "                                                                                 sparse=False))]),\n",
              "                                                  Index(['neighbourhood', 'room_type', 'service_cost'], dtype='object')),\n",
              "                                                 ('num',\n",
              "                                                  Pipeline(steps=[('scaler',\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  Index(['host_acceptance_rate', 'host_is_superhost', 'has_availability',\n",
              "       'number_of_reviews_l30d', 'accommodates', 'review_scores_rating',\n",
              "       'instant_bookable'],\n",
              "      dtype='object'))])),\n",
              "                ('model', SVC(random_state=42))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pre&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;ohe&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                 sparse=False))]),\n",
              "                                                  Index([&#x27;neighbourhood&#x27;, &#x27;room_type&#x27;, &#x27;service_cost&#x27;], dtype=&#x27;object&#x27;)),\n",
              "                                                 (&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  Index([&#x27;host_acceptance_rate&#x27;, &#x27;host_is_superhost&#x27;, &#x27;has_availability&#x27;,\n",
              "       &#x27;number_of_reviews_l30d&#x27;, &#x27;accommodates&#x27;, &#x27;review_scores_rating&#x27;,\n",
              "       &#x27;instant_bookable&#x27;],\n",
              "      dtype=&#x27;object&#x27;))])),\n",
              "                (&#x27;model&#x27;, SVC(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pre&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;ohe&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                 sparse=False))]),\n",
              "                                                  Index([&#x27;neighbourhood&#x27;, &#x27;room_type&#x27;, &#x27;service_cost&#x27;], dtype=&#x27;object&#x27;)),\n",
              "                                                 (&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  Index([&#x27;host_acceptance_rate&#x27;, &#x27;host_is_superhost&#x27;, &#x27;has_availability&#x27;,\n",
              "       &#x27;number_of_reviews_l30d&#x27;, &#x27;accommodates&#x27;, &#x27;review_scores_rating&#x27;,\n",
              "       &#x27;instant_bookable&#x27;],\n",
              "      dtype=&#x27;object&#x27;))])),\n",
              "                (&#x27;model&#x27;, SVC(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pre: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;ohe&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                sparse=False))]),\n",
              "                                 Index([&#x27;neighbourhood&#x27;, &#x27;room_type&#x27;, &#x27;service_cost&#x27;], dtype=&#x27;object&#x27;)),\n",
              "                                (&#x27;num&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler())]),\n",
              "                                 Index([&#x27;host_acceptance_rate&#x27;, &#x27;host_is_superhost&#x27;, &#x27;has_availability&#x27;,\n",
              "       &#x27;number_of_reviews_l30d&#x27;, &#x27;accommodates&#x27;, &#x27;review_scores_rating&#x27;,\n",
              "       &#x27;instant_bookable&#x27;],\n",
              "      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;neighbourhood&#x27;, &#x27;room_type&#x27;, &#x27;service_cost&#x27;], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;host_acceptance_rate&#x27;, &#x27;host_is_superhost&#x27;, &#x27;has_availability&#x27;,\n",
              "       &#x27;number_of_reviews_l30d&#x27;, &#x27;accommodates&#x27;, &#x27;review_scores_rating&#x27;,\n",
              "       &#x27;instant_bookable&#x27;],\n",
              "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "JrorH6DGWgA5"
      },
      "id": "JrorH6DGWgA5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 16: Measure our Performance\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/first-algo#corise_clcuzcwbf00052a732gxqjbkd)\n",
        "\n",
        "During the course we've mentioned a few times there are more metrics. In this case let's use another such metric, [F1](https://towardsdatascience.com/the-f1-score-bec2bbc38aa6)! It is commonly used for Classification and therefore, let's try and implement [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)!\n",
        "\n",
        "*Set the parameter `average=\"macro\"` since for the `f1_score()` we have more than two classes that we are trying to predict (4).*"
      ],
      "metadata": {
        "id": "mbbn11SdcqP4"
      },
      "id": "mbbn11SdcqP4"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Do a \"trial exam\" using the validation set.\n",
        "y_predict = pipeline.predict(X_val)\n",
        "\n",
        "# Compare algorithms' \"trial exam\" vs. expected.\n",
        "f1_score(y_val, y_predict, average=\"macro\").round(4) # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "mrO7owjlZ_lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d64560a-74be-40aa-df3d-e3dfc0350237"
      },
      "id": "mrO7owjlZ_lq",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7946"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Do a \"trial exam\" using the validation set.\n",
        "y_predict = pipeline.predict(X_val)\n",
        "\n",
        "# Compare algorithms' \"trial exam\" vs. expected.\n",
        "f1_score(y_val, y_predict, average=\"macro\").round(4)\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "LBHX1VM3WoE7"
      },
      "id": "LBHX1VM3WoE7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### (Optional) Task 17: Confusion Matrix\n",
        "\n",
        "The confusion matrix is a great chart to visually capture how your model is performing. It shows you which labels were expected, and how they were actually predicted. The confusion matrix of the previous model reveals that:\n",
        "\n",
        "- The diagonal are correctly predicted labels, which for the majority seems right\n",
        "- 7 average tips were incorrectly identified to be high tip\n",
        "- 21 average tips were incorrectly identified to be low tip\n",
        "- 59 high tips were incorrectly identified to be average tip\n",
        "- 24 low tips were incorrectly identified as average tip\n",
        "- 6 no tips were incorrectly identified as average tip\n",
        "- 1 no tips were incorrectly identified as high tip\n",
        "- 4 no tips were incorrectly identified as low tip\n",
        "- sum up a row horizontally provides you with sum **actual** total of the class\n",
        "- sum up a row vertically provides you with a sum of **predicted** total of the class\n",
        "\n",
        "[This video](https://youtu.be/Kdsp6soqA7o?t=24) also clearly explains how to interpret it."
      ],
      "metadata": {
        "id": "uyA-8Ab_0SLN"
      },
      "id": "uyA-8Ab_0SLN"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.express as px\n",
        "\n",
        "conf_mat = confusion_matrix(y_val, y_predict, labels=pipeline[\"model\"].classes_)\n",
        "\n",
        "fig = px.imshow(conf_mat,\n",
        "                labels=dict(x=\"Predicted Label\", y=\"True Label\"),\n",
        "                x=pipeline[\"model\"].classes_,\n",
        "                y=pipeline[\"model\"].classes_,\n",
        "                text_auto=True)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YkxPeNyA1pLa"
      },
      "id": "YkxPeNyA1pLa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Optional) Task 18: Example prediction\n",
        "\n",
        "[*\\[Related section on Uplimit\\]*](https://uplimit.com/course/python-for-machine-learning/v2/module/first-algo#corise_clcuzcwbf00052a732gxqjbkdr)\n",
        "\n",
        "Now, just like last week, let's retrain our model, to only use a few features. In this case we'll go with 4 features. This model we'll use to predict in which tip bracket we fit and is used to make another Streamlit app with!\n",
        "\n",
        "Make a selection of these features for X_train and X_val:\n",
        "- \"review_scores_rating\"\n",
        "- \"room_type\"\n",
        "- \"service_cost\"\n",
        "- \"instant_bookable\""
      ],
      "metadata": {
        "id": "vIBePV2WHVCF"
      },
      "id": "vIBePV2WHVCF"
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import ravel  # change column-vector to 1d-array\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X_train = ... # YOUR CODE HERE\n",
        "X_val = ... # YOUR CODE HERE\n",
        "\n",
        "# Select the categorical columns\n",
        "categorical_cols_X = X_train.select_dtypes(include=[\"category\"]).columns\n",
        "numerical_cols_X = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Categorical pipeline: containing only one encoder\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "# Numerical pipeline: containing only one encoder\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Combine the two pipelines into one ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('cat', cat_pipeline, categorical_cols_X),\n",
        "    ('num', num_pipeline, numerical_cols_X)\n",
        "])\n",
        "\n",
        "# Combine the preprocesser with the Algorithm/model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', SVC(random_state=SEED))\n",
        "])\n",
        "\n",
        "# Train the final pipeline on the training set.\n",
        "pipeline.fit(X_train, ravel(y_train))"
      ],
      "metadata": {
        "id": "Br_pSFLjF5kA"
      },
      "id": "Br_pSFLjF5kA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "from numpy import ravel  # change column-vector to 1d-array\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X_train = X_train[[\"review_scores_rating\", \"room_type\", \"service_cost\", \"instant_bookable\"]]\n",
        "X_val = X_val[[\"review_scores_rating\", \"room_type\", \"service_cost\", \"instant_bookable\"]]\n",
        "\n",
        "# Select the categorical columns\n",
        "categorical_cols_X = X_train.select_dtypes(include=[\"category\"]).columns\n",
        "numerical_cols_X = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Categorical pipeline: containing only one encoder\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "# Numerical pipeline: containing only one encoder\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# Combine the two pipelines into one ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('cat', cat_pipeline, categorical_cols_X),\n",
        "    ('num', num_pipeline, numerical_cols_X)\n",
        "])\n",
        "\n",
        "# Combine the preprocesser with the Algorithm/model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', SVC(random_state=SEED))\n",
        "])\n",
        "\n",
        "# Train the final pipeline on the training set.\n",
        "pipeline.fit(X_train, ravel(y_train))\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "dD5A-KaPW9qj"
      },
      "id": "dD5A-KaPW9qj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run it fully, using the pipeline and observing the f1_score."
      ],
      "metadata": {
        "id": "FpA1y4lHFfd_"
      },
      "id": "FpA1y4lHFfd_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a \"trial exam\" using the validation set.\n",
        "y_predict = ... # YOUR CODE HERE\n",
        "\n",
        "# Compare algorithms' \"trial exam\" vs. expected.\n",
        "f1_score( ... ).round(4) # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "H9_V0SESFTWE"
      },
      "id": "H9_V0SESFTWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Show Solution</summary>\n",
        "\n",
        "```python\n",
        "# Do a \"trial exam\" using the validation set.\n",
        "y_predict = pipeline.predict(X_val)\n",
        "\n",
        "# Compare algorithms' \"trial exam\" vs. expected.\n",
        "f1_score(y_val, y_predict, average=\"macro\").round(4)\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "oqf2GIvhXHSZ"
      },
      "id": "oqf2GIvhXHSZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the metric performance has decreased, we were able to distill the number of features to only 4. Now let's try and see if we can predict a new listings' expected average tip."
      ],
      "metadata": {
        "id": "RS5S5qZl8b6J"
      },
      "id": "RS5S5qZl8b6J"
    },
    {
      "cell_type": "code",
      "source": [
        "# review_scores_rating: 0 to 5\n",
        "# room_type: ['Shared room', 'Private room', 'Hotel room', 'Entire home/apt']\n",
        "# service_cost: ['$0.99', '$4.99', '$2.99', '$10.99']\n",
        "# instant_bookable: 0, 1\n",
        "example = pd.DataFrame({\n",
        "    \"review_scores_rating\": [0.50],\n",
        "    \"room_type\": [\"Shared room\"],\n",
        "    \"service_cost\": [\"$0.99\"],\n",
        "    \"instant_bookable\": [0]\n",
        "    })\n",
        "\n",
        "pipeline.predict(example)[0]"
      ],
      "metadata": {
        "id": "nLDyQrDdlRcj"
      },
      "id": "nLDyQrDdlRcj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Today you've made a Machine Learning project, using Numpy, Pandas, Plotly and Scikit-Learn to create predictions given the Airbnb dataset! This is a simplified version of a professional workflow and is often how companies start, by first doing the bare minimum and then keep expanding the complexity of the model. That way you can have something working quickly, and improve steadily with a very quick feedback loop!\n",
        "\n",
        "Now as an extra we'll explore how to deploy this model as a Streamlit app!"
      ],
      "metadata": {
        "id": "SWpIjbkiKvcj"
      },
      "id": "SWpIjbkiKvcj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Make an App for Your Portfolio!\n",
        "\n",
        "<center>\n",
        "  <img src=https://griddb-pro.azureedge.net/en/wp-content/uploads/2021/08/streamlit-1160x650.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "**Participants such as yourselves often want to use the weekly Uplimit projects for their portfolios. To facilitate that, we've created this section. It might seem like a lot, but it's actually just following instructions and copy-pasting. Reach out on Slack if you get stuck!**\n",
        "\n",
        "You will make an app that uses the model you just created, encapsulates that in a neat Streamlit interface, where you can provide input through the use of sliders!\n",
        "\n",
        "<center>\n",
        "  <img src=https://corise-ugc.com/static/course/intermediate-python-for-data-science/assets/clgawo8po03cx12d029tbha5c/Screenshot%202023-04-10%20150104.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "To visualize this, we will again use a library called [Streamlit](https://streamlit.io/). For now you are not expected to know how Streamlit works, but you are expected to be able to copy-paste and follow instructions if you want to share this project as part of your portfolio!\n",
        "\n",
        "We are going to use [Streamlit Share](https://share.streamlit.io/) to host your projects. It's a website that allows us to host our interactive projects for free online! Again, we don't expect you to understand how to use and/or modify the code we will show below. We do expect you to read the instructions and copy-paste our code to the Streamlit Share platform. Feel free to change it any way you like. Some great starting points are [here](https://python.plainenglish.io/how-to-build-web-app-using-streamlit-pandas-numpy-5e134f0cf552), [here](https://docs.streamlit.io/library/get-started/create-an-app), [here](https://streamlit.io/components), and [here](https://streamlit.io/gallery)!"
      ],
      "metadata": {
        "id": "PvhgbGXiLVTc"
      },
      "id": "PvhgbGXiLVTc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# rename our pipeline to model\n",
        "model = pipeline\n",
        "\n",
        "# Dump our model\n",
        "pickle.dump(pipeline, open(\"model.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "4AVyiZlPvLcZ"
      },
      "id": "4AVyiZlPvLcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file locally\n",
        "files.download('model.pkl')"
      ],
      "metadata": {
        "id": "Z1-BMQx4vhkM"
      },
      "id": "Z1-BMQx4vhkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
        "\n",
        "st.title(\"Week 3: The Airbnb dataset of Amsterdam\")\n",
        "st.markdown(\n",
        "    \"The dataset contains modifications with regards to the original for illustrative & learning purposes\"\n",
        ")\n",
        "\n",
        "st.text(\"This widget can be used by hosts to check their expected tips per listing.\")\n",
        "\n",
        "# review_scores_rating: 0 to 5\n",
        "review_scores_rating = st.slider('What rating is this listing?', 0.00, 5.00, 4.50)\n",
        "# room_type: ['Shared room', 'Private room', 'Hotel room', 'Entire home/apt']\n",
        "room_type = st.radio(\n",
        "    \"What room type do you have?\",\n",
        "    ('Shared room', 'Private room', 'Hotel room', 'Entire home/apt'))\n",
        "# service_cost: ['$0.99', '$4.99', '$2.99', '$10.99']\n",
        "service_cost = st.radio(\n",
        "    \"What room type do you have?\",\n",
        "    ('$0.99', '$4.99', '$2.99', '$10.99'))\n",
        "# instant_bookable: 0, 1\n",
        "instant_bookable = st.radio(\n",
        "    \"Is the listing instantly bookable?\",\n",
        "    (\"True\", \"False\"))\n",
        "instant_bookable = 1 if instant_bookable == \"True\" else 0\n",
        "\n",
        "example = pd.DataFrame({\n",
        "    \"review_scores_rating\": [review_scores_rating],\n",
        "    \"room_type\": [room_type],\n",
        "    \"service_cost\": [service_cost],\n",
        "    \"instant_bookable\": [instant_bookable]\n",
        "    })\n",
        "\n",
        "if st.button('Predict?'):\n",
        "    st.write(\"The model predicts that the tipping category for this listing is:\", model.predict(example)[0])"
      ],
      "metadata": {
        "id": "iu9zLIbXpwmy"
      },
      "id": "iu9zLIbXpwmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **%%writefile [FILE_NAME].[FILE_EXTENSION]** command let's us save the code written in the cells in your Google Colab instance. Having it saved like that enables us to download it as a file, as seen below:"
      ],
      "metadata": {
        "id": "_AYCGSIhLvnL"
      },
      "id": "_AYCGSIhLvnL"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file locally\n",
        "files.download('streamlit_app.py')"
      ],
      "metadata": {
        "id": "23mmiMQctwLM"
      },
      "id": "23mmiMQctwLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "pandas==1.5.2\n",
        "scikit-learn==1.2.0"
      ],
      "metadata": {
        "id": "9dXqVGYRuEIL"
      },
      "id": "9dXqVGYRuEIL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file locally\n",
        "files.download('requirements.txt')"
      ],
      "metadata": {
        "id": "3ViZ7akAuM3t"
      },
      "id": "3ViZ7akAuM3t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please verify that you've downloaded three files:\n",
        "- `model.pkl`\n",
        "- `streamlit_app.py`\n",
        "- `requirements.txt`\n",
        "\n",
        "Now let's head over to GitHub and [create an account](https://github.com/signup).\n",
        "\n",
        "Then, since you are logged in [go to GitHub.com](https://github.com) and click on the **+** icon at the top-right corner and select **New repository**.\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/4gkPBCp/Screen-Shot-2022-11-28-at-1-51-02-PM.png width=\"300\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "Here you provide:\n",
        "- **Repository name**: Up to you\n",
        "- **License**: Up to you. We recommend **apache-2.0**.\n",
        "\n",
        "- **Public or private?** Public, otherwise you can't host it on [Streamlit Share](https://share.streamlit.io)!\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/0B533dw/Screen-Shot-2022-11-28-at-1-55-14-PM.png width=\"450\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "Then upload the three files to this URL below. ***Please modify it before copy-pasting it***:\n",
        "\n",
        "```https://github.com/[YOUR_ACCOUNT_NAME]/[YOUR_REPOSITORY_NAME]/upload/main```\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/jTsrgJw/Screen-Shot-2022-11-28-at-1-58-31-PM.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "Commit directly to the `main` branch, then click **Commit changes**.\n",
        "\n",
        "Next, you have to create an account on [Streamlit Share](https://share.streamlit.io/signup).\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/znFngJc/Screen-Shot-2022-11-28-at-1-59-47-PM.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "It's recommended to click **Continue with GitHub**.\n",
        "\n",
        "Then, select **New app** **>** **Deploy a new app...** **>** **From existing repo**.\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/VQPQzt3/Screen-Shot-2022-11-28-at-2-05-04-PM.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "\n",
        "Followed by providing your:\n",
        "\n",
        "```[GITHUB_ACCOUNT_NAME]/[GITHUB_REPOSITORY]```\n",
        "\n",
        "<center>\n",
        "  <img src=https://i.ibb.co/PDSQccD/Screen-Shot-2022-11-28-at-2-10-47-PM.png width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "\n",
        "You will have to wait around 1-5 minutes, then an automatic hyperlink is generated for your new website. An example is this app:\n",
        "\n",
        "```https://[GITHUB_ACCOUNT_NAME]-[GITHUB_REPOSITORY]-[RANDOM_6_LETTER_STRING].streamlit.app/```\n",
        "\n",
        "***Please modify the link before copy-pasting it.***"
      ],
      "metadata": {
        "id": "5STbAwboLzCY"
      },
      "id": "5STbAwboLzCY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🎉 CONGRATULATIONS!!!\n",
        "\n",
        "Awesome!! You've finished all the Weeks' assignments! Finishing Week 1, 2 and 3 is an amazing feat and requires a lot of hard work and dedication. Please take time to enjoy this!\n",
        "\n",
        "If you have any lingering questions, post them on Slack! As you know, we're always here to help.\n",
        "\n",
        "And if you want any additional challenge questions, check out the bonus extensions below.\n",
        "\n",
        "---\n",
        "\n",
        "## Extensions (Optional)\n",
        "\n",
        "<center>\n",
        "  <img src=https://upload.wikimedia.org/wikipedia/commons/c/c6/Celebration_fireworks.jpg width=\"500\" align=\"center\" />\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "🎉🎉 Amazing 🎉🎉 You completed this week's project! Have you thought about extending this project and try some extensions like:\n",
        "\n",
        "- Using [PCA](https://corise.com/course/intermediate-python-for-data-science/v2/module/unsupervised-learning#corise_clc1r07oc00083b6oy094wn04) on some of the available attributes so that you can simplify the model?\n",
        "- [Create features based on the features you have available](https://corise.com/course/intermediate-python-for-data-science/v2/module/professionalize-moar#corise_clc262ibo00253b6o1omtabr1) and what can be found on the internet?\n",
        "- Perform [hyperparameter tuning](https://corise.com/course/intermediate-python-for-data-science/v2/module/professionalize-moar#corise_clc24tq5a00103b6okrxvknkz) to find the most optimal parameters for your algorithm?\n",
        "- Does it make sense to actually use [other metrics](https://corise.com/course/intermediate-python-for-data-science/v2/module/professionalize-moar#corise_clc262ckm00233b6o546sk268)?\n",
        "- Train the model on [different kinds of Classification algorithms](https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/) from the Scikit-learn toolkit!\n",
        "- ...\n",
        "\n",
        "The possibilities are endless!\n",
        "\n",
        "# Next Up?\n",
        "This was the courses' last week. We've hope you've enjoyed it as much as we did! Thank you for taking this course and working on these projects!"
      ],
      "metadata": {
        "id": "hnwmZSP1L_7M"
      },
      "id": "hnwmZSP1L_7M"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}